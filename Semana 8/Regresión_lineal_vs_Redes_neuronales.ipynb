{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Regresión lineal vs Redes neuronales.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMWMoAnrln2rMXezZlR5VeE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fjme95/python-para-la-ciencia-de-datos/blob/main/Semana%208/Regresi%C3%B3n_lineal_vs_Redes_neuronales.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este notebook se utilizará un dataset para regresión, con la finalidad de comprar el desempeño entre una regresión lineal y un par de redes neuronales. \n",
        "\n",
        "Se verá la importación de las depencias, la carga de los datos, el uso de [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) junto a [ColumnTransformer](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html) para conectar facilmente al procesamiento de los datos con el algoritmo (en este caso, regresión lineal). \n",
        "\n",
        "También, se verá la creación de las redes neuronales y el uso de callbacks (en particular, guardar checkpoints) durante el entrenamiento.\n",
        "\n",
        "Para todos los modelos se evaluará el desempeño que se obtuvo y se harán comparaciones entre los tres."
      ],
      "metadata": {
        "id": "1nfpaMZA1LlY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencias"
      ],
      "metadata": {
        "id": "C5RrB_RNCNDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U plotly"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WCX83vYyk1iX",
        "outputId": "a25879e3-dfa4-4682-b8bd-0ec9fa70185b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (5.5.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from plotly) (8.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysF3G6ZafSmP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Manejo de los datos\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Utilidades, modelo y métricas\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Para la red neuronal\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Visualización\n",
        "import plotly.express as px\n",
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import plotly.io as pio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vcCzm4LSm132",
        "outputId": "76a35e9b-60e8-4a9d-c6f1-28519f12c259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pio.templates.default = \"plotly_white\""
      ],
      "metadata": {
        "id": "DlJUqRPUh8ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# directorio para guardar los checkpoints de las redes neuronales\n",
        "checkpoint_filepath = 'checkpoints'"
      ],
      "metadata": {
        "id": "n8BwHVdbsvvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Funciones"
      ],
      "metadata": {
        "id": "SKHnUQyofNqW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_model_evaluation(real_train, predecido_train, real_test, predecido_test, plot_title):\n",
        "    \n",
        "    fig = make_subplots(\n",
        "        rows = 1, cols = 2, \n",
        "        subplot_titles = ['Conjunto de entrenamiento', 'Conjunto de prueba'],        \n",
        "    )\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x = real_train, \n",
        "            y = predecido_train, \n",
        "            mode='markers', \n",
        "            ), \n",
        "        row = 1, col = 1\n",
        "    )\n",
        "\n",
        "    fig.add_shape(\n",
        "        type = 'line', \n",
        "        x0 = np.min(predecido_train), y0 = np.min(predecido_train), \n",
        "        x1 = np.max(predecido_train), y1 = np.max(predecido_train), \n",
        "        line=dict(dash = 'dot')\n",
        "    )\n",
        "\n",
        "    fig.add_trace(\n",
        "        go.Scatter(x = real_test, y = predecido_test, mode = 'markers'), \n",
        "        row = 1, col = 2\n",
        "    )\n",
        "\n",
        "    fig.add_shape(\n",
        "        type = 'line', \n",
        "        x0 = np.min(predecido_test), y0 = np.min(predecido_test), \n",
        "        x1 = np.max(predecido_test), y1 = np.max(predecido_test), \n",
        "        line=dict(dash = 'dot'), \n",
        "        row = 1, col = 2\n",
        "    )\n",
        "    \n",
        "    fig.update_layout(\n",
        "        showlegend=False, \n",
        "        title_text=plot_title\n",
        "    )\n",
        "\n",
        "    fig.update_xaxes(title_text = 'Cargos reales', row = 1, col = 1)\n",
        "    fig.update_xaxes(title_text = 'Cargos reales', row = 1, col = 2)\n",
        "    fig.update_yaxes(title_text = 'Cargos predecidos', row = 1, col = 1)\n",
        "    fig.update_yaxes(title_text = 'Cargos predecidos', row = 1, col = 2)\n",
        "\n",
        "    return fig\n"
      ],
      "metadata": {
        "id": "lXVjHz8CfPCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datos"
      ],
      "metadata": {
        "id": "vRb33QprCO2i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se trabajaran con los datos de [Medical Cost Personal Datasets](https://www.kaggle.com/mirichoi0218/insurance). Contiene 7 columnas descritas a continuación:\n",
        "\n",
        "\n",
        "\n",
        "    age: age of primary beneficiary\n",
        "\n",
        "    sex: insurance contractor gender, female, male\n",
        "\n",
        "    bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height,\n",
        "    objective index of body weight (kg / m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n",
        "\n",
        "    children: Number of children covered by health insurance / Number of dependents\n",
        "\n",
        "    smoker: Smoking\n",
        "\n",
        "    region: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n",
        "\n",
        "    charges: Individual medical costs billed by health insurance\n",
        "\n",
        "\n",
        "Se quiere saber si se pueden predecir los cargos (charges) en función de las demás variables."
      ],
      "metadata": {
        "id": "sZiAlTuAWOz5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Carga de los datos"
      ],
      "metadata": {
        "id": "En-vePooXzlf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv')\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "xwH_MzmFhS8u",
        "outputId": "0c2fbcff-f83b-4547-b058-8643adcfcdd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b2da1691-7e9e-4012-8cb1-bd801d15e9db\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>bmi</th>\n",
              "      <th>children</th>\n",
              "      <th>smoker</th>\n",
              "      <th>region</th>\n",
              "      <th>charges</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19</td>\n",
              "      <td>female</td>\n",
              "      <td>27.900</td>\n",
              "      <td>0</td>\n",
              "      <td>yes</td>\n",
              "      <td>southwest</td>\n",
              "      <td>16884.92400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>18</td>\n",
              "      <td>male</td>\n",
              "      <td>33.770</td>\n",
              "      <td>1</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>1725.55230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>28</td>\n",
              "      <td>male</td>\n",
              "      <td>33.000</td>\n",
              "      <td>3</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>4449.46200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>33</td>\n",
              "      <td>male</td>\n",
              "      <td>22.705</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>21984.47061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>32</td>\n",
              "      <td>male</td>\n",
              "      <td>28.880</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>3866.85520</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1333</th>\n",
              "      <td>50</td>\n",
              "      <td>male</td>\n",
              "      <td>30.970</td>\n",
              "      <td>3</td>\n",
              "      <td>no</td>\n",
              "      <td>northwest</td>\n",
              "      <td>10600.54830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1334</th>\n",
              "      <td>18</td>\n",
              "      <td>female</td>\n",
              "      <td>31.920</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>northeast</td>\n",
              "      <td>2205.98080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1335</th>\n",
              "      <td>18</td>\n",
              "      <td>female</td>\n",
              "      <td>36.850</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>southeast</td>\n",
              "      <td>1629.83350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1336</th>\n",
              "      <td>21</td>\n",
              "      <td>female</td>\n",
              "      <td>25.800</td>\n",
              "      <td>0</td>\n",
              "      <td>no</td>\n",
              "      <td>southwest</td>\n",
              "      <td>2007.94500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1337</th>\n",
              "      <td>61</td>\n",
              "      <td>female</td>\n",
              "      <td>29.070</td>\n",
              "      <td>0</td>\n",
              "      <td>yes</td>\n",
              "      <td>northwest</td>\n",
              "      <td>29141.36030</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1338 rows × 7 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2da1691-7e9e-4012-8cb1-bd801d15e9db')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b2da1691-7e9e-4012-8cb1-bd801d15e9db button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b2da1691-7e9e-4012-8cb1-bd801d15e9db');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      age     sex     bmi  children smoker     region      charges\n",
              "0      19  female  27.900         0    yes  southwest  16884.92400\n",
              "1      18    male  33.770         1     no  southeast   1725.55230\n",
              "2      28    male  33.000         3     no  southeast   4449.46200\n",
              "3      33    male  22.705         0     no  northwest  21984.47061\n",
              "4      32    male  28.880         0     no  northwest   3866.85520\n",
              "...   ...     ...     ...       ...    ...        ...          ...\n",
              "1333   50    male  30.970         3     no  northwest  10600.54830\n",
              "1334   18  female  31.920         0     no  northeast   2205.98080\n",
              "1335   18  female  36.850         0     no  southeast   1629.83350\n",
              "1336   21  female  25.800         0     no  southwest   2007.94500\n",
              "1337   61  female  29.070         0    yes  northwest  29141.36030\n",
              "\n",
              "[1338 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descripción de los datos"
      ],
      "metadata": {
        "id": "zQ9T_gR0X1Zd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ep_Sn_9Vj37c",
        "outputId": "8b37ba95-574c-4945-a975-6b17b131ba19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1338 entries, 0 to 1337\n",
            "Data columns (total 7 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   age       1338 non-null   int64  \n",
            " 1   sex       1338 non-null   object \n",
            " 2   bmi       1338 non-null   float64\n",
            " 3   children  1338 non-null   int64  \n",
            " 4   smoker    1338 non-null   object \n",
            " 5   region    1338 non-null   object \n",
            " 6   charges   1338 non-null   float64\n",
            "dtypes: float64(2), int64(2), object(3)\n",
            "memory usage: 73.3+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Ejercicio**: Realizar un análisis descriptivo de los datos."
      ],
      "metadata": {
        "id": "8uUAN_gTYCem"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## División en conjuntos de entrenamiento y prueba"
      ],
      "metadata": {
        "id": "nbw8X7yPX8zX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Separamos el DataFrame en dos: \n",
        "1. X: Contiene las variables regresoras.\n",
        "2. y: La variable de respuesta."
      ],
      "metadata": {
        "id": "Eb5HfsgiYS_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop(columns = 'charges')\n",
        "y = data['charges']"
      ],
      "metadata": {
        "id": "_h9fy3zBhVrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora divimos el dataset en el conjunto de entrenamiento y de prueba. Esto, como hemos visto, nos ayuda a evaluar si el modelo \"aprendió\" a generalizar las predicciones o si se sobreajustó.\n",
        "\n",
        "El conjunto de entrenamiento tendrá el 70% de los datos totales."
      ],
      "metadata": {
        "id": "IOK-uULdYm0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = .7, random_state = 10)\n",
        "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bfX60prNhWJb",
        "outputId": "11a5e852-f33f-499d-841e-e6da98c8875b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(936, 6) (936,) (402, 6) (402,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regressión Lineal"
      ],
      "metadata": {
        "id": "TQPMRLmPCQ_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero veremos los resultados que obtenemos con una regresión lineal (múltiple). Sin entrar en detalle a los supuestos del algoritmo, la hipótesis es la siguiente.\n",
        "\n",
        "\\begin{align}\n",
        "    y_i = \\beta_0 + \\beta_1x_{i1} + \\dots + \\beta_px_{ip} + \\epsilon_i,\n",
        "\\end{align}\n",
        "\n",
        "donde, para el *i-ésimo* caso, $y_i$ es la variable de respuesta, $x_{ij}$ es la medición de la *j-ésima* variable, $b_j$ es el coeficiente de ésta variable y $\\epsilon_i$ es el *error* (que queremos que sea lo más pequeño posible). \n",
        "\n",
        "Es decir, **la variable de respuesta es una *combinación lineal* de los regresores más un error**. Equivalentemente, se encuentra el hiperplano que \n",
        "\"mejor se ajuste\" a los datos cuando se minimiza el error."
      ],
      "metadata": {
        "id": "IIHThcdcZAd9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algoritmo/Arquitectura"
      ],
      "metadata": {
        "id": "eq777ocRclwp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Crearemos un [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) para facilitar el uso del modelo para las predicciones. \n",
        "\n",
        "El primer paso es un tranformador de columnas que se va a encargar de crear las variables dummies de las variables categóricas.\n",
        "\n",
        "El segundo paso, la regresión lineal."
      ],
      "metadata": {
        "id": "KFwNMLsWbcWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = ColumnTransformer(\n",
        "    [\n",
        "     (\"dummies\", OneHotEncoder(drop = 'first'), ['sex', 'smoker', 'region'])\n",
        "    ], \n",
        "    remainder = 'passthrough'\n",
        ")\n",
        "\n",
        "lr_pipe = Pipeline(\n",
        "    [\n",
        "     (\"transformador\", transformer), \n",
        "     ('linear_regression', LinearRegression())\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "EBPMQLhXiqRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento"
      ],
      "metadata": {
        "id": "Td3pEmDjcqNi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos la función ```fit``` para ajustar el modelo a nuestros datos."
      ],
      "metadata": {
        "id": "aTzQkSmscuNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr_pipe.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGN27lfvcrft",
        "outputId": "9dad9b95-6d80-4dbc-ee42-9c4e2431c370"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('transformador',\n",
              "                 ColumnTransformer(remainder='passthrough',\n",
              "                                   transformers=[('dummies',\n",
              "                                                  OneHotEncoder(drop='first'),\n",
              "                                                  ['sex', 'smoker',\n",
              "                                                   'region'])])),\n",
              "                ('linear_regression', LinearRegression())])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación"
      ],
      "metadata": {
        "id": "4Qp8nI5TCTVD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con el modelo entrenado, usamos la función ```predict``` para obtener las predicciones del modelo con los datos que le suministramos.\n",
        "\n",
        "Obtenemos las predicciones de ambos conjuntos de datos (entrenamiento y prueba)."
      ],
      "metadata": {
        "id": "aIPN7alXc0kR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train_lr = lr_pipe.predict(X_train)\n",
        "y_pred_test_lr = lr_pipe.predict(X_test)"
      ],
      "metadata": {
        "id": "UnCzu25Zh5Sy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Métricas"
      ],
      "metadata": {
        "id": "yaQFUywECXIn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Las métricas que usaremos son 3:\n",
        "\n",
        "1. **Coeficiente de determinación ($R^2$)**: El mejor posible valor es 1 (toda la variabilidad de la variable dependiente se puede explicar con las variables independientes). Un modelo que siempre predice la media de la variable de respuesta tiene un resultado de 0. Modelos con peor desempeño que el que sólo predice la media, obtienen valores negativos.\n",
        "\n",
        "2. **Error cuadrático promedio (MSE)**: El promedio de los errores al cuadrado.\n",
        "\n",
        "3. **Error absoluto promedio (MSE)**: El promedio de los valores absolutos de los errores."
      ],
      "metadata": {
        "id": "IXn79Fz1dS58"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    f\"R2 train: {r2_score(y_train, y_pred_train_lr)}\", \n",
        "    f\"R2 test: {r2_score(y_test, y_pred_test_lr)}\", \n",
        "    \"\",\n",
        "    f\"MSE train: {mean_squared_error(y_train, y_pred_train_lr)}\", \n",
        "    f\"MSE test: {mean_squared_error(y_test, y_pred_test_lr)}\", \n",
        "    \"\",\n",
        "    f\"MAE train: {mean_absolute_error(y_train, y_pred_train_lr)}\", \n",
        "    f\"MAE test: {mean_absolute_error(y_test, y_pred_test_lr)}\", \n",
        "    sep = '\\n'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UpBhxqbiUN5",
        "outputId": "701c4604-ce59-43a5-df87-5cd104b9f218"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 train: 0.7618780003618578\n",
            "R2 test: 0.7166124432331722\n",
            "\n",
            "MSE train: 36576361.89065711\n",
            "MSE test: 36800107.88801561\n",
            "\n",
            "MAE train: 4175.618401731288\n",
            "MAE test: 4226.647664219914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El coeficiente de determinación de .76 en entramiento y .71 en prueba nos indica que el modelo se comporta de manera similar con datos vistos durante el entramiento y fuera de estos (aparentemente no hay sobreajuste)."
      ],
      "metadata": {
        "id": "c_B5iynagv98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualización"
      ],
      "metadata": {
        "id": "_WL7bVz6CZNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_evaluation(y_train, y_pred_train_lr, y_test, y_pred_test_lr, \"Regresión lineal\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "9X7uPbRggOt7",
        "outputId": "93e442fc-b064-4d29-b257-eff99e12b9a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"df23babc-5ae6-4ddc-9d69-0d89fbdbdd35\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"df23babc-5ae6-4ddc-9d69-0d89fbdbdd35\")) {                    Plotly.newPlot(                        \"df23babc-5ae6-4ddc-9d69-0d89fbdbdd35\",                        [{\"mode\":\"markers\",\"x\":[3167.45585,2689.4954,11576.13,16586.49771,6746.7425,5976.8311,5649.715,15161.5344,2007.945,19214.705530000003,16455.70785,10594.50155,27117.99378,4296.2712,7151.092,8410.04685,22331.5668,37165.1638,17468.9839,2494.022,6356.2707,11305.93455,12235.8392,14256.1928,13822.803,7173.35995,9140.951,1711.0268,5926.846,32108.662819999998,17942.106,43896.3763,8671.19125,6112.35295,15019.76005,11365.952,2709.24395,43254.41795,1263.249,3757.8448,20420.60465,2302.3,11658.11505,11394.06555,1967.0227,3353.4703,10269.46,18963.171919999997,31620.001060000002,30259.995560000003,6571.544,1632.56445,2755.02095,5934.3798,14319.031,19964.7463,6849.026,2498.4144,1631.6683,3766.8838,9625.92,3309.7926,37079.372,15359.1045,36124.5737,2842.76075,1622.1885,46599.1084,6948.7008,4500.33925,2128.43105,9095.06825,19673.335730000003,48824.45,8688.85885,3645.0894,2480.9791,2154.361,33307.5508,12643.3778,27375.90478,8556.907,37270.1512,6593.5083,9101.798,13019.16105,28868.6639,1621.3402,6571.02435,34828.654,4260.744000000001,8765.249,4040.55825,9875.6804,4337.7352,11356.6609,9290.1395,34439.8559,8891.1395,7162.0122,8026.6666,35160.13457,13555.0049,4719.73655,7222.78625,2534.39375,1719.4363,1635.73365,1725.5523,1824.2854,8342.90875,12829.4551,12979.358,4391.652,19933.458,7623.518,7640.3092,28101.33305,8442.667,2721.3208,2803.69785,12044.341999999999,1131.5066,11743.9341,5974.3847,2731.9122,26140.3603,4243.59005,4846.92015,28287.897660000002,20630.28351,2741.948,29186.48236,1261.859,6877.9801,14283.4594,4402.233,37742.5757,13217.0945,11163.568000000001,13451.122,26467.09737,58571.074479999996,13204.28565,1639.5631,2473.3341,48970.2476,9910.35985,3044.2133,60021.39897,41034.2214,9877.6077,8125.7845,2020.5523,25382.297000000002,34303.1672,7742.1098,10702.6424,8516.829,27322.733860000004,37701.8768,26125.674769999998,30184.9367,10107.2206,3070.8087,5152.134,4746.344,5910.944,4415.1588,14043.4767,16085.1275,8428.0693,39727.614,8283.6807,14210.53595,10355.641,12265.5069,6455.86265,11488.31695,4134.08245,9563.029,18804.7524,11856.4115,11264.541000000001,1631.8212,38792.6856,42560.4304,2150.469,7050.642,42112.2356,41949.2441,4462.7218,5484.4673,20234.85475,1877.9294,11085.5868,8219.2039,13126.67745,5227.98875,3268.84665,12479.70895,6250.435,28923.136919999997,2136.88225,13937.6665,36580.28216,6652.5288,3947.4131,19444.2658,12741.16745,1832.094,13770.0979,6113.23105,6198.7518,38511.6283,14382.70905,16450.8947,7133.9025,37829.7242,6406.4107,6933.24225,2137.6536,12404.8791,10096.97,26109.32905,3906.127,44423.803,12124.9924,1253.9360000000001,34166.273,45702.02235,13470.86,2632.992,38709.176,9182.17,1826.8429999999998,7243.8136,12430.95335,10118.424,4827.90495,5312.16985,3213.62205,4779.6023,4931.647,23082.95533,4433.9159,7985.815,33750.2918,48675.5177,33475.81715,9872.701,44260.7499,10601.412,2117.33885,11070.535,33900.653,2639.0429,5428.7277,11837.16,9301.89355,35491.64,47291.055,9566.9909,14410.9321,20878.78443,42983.4585,10796.35025,11884.04858,11244.3769,3279.86855,6796.86325,27037.9141,10848.1343,10560.4917,44202.6536,9288.0267,9620.3307,5458.04645,36149.4835,11381.3254,21595.38229,21880.82,1977.815,36837.467000000004,6640.54485,4347.02335,36219.40545,26926.5144,4435.0942,6128.79745,3597.5959999999995,1704.5681,16776.30405,2727.3951,7348.142,1242.816,45863.205,24671.66334,6186.1269999999995,19539.243000000002,2566.4707,49577.6624,39836.519,6402.29135,8827.2099,17904.52705,6272.4772,6500.2359,2257.47525,2155.6815,2362.22905,1815.8759,1875.344,9724.53,11538.421,2219.4451,2156.7518,42969.8527,1708.92575,9722.7695,15820.698999999999,11842.442,8582.3023,33907.547999999995,38998.546,7345.7266,4463.2051,15828.821730000001,2867.1196,5438.7491,4564.19145,46718.16325,12592.5345,23563.016180000002,6185.3208,10976.24575,27941.28758,4189.1131,41097.16175,36197.699,15817.9857,26018.95052,3857.75925,3490.5491,6238.298000000001,2396.0959,7419.4779,8413.46305,1880.07,7050.0213,4518.82625,11552.903999999999,10982.5013,4673.3922,23807.2406,20149.3229,12949.1554,4753.6368,27000.98473,24180.9335,7731.4271,7749.1564,9644.2525,42211.1382,6940.90985,24869.8368,13457.9608,12224.35085,8551.347,22478.6,12925.886,5012.471,9414.92,14988.431999999999,4618.0799,2730.10785,19749.383380000003,3556.9223,39241.442,17043.3414,8277.523000000001,4830.63,7518.02535,10214.636,18767.7377,1242.26,4536.259,8965.79575,34617.84065,16796.41194,11848.141000000001,8932.084,20009.63365,1759.338,45008.9555,37465.34375,46661.4424,11881.9696,5972.378000000001,1252.407,3875.7341,1743.214,3981.9768,13635.6379,10381.4787,9048.0273,3704.3545,5209.57885,7265.7025,47462.894,11674.13,1737.376,2775.19215,3693.428,5028.1466,1639.5631,9058.7303,10226.2842,4529.477,1629.8335,12815.44495,24915.04626,14692.66935,7986.47525,8017.06115,12105.32,3554.203,10461.9794,5615.369000000001,4795.6568,19515.5416,2103.08,4349.462,12523.6048,10072.05505,1136.3994,21677.28345,1704.70015,21774.32215,13063.883,5116.5004,10825.2537,3353.284,4149.736,13405.3903,10977.2063,14349.8544,11741.726,44641.1974,4762.329000000001,7371.772,3176.8159,2850.68375,40273.6455,17560.37975,4433.3877,1526.3120000000001,1702.4553,6373.55735,10942.13205,8547.6913,8604.48365,3176.2877,11150.78,4527.18295,12233.828000000001,9850.431999999999,7196.866999999999,11729.6795,21223.6758,10435.06525,5272.1758,2250.8352,33732.6867,14358.36437,8233.0975,17496.306,2789.0574,14901.5167,5124.1887,8027.968000000001,13143.33665,43578.9394,10807.4863,8522.003,5400.9805,9447.3824,10601.63225,4237.12655,14001.2867,1615.7667,11840.77505,40182.246,15230.32405,4877.98105,28340.18885,2699.56835,9748.9106,8539.671,47403.88,20167.336030000002,17352.6803,4058.1161,16297.846000000001,8596.8278,2497.0383,38282.7495,36898.73308,14474.675,13224.693000000001,42124.5153,14571.8908,11253.421,1534.3045,18246.4955,32548.3405,3877.30425,11842.62375,38126.2465,16577.7795,24915.22085,8569.8618,7804.1605,10736.87075,8968.33,9991.03765,21195.818,11658.37915,8603.8234,13919.8229,4915.05985,2217.6012,1634.5734,4934.705,32787.458589999995,12269.68865,2217.46915,5373.36425,8601.3293,12142.5786,7077.1894,20709.02034,1633.0444,2130.6759,7537.1639,14426.07385,1261.442,5757.41345,2211.13075,8782.469000000001,6748.5912,11833.7823,10928.848999999998,5709.1644,2203.73595,5354.07465,23065.4207,6875.960999999999,22412.6485,2055.3249,12950.0712,40003.33225,12363.546999999999,4032.2407,5855.9025,17626.23951,11033.6617,3594.17085,11396.9002,41999.52,9391.346,3577.9990000000003,4504.6624,40720.55105,3392.3652,9264.796999999999,22192.43711,10450.552,2416.955,5989.52365,20177.671130000002,21984.47061,8835.26495,36307.7983,18648.4217,39722.7462,12485.8009,5327.40025,6496.8859999999995,10579.711000000001,6686.4313,10577.087,1980.07,11455.28,2221.56445,5397.6167,1986.9334,17179.522,3761.292,8733.22925,6082.405,9855.1314,6117.4945,9866.30485,13844.7972,7935.29115,2913.5690000000004,18328.2381,1682.5970000000002,39611.7577,2201.0971,8823.279,30063.58055,39047.285,24535.69855,3659.3459999999995,1163.4627,4718.20355,2020.1770000000001,24059.68019,3206.49135,7201.70085,11881.358,46130.5265,23288.9284,8520.026,8023.13545,42760.5022,21348.706000000002,35147.52848,14451.83515,12096.6512,34254.05335,4005.4225,14001.1338,9715.841,12890.05765,48673.5588,7228.21565,19719.6947,39774.2763,2974.1259999999997,6548.19505,1964.78,3861.20965,2200.83085,20781.48892,19521.9682,13981.85035,8515.7587,14235.072,41919.097,13143.86485,25992.82104,12638.195,10959.6947,42111.6647,7729.64575,24227.33724,10106.13425,1842.519,16138.76205,9283.562,43943.8761,3484.3309999999997,48885.135610000005,1731.6770000000001,14394.39815,44501.3982,2404.7338,3591.48,10806.839,11944.59435,2464.6188,46151.1245,4894.7533,9193.8385,14119.62,4133.64165,11945.1327,20745.9891,25081.76784,2710.82855,11657.7189,13470.8044,9800.8882,23244.7902,19350.3689,4449.462,13880.948999999999,16115.3045,33471.97189,2304.0022,47305.305,2138.0707,3056.3881,28468.91901,2395.17155,13887.204,21771.3423,8269.044,12222.8983,48549.17835,8605.3615,5325.651,20984.0936,10407.08585,12032.326000000001,10422.91665,12648.7034,9282.4806,47896.79135,13393.756000000001,1981.5819,9411.005,9541.69555,2902.9065,21978.6769,3161.454,9704.66805,8444.474,11187.6567,20277.80751,3366.6697,11436.73815,4747.0529,1917.3184,13041.921,18806.14547,2457.502,7337.7480000000005,4922.9159,1727.54,3756.6216,7954.517,25333.33284,11362.755,43813.8661,5969.723000000001,1769.53165,2438.0552,13844.506000000001,5261.46945,9634.538,10923.9332,6610.1097,34838.873,24393.6224,3410.324,12957.118,3260.199,28476.734989999997,8116.26885,8978.1851,4661.28635,19107.7796,12574.048999999999,2866.091,8334.5896,16884.924,13747.87235,47496.49445,4357.04365,5836.5204,5472.4490000000005,21797.0004,8823.98575,19496.71917,13112.6048,2690.1138,4667.60765,10602.385,4428.88785,11512.405,11830.6072,3393.35635,2719.27975,46200.9851,12982.8747,18955.22017,10564.8845,8534.6718,2254.7967,4571.41305,11326.71487,6551.7501,11520.09985,2045.68525,48517.56315,21659.9301,4438.2634,36950.2567,2527.81865,4687.7970000000005,5125.2157,7448.40395,30942.1918,8334.45755,3972.9247,6196.448,16657.71745,27724.28875,6289.7549,39725.51805,11743.298999999999,7624.63,2205.9808,6653.7886,14455.64405,47928.03,1712.227,4889.9995,2585.269,12629.1656,40419.0191,25678.77845,35069.37452,7682.67,1909.52745,2643.2685,10704.47,8988.15875,4320.41085,24603.04837,5693.4305,12730.9996,11286.5387,13831.1152,8062.764,39125.33225,15006.57945,8871.1517,3943.5954,32734.1863,4670.64,8798.593,8083.9198,11345.518999999998,9432.9253,7441.053000000001,2026.9741,7045.499,1632.03625,37607.5277,13462.52,2102.2647,2104.1134,6414.178000000001,11363.2832,4562.8421,4719.52405,6067.12675,5257.50795,6666.243,1984.4533,3935.1799,23401.30575,6600.361,1627.28245,8124.4084,3062.50825,1727.785,3180.5101,43753.33705,30166.618169999998,15555.18875,4234.927,18259.216,12129.61415,5148.5526,11737.84884,5594.8455,3292.52985,2457.21115,35595.5898,24667.418999999998,40974.1649,24520.264,10043.249,3172.018,36910.60803,6799.4580000000005,3866.8552,10141.1362,6079.6715,1744.465,19040.876,14313.8463,4266.1658,12146.971000000001,3989.841,4561.1885,10085.846,1837.237,47269.85400000001,23045.56616,12981.3457,3925.7582,3558.62025,5245.2269,4185.0979,11272.331390000001,5729.0053,11879.10405,16069.08475,63770.42801,8280.6227,11082.5772,4837.5823,8527.532,4074.4537,5031.26955,11090.7178,1532.4697,34672.1472,2322.6218,6858.4796,13224.05705,52590.82939,8059.6791,7526.70645,7152.6714,8944.1151,12029.2867,22218.1149,3046.062,1748.774,43921.1837,1674.6323,4137.5227,9447.25035,7650.77375,2261.5688,9861.025,5979.731,7147.4728],\"y\":[-818.091272561036,6666.5811243754,17055.05790685661,2394.561296457734,7798.843356927067,7703.498098656393,8196.6387244844,16329.80238476698,1007.5897163084701,8425.064913728569,16668.471772219713,11305.190631483078,8825.955508475545,2457.756915647762,8131.697484426022,10795.207989105456,32427.31083913072,29849.804431827095,26537.455340029428,3934.838710735243,13345.56817384392,11931.256905767637,15403.50311923806,13227.675906912216,15160.294264005151,6593.80149263734,11706.895773164688,-423.192676449964,7361.849867796405,10063.118942726167,27469.987637547772,37011.098173164806,12771.706594456195,9610.704199440526,15651.327489103584,14758.402230110314,1566.8876331943939,37044.69132925247,3478.7609709620137,3832.728716181042,6359.781486464126,351.9128162240049,11920.331969480547,14263.51868835519,1823.2834779368168,2012.2178554201018,13140.841450051932,8076.918484203819,16958.973545198845,12707.762416337635,9842.451828831883,999.2052617776571,6694.565547106311,9095.483482529617,17216.491414729502,30721.89460364996,8675.133622170757,4895.762452972987,4318.19383488028,7408.636077638457,11113.14848690457,3055.0500589294825,31155.422624871022,25221.426188712434,29796.520023320274,2154.888396816905,1986.3879364647655,39024.51765615588,12434.631071129148,3312.0717094606953,2966.506586462703,9048.731315943296,6222.91673154743,40584.26519839457,7698.262253923513,3643.0508225519916,6368.077720734469,1718.4966157038289,27122.116349749445,15189.601726143836,4897.7913758242175,12036.37676595018,31923.286799775768,6616.578247796238,9210.30571539341,14366.647609690051,38063.354429988045,-1761.6939507890984,9672.427856210845,27625.823058668055,5709.09754672489,11645.986853762905,5192.639653544931,14713.301358733075,5172.011347077736,14014.284372727521,11333.224217688698,27172.58766586512,7981.395478117833,11324.118539808816,9218.102505838386,11022.254305842664,14442.421141382049,4319.101631875823,6497.155903854473,6040.775600877554,1645.3448140799283,1778.7532747376808,3149.735716283485,3453.9536506266886,13294.840122658887,23198.979638802582,15618.969092968884,3403.080498897025,29289.11339371042,7476.116202844725,8969.715090890917,36909.01020992207,8165.258430359867,2570.761314211066,538.6113941211115,12106.819604892,1294.3998084204613,14410.636810753047,7101.74173777497,1010.3612859394652,9526.259797617964,5849.949256322354,7122.495296090772,15291.799197252036,13719.784006254096,3478.9299936462103,13616.241010034202,3136.8539477339327,4221.372284199049,23742.967133157446,6674.595656753012,31874.896382968374,12628.750207002617,14214.96609236558,11657.602634879004,14852.526202816789,33439.98347304115,9478.07698795585,2720.7071237310447,4487.5890929800225,40911.144613947516,11710.05617634558,2257.802502883862,39005.20720272194,35286.95755566904,13574.08454176718,11847.856779445352,3989.1839658241734,35442.74639362801,27367.37905060196,8165.078037770607,11264.03338736577,9468.120218046766,13608.476731034203,32127.990912204314,24467.84340321196,39024.24566078081,9070.209526506944,3038.654518195439,6534.350097498358,5517.990398895687,7368.964019353396,5008.109749328491,14590.777102493703,18717.38963910352,7027.082678870578,34251.028041721336,10656.196918421974,17211.00661259287,10693.322883750352,15445.217256196356,8069.307716606076,13653.264857235441,3175.3724338355187,12671.70879281314,6908.087813482129,15276.182783275133,14790.233792231997,4355.8036074353695,30027.77101873865,36209.44008876846,761.1569506652013,8310.888757359342,33971.20477693515,35604.34397398455,7902.841221887931,6597.802472506111,30702.856145591086,2556.8532474175627,12256.378492095737,9615.39994695389,2292.640705807955,5480.8384689312625,6520.717840854395,12120.286382244445,5701.76174835191,12298.467930488983,5045.301287689439,13751.917262213472,16576.61157218742,5381.594083515509,4929.550450315819,30272.630163177422,15566.105130524393,-864.677452300075,13663.98095552058,7751.901324277824,6601.076760760519,31435.557224895387,14167.010906785406,26093.840110676163,5397.272735184597,31592.549654521168,8230.605725540856,5489.245632357644,1655.0032906088818,7787.894352435327,10461.070384066923,36179.03574148043,3979.567427028902,37478.573035570545,11623.834862525135,1187.9839153338708,27773.140184103515,39404.07741360816,16512.682364717755,3611.8823976302483,32752.378204399083,7543.199062895092,4202.565024529347,8449.136212567613,13947.321199384969,11945.482584888272,2445.2072183306227,5777.8253927942715,3661.071124316084,5543.94425645017,4148.098784285405,2692.3748175374276,7133.11031811644,7252.348934128568,27493.459063944476,40716.777028545606,27083.25292625026,12486.655200935045,37823.80351052774,14772.597928877538,238.08854110261746,10161.608126654308,27895.707294627526,3354.655831912667,9578.318131227385,10660.272962729203,10100.357605195302,30220.42080141573,39890.91484602388,8877.616197819982,17054.071381006488,11146.540407024248,36836.040297147556,11505.548239170039,-335.51607867612074,11324.12801328248,2571.0712284724887,9456.490711565675,35701.73820474536,14831.897896349594,13019.839836715473,37576.029346004594,10813.525542382013,11763.475564280867,3603.5092024900478,28184.892163539294,18420.391455849902,-273.72770030700485,32925.864274038715,2725.266975264987,30693.29132914668,4047.1135874262363,6104.148086882567,29214.960925897863,35686.72598269266,4858.320786551147,9182.180508469759,6968.354227880427,-398.59275520761366,28090.8189836504,4064.895005717779,10649.972897747724,-1547.2722704907774,38042.4170606932,6741.645386968634,8319.205835008412,29674.091385584066,3394.170126904144,41428.325516393495,33894.039658953116,5799.792614234742,9550.796611802129,28532.981559021962,5178.109199352428,7935.475456613389,2713.2356803548737,1923.805836607513,4218.490483401674,1385.4161600967964,2040.4086353763232,11107.174247283248,18173.98971242771,4631.529067984655,2187.0742444931348,36450.291338740964,673.2857626124223,12167.917455873521,27354.201111088212,11959.519650995913,8094.973623620088,28051.32432617268,33907.98110655759,10728.076709629662,8807.120576727404,9723.245566565321,3472.9098185060757,11104.83684907958,5415.775058258456,40435.2005408571,19381.367314663676,9471.761600669764,8001.397310373133,13650.413101908664,16750.702434208786,4526.73450768596,35465.86000869589,30119.14723904255,26428.56996819084,5474.855797703942,5760.905122530905,5595.159532595077,8708.280031061066,1453.2447660933394,8230.510626547333,10843.774782784745,3083.3900631888064,9651.995724996643,7859.435118394173,13624.267163293105,15182.79165479902,8659.84084782028,34635.92426577353,30683.728016728437,15079.021413920094,10436.193657335698,14082.752337376176,34259.388224631286,9637.997497456276,10933.335850507687,7237.162296752551,35983.96184602719,8001.106704549722,35275.34357840211,13220.282737998172,12577.641572257973,10668.748673037855,32723.15938599878,13295.926082443282,7786.429612940494,11220.459303055479,13345.748584365407,6043.842171472232,6698.912051847892,11151.204069700503,6826.009033552433,32776.485714307055,26690.763929788885,9261.051256684568,7641.5876749895615,8209.150570880749,11189.6720343663,30079.04716228897,-1684.0350797820101,10653.693596700512,9222.37200252515,27524.832404052588,5490.2445875989,13361.338446231046,8665.613618429608,30707.398825585657,5500.76741936355,37870.46756768759,30869.292594838636,40361.94478202581,12658.93472580655,10526.345191617835,811.8861897829811,8569.004888733205,1534.6459499178127,4857.309873929615,15753.520690388224,13893.55810490876,12803.717507299469,5328.426118958356,4212.347110581704,12019.264243249403,39995.6922347832,14365.845284094245,98.63645235987315,-114.25062765526309,10428.633695537868,7191.5952972437335,2720.7071237310447,15436.401586155695,13935.350437854628,3957.1502691157766,3866.876564219212,12005.029598530418,10801.892364061565,15800.266547784478,7414.754770161908,8438.864407139787,13351.881456646308,4055.062983231506,12588.351693277164,5152.510003645435,9492.970374734505,30126.19324075859,146.84791861694612,6443.239043869809,11500.488625243976,11492.391376079013,2497.9125301833046,32753.64132881685,-366.11158800094563,32444.5042927019,17887.408307013688,3245.288438764317,13699.289809849379,3173.202572222106,6276.239405948574,17157.455995543034,16162.601510877761,12004.512664598926,12162.304724731635,38310.87375274327,9449.921166018616,11428.121289853032,2135.564092701261,4103.758429216967,34272.5659131688,27042.37239121504,7003.185649289768,2541.416309478973,-918.2914305142949,12085.851338692784,10690.394120368754,9650.030750785012,8380.676679524957,2005.6394238745888,11069.421478667235,5796.839782648491,11713.086885352644,11504.486204655183,10882.193662450394,10692.935961434112,31649.54031084153,12538.367158237892,7333.372826809875,2680.9506884012008,26907.6902271259,7200.432662096893,8932.285710925087,27846.022348942246,3296.271929044844,17722.126017424827,10885.136151196984,9343.532741594241,14663.210880283208,37636.1455857859,12631.465451090178,11652.041279080007,10323.725124848952,9980.868055748124,13059.173660643133,5004.331081850436,13549.053992490182,406.77748915103075,13090.761816019141,34604.8509398453,17995.442132430613,5444.399537631143,9533.406982977955,4748.10734415636,16984.7209835408,7796.729677921972,39908.672611084316,23691.549453517375,26519.024047744442,7898.167312778805,25164.092491425876,11667.902016353535,4557.274499977189,32141.143759577968,27966.626697122807,15906.19300338638,13004.016495970285,36109.06782376686,24951.75974187056,12054.977606407349,4387.8792418774465,27671.77737753443,25916.452949494484,5911.571836833198,13545.49815691249,30633.47132544676,26445.6671052692,35529.3113210876,15103.447771272902,12709.722186899162,14371.089213425152,10655.67658403292,7601.033567571409,31778.16643507295,11985.294303893883,8218.270843491617,15342.470214394976,5641.417391415609,5791.260306344237,5032.779513426969,4900.29423538718,34229.65364230001,14825.730455324974,5758.779139137569,7961.636115276644,8816.477424535853,10979.97521256876,7199.822558023941,15903.662675161202,4656.681787876078,3518.6864289760542,9773.830940256183,2759.594615434882,3034.2818407655086,11919.992009748526,4199.6831132175175,8585.379353044522,8253.579697820413,9709.936445121977,12380.04990394403,10882.337575062276,2380.7377496441277,4197.987321398596,32827.29612078876,9474.496246097111,33004.28072305555,4039.6891066342905,16574.12914102965,34887.48017900891,15418.563588214518,4056.9066284101173,4759.1574249075675,3697.906039937581,16820.137441763258,2450.8685698408262,15428.607942062084,36043.687147774894,14088.863198751165,6297.532954099406,6819.7605687556315,36042.43156484877,4092.657370164172,7222.506271488059,10471.429587574192,14853.955804024918,5090.3040566569,10529.993990415758,4682.252867278359,3754.7590143615744,11532.147811408857,28357.928804262036,27856.633498185667,30918.55269932131,16921.29607719526,8171.614656899146,7230.981981796704,14746.306213601401,6554.545718632169,8789.225022386123,3538.8252979585595,11590.519774828634,1613.418675322744,9496.310128636997,4848.674596478206,28552.721123482053,4800.160584949674,14488.243941477042,9529.980496570752,8045.447976169111,5675.2085382397745,12407.13848705967,15169.967507766767,12302.705563373263,882.5841492535437,27272.901708891888,5679.863073793709,32109.730539034917,118.35636137398615,9956.622545377035,16590.83459602512,33428.54160282257,34998.491534240835,5656.0517527821685,9154.842272434045,3333.7416793887023,4016.37152071558,4227.414580759545,1907.0880951560284,8094.3174404365745,12627.998086749187,40064.750838492735,3612.9153211576886,10374.008822634347,12450.133600032535,37208.10391612489,32080.633021883044,27401.897157292315,11901.418655885795,12713.346759793018,28278.888671872348,1470.3612116096992,13511.444219935092,10068.804362441017,5123.205669925897,40488.139424491106,6225.697456085334,29542.910091963608,33740.19641179823,2024.914392013854,8410.5149294068,-222.15195755032983,1301.3769440308479,1666.1520710974382,12130.081355210361,29100.286320728777,16300.426103707618,9204.851810161144,17440.490720286267,36242.47925827431,14793.13554910988,10916.851934943727,13005.235724235743,15010.389358855555,35945.195082898834,8295.979050475085,16254.036869707448,10416.28009252583,1699.6252219105336,26817.647138093194,11718.74863390416,31797.708635690677,2571.870010675617,35525.72166081547,-1303.1823428752577,14708.575727273088,34964.73349931919,5285.826321227134,5463.963325676872,13059.580062773715,14318.663913579996,2343.832057339954,39089.717939558584,5512.391711685545,7783.073621981814,15723.761266893089,4419.473896877425,14498.57311744594,31162.445080740432,801.8313825813093,1956.6616396744066,11822.888467860543,16379.503632625641,15347.882181893561,33347.636384547965,28963.147089660375,6779.1718274765735,14248.998162028809,26738.634928758172,15390.656170408456,1168.874823527436,39932.20339773029,5337.631792549448,3898.1093921730026,13140.12054526962,1225.876595646665,15668.07731539218,32252.159130527256,7175.4184149932735,12220.348732984628,41362.47870913999,6983.320060021859,6991.805698196626,31301.18010791654,11571.13040210622,9264.332788612224,9550.099775224462,15470.550368677043,9449.31651970197,40445.22466971571,14415.196662286988,3532.332557050093,11751.246625642227,15843.563779420932,3701.547422560576,32335.813908183565,2872.5841007004456,14730.006813728545,8729.240011719365,13409.304970801732,5420.007823991604,6346.264754745534,15348.667808274677,5572.860529579015,5397.31969776882,12485.277340010009,10951.180175571157,712.7705495751725,8885.03284746347,7411.390948054766,3638.6627594996426,3531.85053574033,10503.753098871755,12473.714738085979,13972.01607668573,37529.24405271349,6700.389965096958,2694.8043880698897,10160.950218096179,23390.93098199571,6607.244201785139,13232.97203091867,12784.16662096955,12313.415684392454,28838.85126089893,34726.4794215866,8629.59856143397,10148.456721319584,1547.2888529270404,12911.0373819505,11097.045688892576,12960.294927557024,5037.122163318527,28683.41946130957,17403.983463138513,3339.4010724802847,11248.526588638822,25510.754526009816,23432.205336012452,40834.07075174333,6170.21824839446,9286.0875146941,5652.41037015917,14819.685785372323,11624.250720197157,12304.218403726769,12114.702038789062,3220.188483091426,8850.265675328246,15011.932845137195,3331.705927837762,13268.455365687969,10589.711941105728,6467.507290921061,4035.456340901139,38747.46643332536,16364.491410572937,2422.3056657155757,12712.625855010705,8060.829522390817,3655.385704601231,5397.248666979947,10117.324286118313,10266.10806228226,11591.973523841312,1668.5639005475477,40848.155539286745,32284.950211995343,5637.868799511169,30652.427493326144,257.82383573915286,4503.140910674829,5389.0454744043855,12544.123608741618,39114.51080875467,11216.045421432154,4888.961709036761,10485.056639911418,27320.36717509659,4209.128070452436,9428.013498077471,34825.85759849807,10939.206078700441,7749.641821427191,2932.917592157479,9490.176590781371,25576.20125512507,40131.468990548994,1485.314942779114,8437.341573802976,-1250.2158652623239,11687.399362000699,33773.55659408876,35602.98891485686,31080.491445274853,10254.585687801144,3480.9308325754246,4394.053182526037,11673.096878241475,10764.452659188308,8503.58785808057,8005.435958152029,4134.442728345242,13065.05525561098,12076.159373968374,17085.395811746086,10493.04015006795,33740.60740231912,26102.478361977268,10894.312606416173,5603.773715691528,27624.033606764588,8102.367392991673,12551.50082249026,15577.362592436355,9732.368988657527,17143.120381949066,9242.179862891273,5568.794413137908,7045.83277141544,869.2805929509868,31901.515753588294,14461.24022534927,1440.087903002317,1894.824243895664,7229.842069861039,13982.43829434941,5728.83375791238,3658.553351455379,7265.677904267937,5632.809185585105,12673.16597006797,6110.7975492169535,6948.805700140412,33610.200712182675,8421.68232347367,-300.04142648905326,11509.368826449547,5216.000548140732,-2260.5220079138853,779.6576314138547,37021.51520445397,13310.33778441115,19956.54110346932,5119.713464421042,28809.145016595863,12760.675714758505,7147.195105659845,5831.585340919115,8264.71165377757,6336.755773901477,2393.894612735152,29099.925535550257,34522.83702510616,34898.99297263188,34281.65667775094,12284.102164039934,5471.077477233859,15437.43498200781,8019.94301008965,5599.607113596121,12100.33627234001,10351.388788901368,1842.3622708230869,29303.150459818065,15821.675766925768,10534.01075294826,12179.903857132485,9169.4726328855,5194.884243097822,13390.853453527783,400.3785336438268,39886.02227843116,11634.168905645158,15988.393685022045,6136.528177829761,3024.604643273755,8107.727512822679,3575.77903092188,2034.5660219108686,9641.14961509626,13567.36430209349,16384.540635188252,41386.83001998656,9904.001467320199,11356.103090974731,6823.100322658127,12100.804296902988,5600.992218194413,5884.97226459438,15006.595652763051,3936.561971216379,28803.229915500324,5231.091044655561,6668.271021977784,15364.056598169638,37831.35684704209,9786.987803347209,6243.884997746642,11543.638845102145,15298.239156009742,10010.521535832664,32649.329533877197,2712.538843777209,2902.2740428301368,37935.94753402208,3601.2333795338127,4765.84179986368,9948.38688854146,13157.033019935156,3720.1518637615736,9614.636205819166,9162.120532339144,10264.906578229125],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"markers\",\"x\":[7281.5056,5267.81815,12347.171999999999,24513.09126,3736.4647,7358.17565,9788.8659,17085.2676,8211.1002,19798.05455,3077.0955,3385.39915,6837.3687,8538.28845,26392.260290000002,13012.20865,3227.1211,15170.069,11073.176000000001,20773.62775,39556.4945,2134.9015,2198.18985,6555.07035,4340.4409,12622.1795,7740.3369999999995,12475.3513,3987.926,21082.16,1241.565,40103.89,17929.303369999998,8302.53565,3471.4096,5846.9176,13352.0998,9144.565,25656.575259999998,7726.854,13887.9685,30284.642939999998,5266.3656,12797.20962,1146.7966,7046.7222,8627.5411,39597.4072,12323.936000000002,11454.0215,40904.1995,3171.6149,7445.918000000001,13607.36875,27346.04207,12557.6053,10797.3362,5488.262,6282.235,40941.2854,1708.0014,23306.547000000002,28950.4692,1664.9996,17361.7661,7345.084,18157.876,7256.7231,7626.993,26236.57997,7325.0482,1720.3537,7153.5539,6986.696999999999,8232.6388,10370.91255,4889.0368,6474.013000000001,1625.43375,10115.00885,10264.4421,9386.1613,18223.4512,3561.8889,23887.6627,3392.9768,1135.9407,1880.487,5630.45785,10156.7832,22144.032000000003,62592.87309,9500.57305,46113.511,13390.559,4076.4970000000003,6389.37785,9880.068000000001,4751.07,12629.8967,6664.68595,21344.8467,22395.74424,4466.6214,9249.4952,8703.456,38245.59327,36189.1017,19023.26,5080.096,9957.7216,6313.759,4151.0287,17748.5062,37133.8982,21472.4788,4350.5144,41661.602,6985.50695,23241.47453,39983.42595,9174.13565,6710.1919,1633.9618,47055.5321,2899.48935,12231.6136,5138.2567,4646.759,12913.9924,24476.47851,27808.7251,18033.9679,25309.488999999998,6360.9936,20296.86345,10600.5483,9583.8933,34806.4677,5708.866999999999,6457.8434,21259.37795,12928.7911,2904.0879999999997,3537.703,36085.219,10713.643999999998,21232.182259999998,8457.818000000001,18838.70366,3500.6123,6753.0380000000005,1256.299,45710.20785,11289.10925,1141.4451,12644.589,1121.8739,12495.29085,27218.43725,13129.60345,1837.2819,6311.951999999999,12333.828000000001,18218.16139,48173.361,6781.3542,3994.1778,1621.8827,11165.41765,9964.06,7441.501,5240.765,4686.3887,18972.495,6393.60345,3732.6251,19144.57652,7633.7206,11093.6229,46889.2612,11931.12525,4454.40265,1149.3959,34779.615,3277.1609999999996,27533.9129,10959.33,19594.80965,2483.736,10065.413,7512.267,19442.3535,7209.4918,11299.343,10231.4999,14711.7438,4906.40965,13415.0381,6123.5688,5469.0066,1705.6245,12646.207,6435.6237,11482.63485,44400.4064,17663.1442,14590.63205,38711.0,2166.732,8116.68,38746.3551,5584.3057,6775.960999999999,55135.402089999996,18903.49141,5920.1041,3378.91,13430.265,11353.2276,5253.524,4738.2682,2196.4732,11987.1682,1515.3449,17178.6824,12268.63225,14133.03775,7421.19455,36021.0112,5966.8874,17081.08,6059.173000000001,37484.4493,10338.9316,2855.43755,13616.3586,29523.1656,1607.5101,9504.3103,25517.11363,18310.742,9869.8102,38415.474,11554.2236,12609.88702,10594.2257,3238.4357,14394.5579,2585.85065,2927.0647,14254.6082,40932.4295,4883.866,7418.522,9630.396999999999,8606.2174,5003.853,2680.9493,9549.5651,3847.6740000000004,24873.3849,5699.8375,42856.837999999996,3201.24515,22493.65964,11566.30055,7160.3303,1646.4297,9617.66245,2203.47185,1137.011,9487.6442,7731.85785,7443.64305,2523.1695,34472.841,1969.614,7789.635,41676.0811,1694.7964,1906.35825,11763.0009,9225.2564,21098.55405,2709.1119,4992.3764,6600.20595,18608.262,2459.7201,8252.2843,4441.21315,5425.02335,4058.71245,15518.18025,4544.2348,19199.944,8347.1643,5478.0368,14007.222,42303.69215,5377.4578,1628.4709,16420.49455,10436.096,9778.3472,15612.19335,7261.741,11015.1747,10493.9458,6203.90175,10197.7722,6770.1925,1972.95,9304.7019,29141.3603,46255.1125,11013.7119,8162.71625,12244.531,5662.225,5383.536,12094.478000000001,14478.33015,3208.7870000000003,7639.41745,2897.3235,29330.98315,11534.87265,1137.4697,23568.272,13429.0354,9222.4026,36397.576,3956.07145,10965.446000000002,9377.9047,2801.2588,44585.45587,23967.38305,13228.84695,2352.96845,6334.34355,11938.25595,6338.0756,20462.99766,8068.185,4949.7587,9863.4718,39871.7043,19361.9988,6184.2994,1728.8970000000002,7323.734818999999,14418.2804,5246.047,38344.566,7144.86265,2207.69745,3579.8287,10325.206,8310.83915,3481.868,4399.731,5385.3379,4766.022,16232.847,13974.45555,10795.937329999999,3021.80915,8615.3,9361.3268,35585.576,7147.105,4239.89265,11735.87905,17128.42608,5375.0380000000005,10791.96,3443.0640000000003,5415.6612,7727.2532,18765.87545,6358.77645,5002.7827,14449.8544,2597.779,13047.33235,51194.55914,2331.519,11946.6259,8964.06055,7160.094,13725.47184,8825.086,11411.685,8930.93455,24106.91255,17878.900680000002,22462.04375,1391.5287,8240.5896],\"y\":[8662.892823683967,6261.494308989841,15331.327091948984,11299.024976375407,4104.1694197378565,9577.526195656199,9565.698699095235,25909.471812968997,7622.082001534178,31079.908347705616,2143.345450338853,3992.4612279282737,7301.498825377097,11325.651698053054,12904.173104348218,11681.325753562302,5368.230275723963,18548.613958550395,10811.231470787661,31050.837965190312,34319.100971014515,4558.083779589422,1016.5287269640849,7284.109196552923,4224.262463119569,13915.947859276861,11949.12396751127,11048.407864424411,6440.174384508795,31312.005502884884,-1854.9885913960516,34319.56807902653,9330.37679505417,7464.62556619275,7848.100487560052,11843.552048440146,13982.214196041747,12715.35648472069,9899.510180600748,8632.625842198882,15856.126178167626,10421.930599618961,5904.201469716496,11419.13561724772,5055.377063929354,7227.208500693163,6642.779276837522,34553.35748726812,11109.606791665406,11167.794519280233,32900.13851257703,5252.421989334725,10438.854444189557,14785.97401999513,11033.079228486316,13239.720927187325,12215.901151983577,7650.711097081763,7697.798953576548,35221.44550449743,445.91759216574974,33667.347645702685,37805.54084388564,1231.8177085632087,28599.361634795274,9897.777446645949,6321.08113794975,9810.544873196002,8330.88376091493,7967.683189536012,7254.967783248747,1871.003449410462,8695.064076629213,10128.778113717166,8819.456393259821,12907.822772512132,6293.160738827977,10739.647850544174,-754.7777673823985,13438.446384105904,11906.557096198561,12694.04755094743,26755.482191881823,2215.328103722957,34514.613780043306,4243.096460384528,2385.0832125180386,3305.464621320223,9662.509670762836,9686.716614605077,32078.31733642409,32881.184721219775,9521.167199922242,38270.44955627255,13628.810508862403,6223.097993502681,7468.251166152015,14298.762424043878,6680.4742778711625,10844.548344401108,8449.264613897554,2315.7754152307916,2862.5087215018393,7401.472762606765,10754.945464261193,9795.030404844565,31229.33369751983,30082.46931375543,30628.069554722748,4216.46388675764,10631.79540223303,8088.8935155577165,6474.710486387699,26732.342343269436,30319.47367844336,32114.627029443214,5349.57667104826,35609.2947635521,6408.731598585866,3376.9329676145608,34624.29108745308,11603.688577367706,7198.894633571563,4882.340423206613,39294.10478666796,5271.972621558445,14364.105768624693,9317.198172033466,6701.688509692209,11983.669542863838,11850.792042450918,36297.175843000325,27603.166416824843,35184.71501840801,12794.168499955384,30751.036016954135,12952.53287835744,13035.205600273446,28962.966697071115,5527.484119434033,8556.525224706093,32094.295974488483,14010.511760989972,4111.67084346744,4566.687118268801,30035.43022369404,13929.68323154681,9222.105543005899,12011.547434708944,3932.155312517938,7254.85723351561,4620.963399867593,1769.2258548216087,38727.95557349546,14102.530593681833,3739.0350245012414,14578.008031084915,-1075.015862550139,15953.064112631233,36356.41937688783,11285.169490789769,6650.784317809241,7763.916836524204,12049.019668959405,4766.47391484254,40258.89671196145,10345.66026145231,10116.72009419412,1911.1683913545894,10732.578657755523,12190.891428153078,10144.11459378605,7129.7084194235285,11856.671515002836,29448.856549588465,8507.648516765377,595.095534319189,12121.285337485704,9866.211302175852,15721.181331309741,39146.82505776084,11005.584858499893,5856.527687867867,5694.743197365864,27591.632356345242,7072.066246784521,36168.5497187689,9602.704028749904,30797.085037772722,1285.2242876844539,8364.820212074988,9281.109274396567,7555.365491803121,10010.70630562997,13891.914160757417,11915.754376864254,24790.050742414885,3351.245047221304,10940.837339354453,7169.3422297464895,6299.446080643465,-138.7434175542694,15998.681917939964,13803.023774542195,2851.412705231811,37390.716045207446,28013.18607727937,15861.502205979083,32530.052948663786,4761.4691224337475,9727.86822846441,33377.74288233934,8785.30761073847,9138.56346249035,32736.012816520128,6107.386764525589,9502.62885126346,902.4998364793391,15029.73773117585,13169.77402535416,7212.189904333201,7524.920076147391,594.2735532774041,14273.599776286334,-275.7325549535781,26027.769949164274,14565.881117671634,4355.8036074353695,8652.765800234014,29353.89625208545,5883.397186548682,25647.517335301043,7614.161254557828,30275.968982596733,11540.177855068407,5273.080448657001,11011.270278214743,37811.0114483326,-1624.1502288237698,14493.178303554498,10651.53069686957,28455.55919809527,11194.590266772047,32870.661427296625,11547.100769619065,3367.812695003693,12885.436167625368,8151.353444800541,13026.406647379696,386.64067746726687,9643.891486264627,12837.901900432204,33976.861085662575,5140.769063582506,10483.75809531617,11906.536871209599,9598.875406905707,5666.606068926394,168.26276801621316,16165.999486755089,3138.916251396131,35235.39096781927,8588.141449201852,36874.17581466303,8144.580025934785,2812.293098587852,16131.030845607875,10910.411041702835,4409.727818477764,10267.924131414173,2315.7754152307916,2648.351620403664,10393.713095049807,11712.57394920629,11657.428723981484,3279.8763883108004,28205.766453560547,708.0155382193079,9136.793794165718,35407.35820503719,-2802.1991285010226,2701.382819615401,13458.06793973401,9400.749726022681,32563.297005571414,1534.4064659877258,4337.419185152379,9877.328143892377,29456.34620637167,3011.0367896618372,7713.6637065674295,5010.909513395949,7922.886430785889,6957.547618992077,24666.48968986583,2765.166296141306,29588.433320636836,12728.317569600029,6907.374152373064,15128.499432837085,36408.22484821014,8968.552298683346,-7.710921629042787,27685.656187275436,11298.122762452873,11416.617844387016,17110.051250022036,11044.829227049373,12272.774032829784,10992.541734417293,7867.842281820558,9003.194361905076,7600.146864930819,1528.5923939667045,13301.942241866058,37987.27098403462,39639.67284483505,8992.615204596987,10869.987432707323,14226.268513045881,11154.29948237414,6152.294434499527,10685.006675467277,9446.55065880351,977.9790905085665,10363.652640161734,3941.54876843662,38039.48744587782,8400.513050420963,2761.18093806893,33607.28331870615,14383.841063261232,11073.994708558199,29064.14911105094,5971.99149125196,11107.094930953459,10663.119832972632,3478.083171440563,31723.671791683617,34811.48696489789,15519.574088396042,1681.720763638572,10292.010797943414,12759.567887659945,8870.005239788345,15448.196224920332,11826.477540657466,8483.644845784307,9635.494240852,33658.17210703445,30028.619551891697,7377.286805734999,-1986.9963893314198,1990.963743164115,17248.29195463011,8428.955107690235,30659.259259483446,8093.209613338015,3355.1727658441596,5799.667469770622,14593.406655944218,10654.162208738704,3977.578527547732,5939.660563779475,6180.446348103884,2323.0150906748095,25293.757451333495,14481.48074013423,4138.938631239591,5929.282224335868,9547.457727265632,8198.636625114552,28664.441282524273,6193.3287350432,4940.47657453566,12429.285611146315,7294.718241312647,7860.689638704262,8931.866452795504,5915.540305256985,5425.761193261151,11371.676650666988,29719.223848725367,9075.913985751351,5283.83520987778,11414.201147785781,1826.9473437904053,12392.184755890255,31150.746929844947,1977.90585184886,13205.090123356207,11099.277615696508,10971.789298917054,6199.484806162756,10401.101675573544,17571.533541118926,10715.885865509015,34591.93506471053,8013.547530009557,32499.744616920227,2917.938750915746,10775.164566124444],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"Cargos reales\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Cargos predecidos\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"Cargos reales\"}},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Cargos predecidos\"}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Conjunto de entrenamiento\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Conjunto de prueba\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"shapes\":[{\"line\":{\"dash\":\"dot\"},\"type\":\"line\",\"x0\":-2260.5220079138853,\"x1\":41428.325516393495,\"y0\":-2260.5220079138853,\"y1\":41428.325516393495},{\"line\":{\"dash\":\"dot\"},\"type\":\"line\",\"x0\":-2802.1991285010226,\"x1\":40258.89671196145,\"xref\":\"x2\",\"y0\":-2802.1991285010226,\"y1\":40258.89671196145,\"yref\":\"y2\"}],\"title\":{\"text\":\"Regresi\\u00f3n lineal\"},\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('df23babc-5ae6-4ddc-9d69-0d89fbdbdd35');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En la gráfica se observan 3 grupos distintos de puntos, esto nos indica dos cosas: \n",
        "\n",
        "1. Al menos un supuesto del algoritmo no se cumple.\n",
        "2. Existen 3 o 4 grupos en los datos. Posiblemente con ingeniería de características se pueda mejorar el resultado."
      ],
      "metadata": {
        "id": "-owHOIlQj-1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Primer Red Neuronal"
      ],
      "metadata": {
        "id": "BDGKZfYhCa7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La arquitectura de la primera red neuronal será muy similar a una regresión lineal, la única diferencia radica en el algoritmo de optimización de la función de pérdida."
      ],
      "metadata": {
        "id": "Ccm5QUM_tRJS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparación de los datos"
      ],
      "metadata": {
        "id": "5ACuSJgItM-G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ocuparemos el mismo transformador que se creo y ajustó cuando usamos la regresión lineal. Los datos *preparados* nos van a servir para ambas redes neuronales."
      ],
      "metadata": {
        "id": "HqpMILowtPK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X_train_transformed\n",
        "X_train_tr = transformer.transform(X_train)\n",
        "X_test_tr = transformer.transform(X_test)"
      ],
      "metadata": {
        "id": "tKkElOVUm4Vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algoritmo/arquitectura"
      ],
      "metadata": {
        "id": "Ob7LhoEfstFu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La arquitectura sólo consiste en una capa de entrada y una de salida con activación lineal. \n",
        "\n",
        "Al inicio ponemos un par de semillas para asegurarnos que los resultados se puedan replicar."
      ],
      "metadata": {
        "id": "BjreDrilumtY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(10)\n",
        "tf.random.set_seed(10)\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "     layers.Input([X_train_tr.shape[1]]),\n",
        "    #  layers.LayerNormalization(input_shape = [X_train_tr.shape[1]]),\n",
        "     layers.Dense(1, activation='linear')\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "Xic5bz16nbWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilamos el modelo indicando que el optimizador es RMSprop, la función de pérdida el error cuadrático medio e indicamos que monitoreé el error absoluto medio."
      ],
      "metadata": {
        "id": "rSl0QvNEu2S8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer = keras.optimizers.RMSprop(learning_rate=.1), \n",
        "    loss = 'mse', \n",
        "    metrics = ['mae'], \n",
        ")"
      ],
      "metadata": {
        "id": "NsMYYB4QpDp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlmylUUjqGf5",
        "outputId": "b008e925-f05f-4e11-b63b-d28a77c4b663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense (Dense)               (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 9\n",
            "Trainable params: 9\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos el número de épocas, la ruta donde se deben guardar los checkpoints y creamos el callback que se encargará de guardar los checkpoints."
      ],
      "metadata": {
        "id": "M5NeBPZAvKjj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 200\n",
        "\n",
        "weights_filepath = os.path.join(checkpoint_filepath, 'model_1')\n",
        "\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath = weights_filepath, \n",
        "    save_best_only = True, \n",
        "    save_weights_only = True, \n",
        "    monitor = 'val_mae', \n",
        "    mode = 'min'\n",
        ")"
      ],
      "metadata": {
        "id": "yCog_zC0tCHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El callback para los checkpoints sólo guardará los pesos (```save_weights_only = True```) del mejor modelo (```save_best_only = True```) considerando el error absoluto medio del conjunto de validación (```monitor = 'val_mae'```) más pequeño (```mode = 'min'```) hasta el momento."
      ],
      "metadata": {
        "id": "lySolqICvVtD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento"
      ],
      "metadata": {
        "id": "yGCdX2_qs-9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ya que esta definida la arquitectura del modelo, se compilo y se crearon los callbacks necesarios, usamos la función ```fit``` para entrenar la red neuronal."
      ],
      "metadata": {
        "id": "jTKapDAMwByU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    X_train_tr, y_train, \n",
        "    epochs=epochs, \n",
        "    validation_split=.2, \n",
        "    callbacks=[model_checkpoint_callback]\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5JFd_alqLJw",
        "outputId": "0633955f-4b92-489b-f5f0-c3b842bc6c6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "24/24 [==============================] - 1s 9ms/step - loss: 337746464.0000 - mae: 13571.1084 - val_loss: 298195872.0000 - val_mae: 12156.6143\n",
            "Epoch 2/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 332252864.0000 - mae: 13382.3047 - val_loss: 293513088.0000 - val_mae: 11980.5703\n",
            "Epoch 3/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 327301600.0000 - mae: 13207.8291 - val_loss: 288939232.0000 - val_mae: 11806.2158\n",
            "Epoch 4/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 322428896.0000 - mae: 13035.0986 - val_loss: 284512512.0000 - val_mae: 11635.0654\n",
            "Epoch 5/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 317614176.0000 - mae: 12863.3105 - val_loss: 280186688.0000 - val_mae: 11465.4971\n",
            "Epoch 6/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 312897440.0000 - mae: 12691.9795 - val_loss: 275836704.0000 - val_mae: 11292.5635\n",
            "Epoch 7/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 308228000.0000 - mae: 12520.0400 - val_loss: 271586528.0000 - val_mae: 11121.1260\n",
            "Epoch 8/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 303601120.0000 - mae: 12348.9131 - val_loss: 267353712.0000 - val_mae: 10947.8848\n",
            "Epoch 9/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 299092640.0000 - mae: 12176.4707 - val_loss: 263219632.0000 - val_mae: 10776.0918\n",
            "Epoch 10/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 294671104.0000 - mae: 12005.5869 - val_loss: 259263904.0000 - val_mae: 10609.2969\n",
            "Epoch 11/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 290273984.0000 - mae: 11838.3447 - val_loss: 255204768.0000 - val_mae: 10435.9316\n",
            "Epoch 12/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 285852000.0000 - mae: 11668.8047 - val_loss: 251243152.0000 - val_mae: 10265.2881\n",
            "Epoch 13/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 281561856.0000 - mae: 11503.1377 - val_loss: 247343264.0000 - val_mae: 10095.9219\n",
            "Epoch 14/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 277302368.0000 - mae: 11340.3721 - val_loss: 243469552.0000 - val_mae: 9927.3262\n",
            "Epoch 15/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 273095040.0000 - mae: 11175.8975 - val_loss: 239711072.0000 - val_mae: 9765.7227\n",
            "Epoch 16/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 269027648.0000 - mae: 11020.0918 - val_loss: 236072592.0000 - val_mae: 9611.0488\n",
            "Epoch 17/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 265007264.0000 - mae: 10869.6162 - val_loss: 232387872.0000 - val_mae: 9453.4336\n",
            "Epoch 18/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 261012000.0000 - mae: 10717.5518 - val_loss: 228795104.0000 - val_mae: 9301.0850\n",
            "Epoch 19/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 257138640.0000 - mae: 10575.6113 - val_loss: 225394512.0000 - val_mae: 9158.1865\n",
            "Epoch 20/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 253347104.0000 - mae: 10438.1582 - val_loss: 222032944.0000 - val_mae: 9016.1025\n",
            "Epoch 21/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 249638240.0000 - mae: 10301.2852 - val_loss: 218606528.0000 - val_mae: 8872.3457\n",
            "Epoch 22/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 245880752.0000 - mae: 10165.2832 - val_loss: 215314496.0000 - val_mae: 8738.1611\n",
            "Epoch 23/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 242281744.0000 - mae: 10033.2529 - val_loss: 212192128.0000 - val_mae: 8612.1133\n",
            "Epoch 24/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 238762448.0000 - mae: 9905.8184 - val_loss: 209016304.0000 - val_mae: 8485.0088\n",
            "Epoch 25/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 235271216.0000 - mae: 9778.1426 - val_loss: 205905088.0000 - val_mae: 8360.9102\n",
            "Epoch 26/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 231797856.0000 - mae: 9652.0586 - val_loss: 202884048.0000 - val_mae: 8242.5137\n",
            "Epoch 27/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 228442768.0000 - mae: 9532.1426 - val_loss: 199911008.0000 - val_mae: 8127.6929\n",
            "Epoch 28/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 225145296.0000 - mae: 9414.8447 - val_loss: 196984512.0000 - val_mae: 8017.1714\n",
            "Epoch 29/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 221861488.0000 - mae: 9303.0693 - val_loss: 194104032.0000 - val_mae: 7912.0298\n",
            "Epoch 30/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 218699904.0000 - mae: 9195.7998 - val_loss: 191381328.0000 - val_mae: 7814.2778\n",
            "Epoch 31/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 215552528.0000 - mae: 9092.1777 - val_loss: 188574896.0000 - val_mae: 7717.3179\n",
            "Epoch 32/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 212470864.0000 - mae: 8989.3398 - val_loss: 185988496.0000 - val_mae: 7630.9692\n",
            "Epoch 33/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 209527872.0000 - mae: 8890.0312 - val_loss: 183402960.0000 - val_mae: 7545.6582\n",
            "Epoch 34/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 206613216.0000 - mae: 8796.3672 - val_loss: 180828608.0000 - val_mae: 7464.4316\n",
            "Epoch 35/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 203743824.0000 - mae: 8706.6543 - val_loss: 178416288.0000 - val_mae: 7396.0273\n",
            "Epoch 36/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 200980096.0000 - mae: 8620.5332 - val_loss: 176027984.0000 - val_mae: 7334.6641\n",
            "Epoch 37/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 198275920.0000 - mae: 8537.3447 - val_loss: 173700256.0000 - val_mae: 7279.7026\n",
            "Epoch 38/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 195529376.0000 - mae: 8456.5908 - val_loss: 171224736.0000 - val_mae: 7227.1079\n",
            "Epoch 39/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 192877456.0000 - mae: 8377.6670 - val_loss: 169109616.0000 - val_mae: 7186.7500\n",
            "Epoch 40/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 190301712.0000 - mae: 8306.1943 - val_loss: 166878688.0000 - val_mae: 7146.2002\n",
            "Epoch 41/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 187785056.0000 - mae: 8239.0449 - val_loss: 164769168.0000 - val_mae: 7111.8535\n",
            "Epoch 42/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 185340080.0000 - mae: 8175.9688 - val_loss: 162680160.0000 - val_mae: 7081.7788\n",
            "Epoch 43/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 182943360.0000 - mae: 8115.8828 - val_loss: 160742528.0000 - val_mae: 7058.4854\n",
            "Epoch 44/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 180641968.0000 - mae: 8060.2734 - val_loss: 158793024.0000 - val_mae: 7045.9351\n",
            "Epoch 45/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 178365232.0000 - mae: 8009.7197 - val_loss: 156969216.0000 - val_mae: 7039.3364\n",
            "Epoch 46/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 176264560.0000 - mae: 7964.7178 - val_loss: 155198624.0000 - val_mae: 7035.4741\n",
            "Epoch 47/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 174081616.0000 - mae: 7925.7397 - val_loss: 153367424.0000 - val_mae: 7037.8252\n",
            "Epoch 48/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 172026464.0000 - mae: 7894.6338 - val_loss: 151714944.0000 - val_mae: 7045.3140\n",
            "Epoch 49/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 170086928.0000 - mae: 7864.4478 - val_loss: 150150928.0000 - val_mae: 7054.4976\n",
            "Epoch 50/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 168148048.0000 - mae: 7844.1323 - val_loss: 148687280.0000 - val_mae: 7065.8130\n",
            "Epoch 51/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 166333280.0000 - mae: 7823.6128 - val_loss: 147155664.0000 - val_mae: 7079.7891\n",
            "Epoch 52/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 164534080.0000 - mae: 7815.0962 - val_loss: 145758272.0000 - val_mae: 7097.2573\n",
            "Epoch 53/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 162836016.0000 - mae: 7808.7104 - val_loss: 144400160.0000 - val_mae: 7118.3921\n",
            "Epoch 54/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 161214832.0000 - mae: 7814.8638 - val_loss: 143174176.0000 - val_mae: 7144.7075\n",
            "Epoch 55/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 159570352.0000 - mae: 7814.9741 - val_loss: 141857008.0000 - val_mae: 7179.3125\n",
            "Epoch 56/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 157989024.0000 - mae: 7827.6597 - val_loss: 140674528.0000 - val_mae: 7212.5371\n",
            "Epoch 57/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 156499888.0000 - mae: 7844.9492 - val_loss: 139547232.0000 - val_mae: 7248.4102\n",
            "Epoch 58/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 155046080.0000 - mae: 7865.5703 - val_loss: 138427824.0000 - val_mae: 7290.9131\n",
            "Epoch 59/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 153651664.0000 - mae: 7893.8389 - val_loss: 137402576.0000 - val_mae: 7339.9399\n",
            "Epoch 60/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 152271456.0000 - mae: 7924.3174 - val_loss: 136405760.0000 - val_mae: 7392.1729\n",
            "Epoch 61/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 150998784.0000 - mae: 7960.8662 - val_loss: 135468608.0000 - val_mae: 7445.0020\n",
            "Epoch 62/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 149812400.0000 - mae: 8006.5630 - val_loss: 134686848.0000 - val_mae: 7496.5791\n",
            "Epoch 63/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 148738144.0000 - mae: 8049.6353 - val_loss: 133901032.0000 - val_mae: 7556.5811\n",
            "Epoch 64/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 147637616.0000 - mae: 8092.3555 - val_loss: 133146976.0000 - val_mae: 7620.6309\n",
            "Epoch 65/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 146642576.0000 - mae: 8141.7949 - val_loss: 132524760.0000 - val_mae: 7677.9355\n",
            "Epoch 66/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 145743808.0000 - mae: 8191.5703 - val_loss: 131914232.0000 - val_mae: 7738.5337\n",
            "Epoch 67/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 144839136.0000 - mae: 8237.6494 - val_loss: 131296320.0000 - val_mae: 7807.1621\n",
            "Epoch 68/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 143999344.0000 - mae: 8291.9629 - val_loss: 130807928.0000 - val_mae: 7866.9297\n",
            "Epoch 69/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 143218640.0000 - mae: 8334.9199 - val_loss: 130311832.0000 - val_mae: 7931.2905\n",
            "Epoch 70/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 142507520.0000 - mae: 8390.4863 - val_loss: 129905400.0000 - val_mae: 7987.8018\n",
            "Epoch 71/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 141842400.0000 - mae: 8424.2959 - val_loss: 129466848.0000 - val_mae: 8053.3267\n",
            "Epoch 72/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 141191504.0000 - mae: 8480.3574 - val_loss: 129105952.0000 - val_mae: 8112.8564\n",
            "Epoch 73/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 140577280.0000 - mae: 8525.7979 - val_loss: 128810528.0000 - val_mae: 8165.8994\n",
            "Epoch 74/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 140058080.0000 - mae: 8578.4727 - val_loss: 128515232.0000 - val_mae: 8223.7637\n",
            "Epoch 75/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 139460384.0000 - mae: 8626.0586 - val_loss: 128208240.0000 - val_mae: 8293.3242\n",
            "Epoch 76/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 138930128.0000 - mae: 8683.1768 - val_loss: 127983816.0000 - val_mae: 8350.5557\n",
            "Epoch 77/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 138505680.0000 - mae: 8731.8184 - val_loss: 127801304.0000 - val_mae: 8402.9434\n",
            "Epoch 78/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 138061776.0000 - mae: 8767.1621 - val_loss: 127623880.0000 - val_mae: 8462.5566\n",
            "Epoch 79/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 137704720.0000 - mae: 8816.7529 - val_loss: 127498608.0000 - val_mae: 8511.1094\n",
            "Epoch 80/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 137386272.0000 - mae: 8862.5537 - val_loss: 127398576.0000 - val_mae: 8556.0674\n",
            "Epoch 81/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 137074160.0000 - mae: 8892.5820 - val_loss: 127306952.0000 - val_mae: 8608.0840\n",
            "Epoch 82/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 136805872.0000 - mae: 8944.6689 - val_loss: 127251640.0000 - val_mae: 8644.9561\n",
            "Epoch 83/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 136604848.0000 - mae: 8974.9248 - val_loss: 127202056.0000 - val_mae: 8684.8076\n",
            "Epoch 84/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 136376688.0000 - mae: 9004.8057 - val_loss: 127166184.0000 - val_mae: 8727.9453\n",
            "Epoch 85/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 136155104.0000 - mae: 9037.1260 - val_loss: 127143192.0000 - val_mae: 8771.9365\n",
            "Epoch 86/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135989888.0000 - mae: 9073.5459 - val_loss: 127135768.0000 - val_mae: 8804.7021\n",
            "Epoch 87/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135848288.0000 - mae: 9097.9170 - val_loss: 127134112.0000 - val_mae: 8835.4111\n",
            "Epoch 88/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135721152.0000 - mae: 9118.2451 - val_loss: 127142648.0000 - val_mae: 8867.5342\n",
            "Epoch 89/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 135576672.0000 - mae: 9141.9727 - val_loss: 127169424.0000 - val_mae: 8908.3389\n",
            "Epoch 90/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135462400.0000 - mae: 9181.5342 - val_loss: 127183088.0000 - val_mae: 8931.5400\n",
            "Epoch 91/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135376640.0000 - mae: 9199.3896 - val_loss: 127198640.0000 - val_mae: 8954.2178\n",
            "Epoch 92/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135314336.0000 - mae: 9211.1963 - val_loss: 127212144.0000 - val_mae: 8973.2959\n",
            "Epoch 93/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 135245664.0000 - mae: 9237.3047 - val_loss: 127231304.0000 - val_mae: 8992.3174\n",
            "Epoch 94/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 135194400.0000 - mae: 9249.1367 - val_loss: 127233104.0000 - val_mae: 9001.0029\n",
            "Epoch 95/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135144720.0000 - mae: 9258.3633 - val_loss: 127257184.0000 - val_mae: 9021.5742\n",
            "Epoch 96/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 135081600.0000 - mae: 9276.2021 - val_loss: 127272760.0000 - val_mae: 9036.6230\n",
            "Epoch 97/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 135044320.0000 - mae: 9281.9111 - val_loss: 127285784.0000 - val_mae: 9049.2197\n",
            "Epoch 98/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134990640.0000 - mae: 9292.9492 - val_loss: 127328800.0000 - val_mae: 9072.9053\n",
            "Epoch 99/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134939472.0000 - mae: 9318.4248 - val_loss: 127359008.0000 - val_mae: 9090.2676\n",
            "Epoch 100/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134919136.0000 - mae: 9318.5244 - val_loss: 127357008.0000 - val_mae: 9094.8506\n",
            "Epoch 101/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134875264.0000 - mae: 9326.0977 - val_loss: 127377864.0000 - val_mae: 9107.8809\n",
            "Epoch 102/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134840336.0000 - mae: 9336.5352 - val_loss: 127394672.0000 - val_mae: 9118.8799\n",
            "Epoch 103/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134803440.0000 - mae: 9351.4189 - val_loss: 127384464.0000 - val_mae: 9120.4365\n",
            "Epoch 104/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134780896.0000 - mae: 9348.9248 - val_loss: 127405232.0000 - val_mae: 9132.2500\n",
            "Epoch 105/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134751792.0000 - mae: 9350.7246 - val_loss: 127424024.0000 - val_mae: 9143.1055\n",
            "Epoch 106/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134714624.0000 - mae: 9365.4443 - val_loss: 127448536.0000 - val_mae: 9155.2490\n",
            "Epoch 107/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134687264.0000 - mae: 9377.5498 - val_loss: 127472560.0000 - val_mae: 9167.2266\n",
            "Epoch 108/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134669248.0000 - mae: 9384.3193 - val_loss: 127469200.0000 - val_mae: 9170.5000\n",
            "Epoch 109/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134639616.0000 - mae: 9382.0938 - val_loss: 127494448.0000 - val_mae: 9181.9629\n",
            "Epoch 110/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134610448.0000 - mae: 9397.9609 - val_loss: 127490176.0000 - val_mae: 9184.1279\n",
            "Epoch 111/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134587008.0000 - mae: 9397.6514 - val_loss: 127480352.0000 - val_mae: 9185.7021\n",
            "Epoch 112/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134577776.0000 - mae: 9397.4014 - val_loss: 127486816.0000 - val_mae: 9191.6904\n",
            "Epoch 113/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134541968.0000 - mae: 9403.3057 - val_loss: 127475056.0000 - val_mae: 9192.3906\n",
            "Epoch 114/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134534608.0000 - mae: 9396.1562 - val_loss: 127456920.0000 - val_mae: 9193.3271\n",
            "Epoch 115/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134506544.0000 - mae: 9398.9775 - val_loss: 127445424.0000 - val_mae: 9193.9111\n",
            "Epoch 116/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134491664.0000 - mae: 9408.7910 - val_loss: 127436976.0000 - val_mae: 9195.6641\n",
            "Epoch 117/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134466912.0000 - mae: 9407.6680 - val_loss: 127420960.0000 - val_mae: 9195.3193\n",
            "Epoch 118/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134446032.0000 - mae: 9407.0254 - val_loss: 127400416.0000 - val_mae: 9193.4111\n",
            "Epoch 119/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134427008.0000 - mae: 9401.3984 - val_loss: 127388936.0000 - val_mae: 9195.0205\n",
            "Epoch 120/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134406928.0000 - mae: 9405.8379 - val_loss: 127373424.0000 - val_mae: 9194.6621\n",
            "Epoch 121/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134386208.0000 - mae: 9412.4355 - val_loss: 127351000.0000 - val_mae: 9192.3701\n",
            "Epoch 122/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134362928.0000 - mae: 9404.6230 - val_loss: 127317984.0000 - val_mae: 9187.5654\n",
            "Epoch 123/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134355520.0000 - mae: 9403.8799 - val_loss: 127312936.0000 - val_mae: 9189.9326\n",
            "Epoch 124/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134330384.0000 - mae: 9392.7197 - val_loss: 127305424.0000 - val_mae: 9191.7715\n",
            "Epoch 125/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134306608.0000 - mae: 9397.2041 - val_loss: 127318024.0000 - val_mae: 9198.8672\n",
            "Epoch 126/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134286608.0000 - mae: 9402.8271 - val_loss: 127304344.0000 - val_mae: 9199.0410\n",
            "Epoch 127/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134267312.0000 - mae: 9405.2129 - val_loss: 127266840.0000 - val_mae: 9192.7012\n",
            "Epoch 128/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134254256.0000 - mae: 9403.1406 - val_loss: 127252048.0000 - val_mae: 9192.7314\n",
            "Epoch 129/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134227968.0000 - mae: 9407.4473 - val_loss: 127237544.0000 - val_mae: 9192.3115\n",
            "Epoch 130/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134205400.0000 - mae: 9399.8799 - val_loss: 127213904.0000 - val_mae: 9190.0332\n",
            "Epoch 131/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134181664.0000 - mae: 9388.2471 - val_loss: 127214952.0000 - val_mae: 9194.3799\n",
            "Epoch 132/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134169056.0000 - mae: 9398.8008 - val_loss: 127204920.0000 - val_mae: 9195.2070\n",
            "Epoch 133/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134149080.0000 - mae: 9397.3555 - val_loss: 127193688.0000 - val_mae: 9195.8896\n",
            "Epoch 134/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134122840.0000 - mae: 9397.0771 - val_loss: 127217216.0000 - val_mae: 9205.6748\n",
            "Epoch 135/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134105376.0000 - mae: 9410.5645 - val_loss: 127192752.0000 - val_mae: 9202.7324\n",
            "Epoch 136/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134089824.0000 - mae: 9402.6387 - val_loss: 127189080.0000 - val_mae: 9205.5400\n",
            "Epoch 137/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134059856.0000 - mae: 9409.7549 - val_loss: 127170640.0000 - val_mae: 9204.7109\n",
            "Epoch 138/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134040360.0000 - mae: 9411.9473 - val_loss: 127141912.0000 - val_mae: 9200.8604\n",
            "Epoch 139/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 134022392.0000 - mae: 9411.0186 - val_loss: 127107736.0000 - val_mae: 9195.4600\n",
            "Epoch 140/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 134002272.0000 - mae: 9396.9014 - val_loss: 127112352.0000 - val_mae: 9199.9883\n",
            "Epoch 141/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133984768.0000 - mae: 9401.0352 - val_loss: 127112456.0000 - val_mae: 9203.6533\n",
            "Epoch 142/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133961312.0000 - mae: 9401.5537 - val_loss: 127124320.0000 - val_mae: 9210.5869\n",
            "Epoch 143/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133942472.0000 - mae: 9404.8779 - val_loss: 127138544.0000 - val_mae: 9217.6904\n",
            "Epoch 144/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133921976.0000 - mae: 9425.0645 - val_loss: 127118160.0000 - val_mae: 9216.4219\n",
            "Epoch 145/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133903840.0000 - mae: 9418.7871 - val_loss: 127086888.0000 - val_mae: 9212.7256\n",
            "Epoch 146/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133889344.0000 - mae: 9403.9570 - val_loss: 127064144.0000 - val_mae: 9209.9561\n",
            "Epoch 147/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133860064.0000 - mae: 9403.4082 - val_loss: 127053904.0000 - val_mae: 9210.8643\n",
            "Epoch 148/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133842232.0000 - mae: 9408.5361 - val_loss: 127043560.0000 - val_mae: 9212.0703\n",
            "Epoch 149/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133823632.0000 - mae: 9408.2578 - val_loss: 127037840.0000 - val_mae: 9214.5781\n",
            "Epoch 150/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133801248.0000 - mae: 9415.3008 - val_loss: 127026000.0000 - val_mae: 9215.8887\n",
            "Epoch 151/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133785072.0000 - mae: 9411.7363 - val_loss: 127009184.0000 - val_mae: 9215.0225\n",
            "Epoch 152/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133764176.0000 - mae: 9409.0996 - val_loss: 126992344.0000 - val_mae: 9214.9609\n",
            "Epoch 153/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133740424.0000 - mae: 9417.4590 - val_loss: 126969136.0000 - val_mae: 9213.1572\n",
            "Epoch 154/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133724400.0000 - mae: 9415.2256 - val_loss: 126956000.0000 - val_mae: 9213.6924\n",
            "Epoch 155/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133701224.0000 - mae: 9421.7070 - val_loss: 126928264.0000 - val_mae: 9210.8174\n",
            "Epoch 156/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133678872.0000 - mae: 9410.5059 - val_loss: 126930768.0000 - val_mae: 9215.1660\n",
            "Epoch 157/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133666080.0000 - mae: 9410.3447 - val_loss: 126899200.0000 - val_mae: 9211.2383\n",
            "Epoch 158/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133647904.0000 - mae: 9400.8516 - val_loss: 126892328.0000 - val_mae: 9212.7842\n",
            "Epoch 159/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133622144.0000 - mae: 9405.3613 - val_loss: 126878928.0000 - val_mae: 9212.9697\n",
            "Epoch 160/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133604376.0000 - mae: 9408.8623 - val_loss: 126870072.0000 - val_mae: 9214.3945\n",
            "Epoch 161/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133588568.0000 - mae: 9412.1123 - val_loss: 126865368.0000 - val_mae: 9217.1543\n",
            "Epoch 162/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133570672.0000 - mae: 9411.6533 - val_loss: 126844216.0000 - val_mae: 9215.2500\n",
            "Epoch 163/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133549576.0000 - mae: 9409.4131 - val_loss: 126820256.0000 - val_mae: 9212.6426\n",
            "Epoch 164/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133522112.0000 - mae: 9407.6787 - val_loss: 126809712.0000 - val_mae: 9212.7490\n",
            "Epoch 165/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133511168.0000 - mae: 9402.9834 - val_loss: 126795560.0000 - val_mae: 9212.4258\n",
            "Epoch 166/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133491992.0000 - mae: 9416.5996 - val_loss: 126778552.0000 - val_mae: 9211.5762\n",
            "Epoch 167/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133477416.0000 - mae: 9396.1816 - val_loss: 126760200.0000 - val_mae: 9210.7451\n",
            "Epoch 168/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133445544.0000 - mae: 9410.4326 - val_loss: 126743224.0000 - val_mae: 9210.0469\n",
            "Epoch 169/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133433176.0000 - mae: 9407.2803 - val_loss: 126739848.0000 - val_mae: 9212.8994\n",
            "Epoch 170/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133411016.0000 - mae: 9408.0010 - val_loss: 126726032.0000 - val_mae: 9212.7529\n",
            "Epoch 171/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133388120.0000 - mae: 9402.2832 - val_loss: 126727648.0000 - val_mae: 9216.5371\n",
            "Epoch 172/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133373624.0000 - mae: 9410.3594 - val_loss: 126714224.0000 - val_mae: 9216.5771\n",
            "Epoch 173/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133353680.0000 - mae: 9408.7949 - val_loss: 126676656.0000 - val_mae: 9210.6113\n",
            "Epoch 174/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133332960.0000 - mae: 9404.8994 - val_loss: 126660256.0000 - val_mae: 9209.8008\n",
            "Epoch 175/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133315840.0000 - mae: 9405.2510 - val_loss: 126660552.0000 - val_mae: 9213.1621\n",
            "Epoch 176/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133292440.0000 - mae: 9399.6807 - val_loss: 126670296.0000 - val_mae: 9218.8984\n",
            "Epoch 177/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133275120.0000 - mae: 9411.2949 - val_loss: 126648792.0000 - val_mae: 9216.7988\n",
            "Epoch 178/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133259936.0000 - mae: 9410.1855 - val_loss: 126630304.0000 - val_mae: 9215.7578\n",
            "Epoch 179/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133238848.0000 - mae: 9409.4033 - val_loss: 126590152.0000 - val_mae: 9209.2158\n",
            "Epoch 180/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133219144.0000 - mae: 9397.9102 - val_loss: 126571000.0000 - val_mae: 9207.5840\n",
            "Epoch 181/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133196064.0000 - mae: 9395.7617 - val_loss: 126569264.0000 - val_mae: 9210.3896\n",
            "Epoch 182/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133183096.0000 - mae: 9393.9209 - val_loss: 126524632.0000 - val_mae: 9202.5322\n",
            "Epoch 183/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133158120.0000 - mae: 9396.0312 - val_loss: 126516248.0000 - val_mae: 9203.5420\n",
            "Epoch 184/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133136856.0000 - mae: 9392.4102 - val_loss: 126513488.0000 - val_mae: 9205.9473\n",
            "Epoch 185/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133116072.0000 - mae: 9392.2930 - val_loss: 126521232.0000 - val_mae: 9211.0234\n",
            "Epoch 186/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 133095272.0000 - mae: 9399.1611 - val_loss: 126517664.0000 - val_mae: 9213.7295\n",
            "Epoch 187/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133078720.0000 - mae: 9403.3086 - val_loss: 126509336.0000 - val_mae: 9214.9639\n",
            "Epoch 188/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133059696.0000 - mae: 9408.1670 - val_loss: 126505240.0000 - val_mae: 9217.1338\n",
            "Epoch 189/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133044432.0000 - mae: 9402.7715 - val_loss: 126491344.0000 - val_mae: 9216.6963\n",
            "Epoch 190/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133016392.0000 - mae: 9404.5713 - val_loss: 126475024.0000 - val_mae: 9215.7617\n",
            "Epoch 191/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133006232.0000 - mae: 9398.1299 - val_loss: 126467184.0000 - val_mae: 9217.3594\n",
            "Epoch 192/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 132981536.0000 - mae: 9406.5254 - val_loss: 126437880.0000 - val_mae: 9213.6680\n",
            "Epoch 193/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 132956776.0000 - mae: 9399.5381 - val_loss: 126424264.0000 - val_mae: 9213.9600\n",
            "Epoch 194/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 132948504.0000 - mae: 9400.0195 - val_loss: 126397624.0000 - val_mae: 9210.5000\n",
            "Epoch 195/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 132923872.0000 - mae: 9393.5225 - val_loss: 126383984.0000 - val_mae: 9210.5156\n",
            "Epoch 196/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 132901576.0000 - mae: 9400.1689 - val_loss: 126344736.0000 - val_mae: 9204.3770\n",
            "Epoch 197/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 132887232.0000 - mae: 9401.7402 - val_loss: 126323160.0000 - val_mae: 9202.4883\n",
            "Epoch 198/200\n",
            "24/24 [==============================] - 0s 3ms/step - loss: 132863168.0000 - mae: 9393.8145 - val_loss: 126318792.0000 - val_mae: 9204.9844\n",
            "Epoch 199/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 132843136.0000 - mae: 9390.8389 - val_loss: 126300560.0000 - val_mae: 9203.6914\n",
            "Epoch 200/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 132822472.0000 - mae: 9388.8994 - val_loss: 126271216.0000 - val_mae: 9199.8682\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(x = list(range(epochs)), y = history.history['mae'], name = 'entrenamiento')\n",
        ")\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(x = list(range(epochs)), y = history.history['val_mae'], name = 'validación')\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    hovermode = 'x unified', \n",
        "    legend_title = 'Conjunto de', \n",
        "    xaxis_title = 'Época', \n",
        "    yaxis_title = 'Error Absoluto Medio (MAE)', \n",
        "    title = 'MAE en cada época del entrenamiento<br><sup>Primera red neuronal</sup>', \n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "xZJqyyCarj1r",
        "outputId": "1303298e-0084-4934-fee3-87b615579b6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"08701556-5ac5-4e5c-b196-05509ce8a085\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"08701556-5ac5-4e5c-b196-05509ce8a085\")) {                    Plotly.newPlot(                        \"08701556-5ac5-4e5c-b196-05509ce8a085\",                        [{\"name\":\"entrenamiento\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199],\"y\":[13571.1083984375,13382.3046875,13207.8291015625,13035.0986328125,12863.310546875,12691.9794921875,12520.0400390625,12348.9130859375,12176.470703125,12005.5869140625,11838.3447265625,11668.8046875,11503.1376953125,11340.3720703125,11175.8974609375,11020.091796875,10869.6162109375,10717.5517578125,10575.611328125,10438.158203125,10301.28515625,10165.283203125,10033.2529296875,9905.818359375,9778.142578125,9652.05859375,9532.142578125,9414.8447265625,9303.0693359375,9195.7998046875,9092.177734375,8989.33984375,8890.03125,8796.3671875,8706.654296875,8620.533203125,8537.3447265625,8456.5908203125,8377.6669921875,8306.1943359375,8239.044921875,8175.96875,8115.8828125,8060.2734375,8009.7197265625,7964.7177734375,7925.73974609375,7894.6337890625,7864.44775390625,7844.13232421875,7823.61279296875,7815.09619140625,7808.71044921875,7814.86376953125,7814.97412109375,7827.65966796875,7844.94921875,7865.5703125,7893.8388671875,7924.3173828125,7960.8662109375,8006.56298828125,8049.63525390625,8092.35546875,8141.794921875,8191.5703125,8237.6494140625,8291.962890625,8334.919921875,8390.486328125,8424.2958984375,8480.357421875,8525.7978515625,8578.47265625,8626.05859375,8683.1767578125,8731.818359375,8767.162109375,8816.7529296875,8862.5537109375,8892.58203125,8944.6689453125,8974.9248046875,9004.8056640625,9037.1259765625,9073.5458984375,9097.9169921875,9118.2451171875,9141.97265625,9181.5341796875,9199.3896484375,9211.1962890625,9237.3046875,9249.13671875,9258.36328125,9276.2021484375,9281.9111328125,9292.94921875,9318.4248046875,9318.5244140625,9326.09765625,9336.53515625,9351.4189453125,9348.9248046875,9350.724609375,9365.4443359375,9377.5498046875,9384.3193359375,9382.09375,9397.9609375,9397.6513671875,9397.4013671875,9403.3056640625,9396.15625,9398.9775390625,9408.791015625,9407.66796875,9407.025390625,9401.3984375,9405.837890625,9412.435546875,9404.623046875,9403.8798828125,9392.7197265625,9397.2041015625,9402.8271484375,9405.212890625,9403.140625,9407.447265625,9399.8798828125,9388.2470703125,9398.80078125,9397.35546875,9397.0771484375,9410.564453125,9402.638671875,9409.7548828125,9411.947265625,9411.0185546875,9396.9013671875,9401.03515625,9401.5537109375,9404.8779296875,9425.064453125,9418.787109375,9403.95703125,9403.408203125,9408.5361328125,9408.2578125,9415.30078125,9411.736328125,9409.099609375,9417.458984375,9415.2255859375,9421.70703125,9410.505859375,9410.3447265625,9400.8515625,9405.361328125,9408.8623046875,9412.1123046875,9411.6533203125,9409.4130859375,9407.6787109375,9402.9833984375,9416.599609375,9396.181640625,9410.4326171875,9407.2802734375,9408.0009765625,9402.283203125,9410.359375,9408.794921875,9404.8994140625,9405.2509765625,9399.6806640625,9411.294921875,9410.185546875,9409.4033203125,9397.91015625,9395.76171875,9393.9208984375,9396.03125,9392.41015625,9392.29296875,9399.1611328125,9403.30859375,9408.1669921875,9402.771484375,9404.5712890625,9398.1298828125,9406.525390625,9399.5380859375,9400.01953125,9393.5224609375,9400.1689453125,9401.740234375,9393.814453125,9390.8388671875,9388.8994140625],\"type\":\"scatter\"},{\"name\":\"validaci\\u00f3n\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199],\"y\":[12156.6142578125,11980.5703125,11806.2158203125,11635.0654296875,11465.4970703125,11292.5634765625,11121.1259765625,10947.884765625,10776.091796875,10609.296875,10435.931640625,10265.2880859375,10095.921875,9927.326171875,9765.72265625,9611.048828125,9453.43359375,9301.0849609375,9158.1865234375,9016.1025390625,8872.345703125,8738.1611328125,8612.11328125,8485.0087890625,8360.91015625,8242.513671875,8127.69287109375,8017.17138671875,7912.02978515625,7814.27783203125,7717.31787109375,7630.96923828125,7545.658203125,7464.431640625,7396.02734375,7334.6640625,7279.70263671875,7227.10791015625,7186.75,7146.2001953125,7111.853515625,7081.77880859375,7058.4853515625,7045.93505859375,7039.33642578125,7035.47412109375,7037.8251953125,7045.31396484375,7054.49755859375,7065.81298828125,7079.7890625,7097.25732421875,7118.39208984375,7144.70751953125,7179.3125,7212.537109375,7248.41015625,7290.9130859375,7339.93994140625,7392.1728515625,7445.001953125,7496.5791015625,7556.5810546875,7620.630859375,7677.935546875,7738.53369140625,7807.162109375,7866.9296875,7931.29052734375,7987.8017578125,8053.32666015625,8112.8564453125,8165.8994140625,8223.763671875,8293.32421875,8350.5556640625,8402.943359375,8462.556640625,8511.109375,8556.0673828125,8608.083984375,8644.9560546875,8684.8076171875,8727.9453125,8771.9365234375,8804.7021484375,8835.4111328125,8867.5341796875,8908.3388671875,8931.5400390625,8954.2177734375,8973.2958984375,8992.3173828125,9001.0029296875,9021.57421875,9036.623046875,9049.2197265625,9072.9052734375,9090.267578125,9094.8505859375,9107.880859375,9118.8798828125,9120.4365234375,9132.25,9143.10546875,9155.2490234375,9167.2265625,9170.5,9181.962890625,9184.1279296875,9185.7021484375,9191.6904296875,9192.390625,9193.3271484375,9193.9111328125,9195.6640625,9195.3193359375,9193.4111328125,9195.0205078125,9194.662109375,9192.3701171875,9187.5654296875,9189.9326171875,9191.771484375,9198.8671875,9199.041015625,9192.701171875,9192.7314453125,9192.3115234375,9190.033203125,9194.3798828125,9195.20703125,9195.8896484375,9205.6748046875,9202.732421875,9205.5400390625,9204.7109375,9200.8603515625,9195.4599609375,9199.98828125,9203.6533203125,9210.5869140625,9217.6904296875,9216.421875,9212.7255859375,9209.9560546875,9210.8642578125,9212.0703125,9214.578125,9215.888671875,9215.0224609375,9214.9609375,9213.1572265625,9213.6923828125,9210.8173828125,9215.166015625,9211.23828125,9212.7841796875,9212.9697265625,9214.39453125,9217.154296875,9215.25,9212.642578125,9212.7490234375,9212.42578125,9211.576171875,9210.7451171875,9210.046875,9212.8994140625,9212.7529296875,9216.537109375,9216.5771484375,9210.611328125,9209.80078125,9213.162109375,9218.8984375,9216.798828125,9215.7578125,9209.2158203125,9207.583984375,9210.3896484375,9202.5322265625,9203.5419921875,9205.947265625,9211.0234375,9213.7294921875,9214.9638671875,9217.1337890625,9216.6962890625,9215.76171875,9217.359375,9213.66796875,9213.9599609375,9210.5,9210.515625,9204.376953125,9202.48828125,9204.984375,9203.69140625,9199.8681640625],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"hovermode\":\"x unified\",\"legend\":{\"title\":{\"text\":\"Conjunto de\"}},\"xaxis\":{\"title\":{\"text\":\"\\u00c9poca\"}},\"yaxis\":{\"title\":{\"text\":\"Error Absoluto Medio (MAE)\"}},\"title\":{\"text\":\"MAE en cada \\u00e9poca del entrenamiento<br><sup>Primera red neuronal</sup>\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('08701556-5ac5-4e5c-b196-05509ce8a085');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La gráfica muestra que el MAE llegó a un mínimo alrededor de la época 40 y luego el desempeño empeoró. Gracias a que agregamos el callback para guardar únicamente el mejor modelo basado en esta métrica, basta que lo carguemos con la función ```load_weights```."
      ],
      "metadata": {
        "id": "rqjmieF5xtpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(weights_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9e_GZ-e62Gm",
        "outputId": "2852773b-55d5-4976-8163-11e4e4804c33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6bc2e85610>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación"
      ],
      "metadata": {
        "id": "eBO1-tkuChQ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos la función ```predict``` para predecir usando el modelo que acabamos de entrenar."
      ],
      "metadata": {
        "id": "pAVPqecfylK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train_nn1 = model.predict(X_train_tr)[:, 0]\n",
        "y_pred_test_nn1 = model.predict(X_test_tr)[:, 0]"
      ],
      "metadata": {
        "id": "CaMHcouE7_DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Métricas"
      ],
      "metadata": {
        "id": "BKOPr_ZCCy_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    f\"R2 train: {r2_score(y_train, y_pred_train_nn1)}\", \n",
        "    f\"R2 test: {r2_score(y_test, y_pred_test_nn1)}\", \n",
        "    \"\",\n",
        "    f\"MSE train: {mean_squared_error(y_train, y_pred_train_nn1)}\", \n",
        "    f\"MSE test: {mean_squared_error(y_test, y_pred_test_nn1)}\", \n",
        "    \"\",\n",
        "    f\"MAE train: {mean_absolute_error(y_train, y_pred_train_nn1)}\", \n",
        "    f\"MAE test: {mean_absolute_error(y_test, y_pred_test_nn1)}\", \n",
        "    sep = '\\n'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gV4if7ut7V2v",
        "outputId": "b0535ad2-59e1-4d31-eee7-d7e09c822663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 train: -0.11405321987258676\n",
            "R2 test: -0.13159156212284318\n",
            "\n",
            "MSE train: 171122423.78878692\n",
            "MSE test: 146946083.4709567\n",
            "\n",
            "MAE train: 7761.346792911993\n",
            "MAE test: 7114.495580171448\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta ocasión, el coeficiente de determinación tuvo valores negativos, lo que nos indica que el modelo no sirve para predecir la variable de respuesta."
      ],
      "metadata": {
        "id": "hCnzhQ6wyxvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualización"
      ],
      "metadata": {
        "id": "O6iJ9GhSCkws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_evaluation(y_train, y_pred_train_nn1, y_test, y_pred_test_nn1, 'Primera red neuronal')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "0vvHsvgcgf5J",
        "outputId": "5a648c77-7347-49d9-a48a-1336d68b29d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"3e153416-e5a9-4168-9a6b-8a441a22aed6\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3e153416-e5a9-4168-9a6b-8a441a22aed6\")) {                    Plotly.newPlot(                        \"3e153416-e5a9-4168-9a6b-8a441a22aed6\",                        [{\"mode\":\"markers\",\"x\":[3167.45585,2689.4954,11576.13,16586.49771,6746.7425,5976.8311,5649.715,15161.5344,2007.945,19214.705530000003,16455.70785,10594.50155,27117.99378,4296.2712,7151.092,8410.04685,22331.5668,37165.1638,17468.9839,2494.022,6356.2707,11305.93455,12235.8392,14256.1928,13822.803,7173.35995,9140.951,1711.0268,5926.846,32108.662819999998,17942.106,43896.3763,8671.19125,6112.35295,15019.76005,11365.952,2709.24395,43254.41795,1263.249,3757.8448,20420.60465,2302.3,11658.11505,11394.06555,1967.0227,3353.4703,10269.46,18963.171919999997,31620.001060000002,30259.995560000003,6571.544,1632.56445,2755.02095,5934.3798,14319.031,19964.7463,6849.026,2498.4144,1631.6683,3766.8838,9625.92,3309.7926,37079.372,15359.1045,36124.5737,2842.76075,1622.1885,46599.1084,6948.7008,4500.33925,2128.43105,9095.06825,19673.335730000003,48824.45,8688.85885,3645.0894,2480.9791,2154.361,33307.5508,12643.3778,27375.90478,8556.907,37270.1512,6593.5083,9101.798,13019.16105,28868.6639,1621.3402,6571.02435,34828.654,4260.744000000001,8765.249,4040.55825,9875.6804,4337.7352,11356.6609,9290.1395,34439.8559,8891.1395,7162.0122,8026.6666,35160.13457,13555.0049,4719.73655,7222.78625,2534.39375,1719.4363,1635.73365,1725.5523,1824.2854,8342.90875,12829.4551,12979.358,4391.652,19933.458,7623.518,7640.3092,28101.33305,8442.667,2721.3208,2803.69785,12044.341999999999,1131.5066,11743.9341,5974.3847,2731.9122,26140.3603,4243.59005,4846.92015,28287.897660000002,20630.28351,2741.948,29186.48236,1261.859,6877.9801,14283.4594,4402.233,37742.5757,13217.0945,11163.568000000001,13451.122,26467.09737,58571.074479999996,13204.28565,1639.5631,2473.3341,48970.2476,9910.35985,3044.2133,60021.39897,41034.2214,9877.6077,8125.7845,2020.5523,25382.297000000002,34303.1672,7742.1098,10702.6424,8516.829,27322.733860000004,37701.8768,26125.674769999998,30184.9367,10107.2206,3070.8087,5152.134,4746.344,5910.944,4415.1588,14043.4767,16085.1275,8428.0693,39727.614,8283.6807,14210.53595,10355.641,12265.5069,6455.86265,11488.31695,4134.08245,9563.029,18804.7524,11856.4115,11264.541000000001,1631.8212,38792.6856,42560.4304,2150.469,7050.642,42112.2356,41949.2441,4462.7218,5484.4673,20234.85475,1877.9294,11085.5868,8219.2039,13126.67745,5227.98875,3268.84665,12479.70895,6250.435,28923.136919999997,2136.88225,13937.6665,36580.28216,6652.5288,3947.4131,19444.2658,12741.16745,1832.094,13770.0979,6113.23105,6198.7518,38511.6283,14382.70905,16450.8947,7133.9025,37829.7242,6406.4107,6933.24225,2137.6536,12404.8791,10096.97,26109.32905,3906.127,44423.803,12124.9924,1253.9360000000001,34166.273,45702.02235,13470.86,2632.992,38709.176,9182.17,1826.8429999999998,7243.8136,12430.95335,10118.424,4827.90495,5312.16985,3213.62205,4779.6023,4931.647,23082.95533,4433.9159,7985.815,33750.2918,48675.5177,33475.81715,9872.701,44260.7499,10601.412,2117.33885,11070.535,33900.653,2639.0429,5428.7277,11837.16,9301.89355,35491.64,47291.055,9566.9909,14410.9321,20878.78443,42983.4585,10796.35025,11884.04858,11244.3769,3279.86855,6796.86325,27037.9141,10848.1343,10560.4917,44202.6536,9288.0267,9620.3307,5458.04645,36149.4835,11381.3254,21595.38229,21880.82,1977.815,36837.467000000004,6640.54485,4347.02335,36219.40545,26926.5144,4435.0942,6128.79745,3597.5959999999995,1704.5681,16776.30405,2727.3951,7348.142,1242.816,45863.205,24671.66334,6186.1269999999995,19539.243000000002,2566.4707,49577.6624,39836.519,6402.29135,8827.2099,17904.52705,6272.4772,6500.2359,2257.47525,2155.6815,2362.22905,1815.8759,1875.344,9724.53,11538.421,2219.4451,2156.7518,42969.8527,1708.92575,9722.7695,15820.698999999999,11842.442,8582.3023,33907.547999999995,38998.546,7345.7266,4463.2051,15828.821730000001,2867.1196,5438.7491,4564.19145,46718.16325,12592.5345,23563.016180000002,6185.3208,10976.24575,27941.28758,4189.1131,41097.16175,36197.699,15817.9857,26018.95052,3857.75925,3490.5491,6238.298000000001,2396.0959,7419.4779,8413.46305,1880.07,7050.0213,4518.82625,11552.903999999999,10982.5013,4673.3922,23807.2406,20149.3229,12949.1554,4753.6368,27000.98473,24180.9335,7731.4271,7749.1564,9644.2525,42211.1382,6940.90985,24869.8368,13457.9608,12224.35085,8551.347,22478.6,12925.886,5012.471,9414.92,14988.431999999999,4618.0799,2730.10785,19749.383380000003,3556.9223,39241.442,17043.3414,8277.523000000001,4830.63,7518.02535,10214.636,18767.7377,1242.26,4536.259,8965.79575,34617.84065,16796.41194,11848.141000000001,8932.084,20009.63365,1759.338,45008.9555,37465.34375,46661.4424,11881.9696,5972.378000000001,1252.407,3875.7341,1743.214,3981.9768,13635.6379,10381.4787,9048.0273,3704.3545,5209.57885,7265.7025,47462.894,11674.13,1737.376,2775.19215,3693.428,5028.1466,1639.5631,9058.7303,10226.2842,4529.477,1629.8335,12815.44495,24915.04626,14692.66935,7986.47525,8017.06115,12105.32,3554.203,10461.9794,5615.369000000001,4795.6568,19515.5416,2103.08,4349.462,12523.6048,10072.05505,1136.3994,21677.28345,1704.70015,21774.32215,13063.883,5116.5004,10825.2537,3353.284,4149.736,13405.3903,10977.2063,14349.8544,11741.726,44641.1974,4762.329000000001,7371.772,3176.8159,2850.68375,40273.6455,17560.37975,4433.3877,1526.3120000000001,1702.4553,6373.55735,10942.13205,8547.6913,8604.48365,3176.2877,11150.78,4527.18295,12233.828000000001,9850.431999999999,7196.866999999999,11729.6795,21223.6758,10435.06525,5272.1758,2250.8352,33732.6867,14358.36437,8233.0975,17496.306,2789.0574,14901.5167,5124.1887,8027.968000000001,13143.33665,43578.9394,10807.4863,8522.003,5400.9805,9447.3824,10601.63225,4237.12655,14001.2867,1615.7667,11840.77505,40182.246,15230.32405,4877.98105,28340.18885,2699.56835,9748.9106,8539.671,47403.88,20167.336030000002,17352.6803,4058.1161,16297.846000000001,8596.8278,2497.0383,38282.7495,36898.73308,14474.675,13224.693000000001,42124.5153,14571.8908,11253.421,1534.3045,18246.4955,32548.3405,3877.30425,11842.62375,38126.2465,16577.7795,24915.22085,8569.8618,7804.1605,10736.87075,8968.33,9991.03765,21195.818,11658.37915,8603.8234,13919.8229,4915.05985,2217.6012,1634.5734,4934.705,32787.458589999995,12269.68865,2217.46915,5373.36425,8601.3293,12142.5786,7077.1894,20709.02034,1633.0444,2130.6759,7537.1639,14426.07385,1261.442,5757.41345,2211.13075,8782.469000000001,6748.5912,11833.7823,10928.848999999998,5709.1644,2203.73595,5354.07465,23065.4207,6875.960999999999,22412.6485,2055.3249,12950.0712,40003.33225,12363.546999999999,4032.2407,5855.9025,17626.23951,11033.6617,3594.17085,11396.9002,41999.52,9391.346,3577.9990000000003,4504.6624,40720.55105,3392.3652,9264.796999999999,22192.43711,10450.552,2416.955,5989.52365,20177.671130000002,21984.47061,8835.26495,36307.7983,18648.4217,39722.7462,12485.8009,5327.40025,6496.8859999999995,10579.711000000001,6686.4313,10577.087,1980.07,11455.28,2221.56445,5397.6167,1986.9334,17179.522,3761.292,8733.22925,6082.405,9855.1314,6117.4945,9866.30485,13844.7972,7935.29115,2913.5690000000004,18328.2381,1682.5970000000002,39611.7577,2201.0971,8823.279,30063.58055,39047.285,24535.69855,3659.3459999999995,1163.4627,4718.20355,2020.1770000000001,24059.68019,3206.49135,7201.70085,11881.358,46130.5265,23288.9284,8520.026,8023.13545,42760.5022,21348.706000000002,35147.52848,14451.83515,12096.6512,34254.05335,4005.4225,14001.1338,9715.841,12890.05765,48673.5588,7228.21565,19719.6947,39774.2763,2974.1259999999997,6548.19505,1964.78,3861.20965,2200.83085,20781.48892,19521.9682,13981.85035,8515.7587,14235.072,41919.097,13143.86485,25992.82104,12638.195,10959.6947,42111.6647,7729.64575,24227.33724,10106.13425,1842.519,16138.76205,9283.562,43943.8761,3484.3309999999997,48885.135610000005,1731.6770000000001,14394.39815,44501.3982,2404.7338,3591.48,10806.839,11944.59435,2464.6188,46151.1245,4894.7533,9193.8385,14119.62,4133.64165,11945.1327,20745.9891,25081.76784,2710.82855,11657.7189,13470.8044,9800.8882,23244.7902,19350.3689,4449.462,13880.948999999999,16115.3045,33471.97189,2304.0022,47305.305,2138.0707,3056.3881,28468.91901,2395.17155,13887.204,21771.3423,8269.044,12222.8983,48549.17835,8605.3615,5325.651,20984.0936,10407.08585,12032.326000000001,10422.91665,12648.7034,9282.4806,47896.79135,13393.756000000001,1981.5819,9411.005,9541.69555,2902.9065,21978.6769,3161.454,9704.66805,8444.474,11187.6567,20277.80751,3366.6697,11436.73815,4747.0529,1917.3184,13041.921,18806.14547,2457.502,7337.7480000000005,4922.9159,1727.54,3756.6216,7954.517,25333.33284,11362.755,43813.8661,5969.723000000001,1769.53165,2438.0552,13844.506000000001,5261.46945,9634.538,10923.9332,6610.1097,34838.873,24393.6224,3410.324,12957.118,3260.199,28476.734989999997,8116.26885,8978.1851,4661.28635,19107.7796,12574.048999999999,2866.091,8334.5896,16884.924,13747.87235,47496.49445,4357.04365,5836.5204,5472.4490000000005,21797.0004,8823.98575,19496.71917,13112.6048,2690.1138,4667.60765,10602.385,4428.88785,11512.405,11830.6072,3393.35635,2719.27975,46200.9851,12982.8747,18955.22017,10564.8845,8534.6718,2254.7967,4571.41305,11326.71487,6551.7501,11520.09985,2045.68525,48517.56315,21659.9301,4438.2634,36950.2567,2527.81865,4687.7970000000005,5125.2157,7448.40395,30942.1918,8334.45755,3972.9247,6196.448,16657.71745,27724.28875,6289.7549,39725.51805,11743.298999999999,7624.63,2205.9808,6653.7886,14455.64405,47928.03,1712.227,4889.9995,2585.269,12629.1656,40419.0191,25678.77845,35069.37452,7682.67,1909.52745,2643.2685,10704.47,8988.15875,4320.41085,24603.04837,5693.4305,12730.9996,11286.5387,13831.1152,8062.764,39125.33225,15006.57945,8871.1517,3943.5954,32734.1863,4670.64,8798.593,8083.9198,11345.518999999998,9432.9253,7441.053000000001,2026.9741,7045.499,1632.03625,37607.5277,13462.52,2102.2647,2104.1134,6414.178000000001,11363.2832,4562.8421,4719.52405,6067.12675,5257.50795,6666.243,1984.4533,3935.1799,23401.30575,6600.361,1627.28245,8124.4084,3062.50825,1727.785,3180.5101,43753.33705,30166.618169999998,15555.18875,4234.927,18259.216,12129.61415,5148.5526,11737.84884,5594.8455,3292.52985,2457.21115,35595.5898,24667.418999999998,40974.1649,24520.264,10043.249,3172.018,36910.60803,6799.4580000000005,3866.8552,10141.1362,6079.6715,1744.465,19040.876,14313.8463,4266.1658,12146.971000000001,3989.841,4561.1885,10085.846,1837.237,47269.85400000001,23045.56616,12981.3457,3925.7582,3558.62025,5245.2269,4185.0979,11272.331390000001,5729.0053,11879.10405,16069.08475,63770.42801,8280.6227,11082.5772,4837.5823,8527.532,4074.4537,5031.26955,11090.7178,1532.4697,34672.1472,2322.6218,6858.4796,13224.05705,52590.82939,8059.6791,7526.70645,7152.6714,8944.1151,12029.2867,22218.1149,3046.062,1748.774,43921.1837,1674.6323,4137.5227,9447.25035,7650.77375,2261.5688,9861.025,5979.731,7147.4728],\"y\":[4254.76513671875,7358.94482421875,11180.5439453125,5858.49365234375,7316.46875,7655.68017578125,7879.95361328125,10973.283203125,5197.59130859375,8131.37158203125,10599.4765625,9164.21875,8445.7646484375,5134.8037109375,7569.98828125,8438.095703125,7605.236328125,6871.75634765625,5353.94970703125,6445.45556640625,9529.8408203125,9055.3876953125,10320.375,9772.8623046875,10858.8798828125,7132.5615234375,9527.4873046875,4821.548828125,7444.89794921875,8308.458984375,5802.736328125,9318.65234375,9252.12109375,7906.4501953125,10321.9775390625,10582.5869140625,4959.3037109375,9641.9384765625,6101.77880859375,6367.70849609375,7449.6328125,5310.2646484375,9174.4482421875,9976.1064453125,5325.15966796875,5387.8349609375,9316.505859375,7758.24462890625,10798.8955078125,9708.857421875,8188.11572265625,5060.92041015625,6695.46142578125,8023.900390625,11306.546875,7215.72802734375,8230.48046875,6785.09423828125,6189.19580078125,7311.34375,9112.3037109375,5745.0419921875,7411.42333984375,4892.359375,6842.966796875,5356.13134765625,5466.8671875,10788.69921875,9235.7919921875,6140.54052734375,5481.05029296875,7830.2431640625,6906.77880859375,11091.208984375,7878.755859375,6141.86962890625,6975.99169921875,5443.10986328125,5535.06103515625,10027.1494140625,6455.38671875,9183.6123046875,7461.38818359375,7470.68505859375,8081.208984375,9894.6650390625,10088.0439453125,4205.67041015625,8114.95166015625,6153.14208984375,6730.5234375,9004.5068359375,6508.4072265625,9913.134765625,6287.43115234375,10054.0205078125,8916.583984375,5865.42626953125,7575.416015625,8979.5517578125,8100.71240234375,9071.86328125,10296.806640625,5951.3779296875,7421.8251953125,6774.45751953125,5462.3251953125,5302.40283203125,5928.34375,6237.26318359375,9448.3388671875,4291.06494140625,10950.365234375,5776.2978515625,6555.6328125,7695.0712890625,8060.935546875,9827.60546875,8379.7861328125,5683.20556640625,4741.87548828125,9160.37109375,5441.79443359375,10202.0986328125,7469.2724609375,4934.65185546875,8456.6474609375,6778.9599609375,6879.2578125,10537.619140625,10184.953125,5699.34619140625,9447.529296875,5995.865234375,6132.46337890625,4446.65673828125,7521.85400390625,7941.45166015625,9495.0947265625,9922.0947265625,9533.95703125,10010.8203125,7691.23779296875,8519.103515625,5594.1943359375,6393.46826171875,11234.3408203125,8885.6669921875,5371.67431640625,9975.7412109375,8756.2607421875,9803.1220703125,9495.25390625,6163.17724609375,9195.548828125,5913.38818359375,8190.3486328125,8885.865234375,8291.2265625,9713.3076171875,7360.7177734375,5052.87060546875,10142.3984375,8294.462890625,5878.74560546875,7011.55908203125,7050.0869140625,7648.86279296875,6807.70751953125,10153.4765625,11548.904296875,7548.3984375,8964.4375,8911.525390625,11221.3740234375,8734.79296875,10421.0625,7555.37109375,9534.8017578125,5849.34716796875,9418.6416015625,6639.8916015625,10595.666015625,10201.3994140625,6200.8466796875,6724.58349609375,9284.90625,5146.55224609375,7903.0615234375,8136.1689453125,8882.783203125,7440.18896484375,7502.453125,7049.06982421875,5694.1806640625,8878.537109375,8311.55078125,5646.263671875,6648.37744140625,6982.6923828125,8937.578125,7397.5947265625,9409.1201171875,6125.00341796875,10011.0615234375,10721.669921875,6949.33935546875,6568.69091796875,7180.66064453125,10594.29296875,4668.1181640625,10097.3037109375,6986.1435546875,7075.25439453125,6779.74169921875,9698.0947265625,5317.6328125,6522.02490234375,7989.1162109375,7563.69970703125,6566.85302734375,5730.59716796875,7501.7265625,8506.8818359375,9377.8349609375,6144.1640625,9968.724609375,9364.1240234375,5392.1591796875,6110.68896484375,10086.2841796875,11037.927734375,6105.52783203125,8125.0888671875,7842.349609375,6427.19189453125,7571.51025390625,9852.953125,9143.1728515625,5430.36328125,6315.0673828125,5831.66748046875,6860.2119140625,6247.06494140625,5858.177734375,7037.666015625,7033.79296875,5839.37939453125,11123.5322265625,5721.27099609375,9424.2958984375,10063.287109375,10536.384765625,4635.86181640625,8918.7412109375,6199.25634765625,5778.2900390625,7715.9033203125,9123.8134765625,8298.7490234375,7336.37158203125,10725.361328125,7982.9560546875,11156.421875,8707.0517578125,9176.658203125,9209.9462890625,4760.52587890625,8926.7822265625,5405.73779296875,7871.599609375,9646.0068359375,9789.8447265625,9644.3525390625,9809.4365234375,8755.595703125,8839.236328125,6092.13232421875,6153.71044921875,11758.94140625,4767.7998046875,7763.72802734375,5994.865234375,7381.65478515625,6293.06884765625,6740.26318359375,6372.65283203125,9830.642578125,6291.36181640625,7470.8408203125,7019.408203125,4586.2841796875,5996.63037109375,6146.04736328125,8791.7265625,4544.85205078125,9839.791015625,6458.51220703125,7968.52099609375,7095.71728515625,5966.47900390625,11432.865234375,8184.6044921875,6570.81494140625,8494.2578125,6499.25244140625,7138.56982421875,7778.1396484375,5427.89306640625,5548.67919921875,5995.283203125,5596.48681640625,5492.22998046875,9211.5556640625,11173.791015625,6198.076171875,5630.23291015625,9359.6103515625,4918.32275390625,9339.23046875,6549.69482421875,9526.2841796875,7449.51708984375,5898.80322265625,8555.3076171875,8719.109375,7921.55810546875,8025.42236328125,6394.93310546875,9037.3896484375,6224.16015625,10671.7099609375,11274.625,8329.8427734375,7912.04345703125,9916.00390625,11011.0966796875,5939.7978515625,9076.9609375,7178.50146484375,5661.30419921875,6132.30126953125,6823.22216796875,6557.19775390625,7950.26318359375,5286.4306640625,7744.18505859375,8591.9228515625,5857.28759765625,8117.58447265625,7549.18798828125,9373.365234375,10730.72265625,7901.23388671875,8634.5419921875,7317.29248046875,10320.5458984375,8211.6142578125,10428.275390625,8529.86328125,8507.8369140625,8329.1806640625,7613.5703125,9404.4404296875,7723.6259765625,8803.994140625,10060.0029296875,9445.001953125,8759.958984375,7973.69580078125,10020.45703125,7538.201171875,8729.1982421875,10120.41796875,6296.2734375,6549.0634765625,8850.9140625,6904.314453125,7850.60498046875,6082.55419921875,8437.376953125,6786.40625,7838.67138671875,9401.1943359375,7385.87939453125,4502.48681640625,8615.6865234375,7484.0234375,5807.46044921875,6360.7109375,9960.529296875,8303.525390625,7126.28125,6538.85400390625,10216.6201171875,6645.8701171875,10232.0341796875,9532.6494140625,8185.2861328125,5275.654296875,7935.9794921875,5310.25927734375,6382.23095703125,10488.3623046875,9675.12109375,9728.4697265625,7070.91943359375,6028.3740234375,8854.857421875,10660.1806640625,10132.9150390625,4865.42333984375,4729.0185546875,8280.6884765625,7434.54638671875,5594.1943359375,10544.0029296875,10293.7001953125,6137.31494140625,6049.390625,9276.583984375,8120.32763671875,10595.080078125,7084.1015625,7581.76220703125,9591.8515625,6369.31103515625,9422.923828125,6407.0419921875,8083.5107421875,7551.55029296875,5032.060546875,6924.107421875,9351.21484375,9155.271484375,5814.60986328125,7874.4599609375,4596.34619140625,7790.98193359375,11438.4794921875,6073.306640625,9792.361328125,6070.83544921875,6969.1875,11494.216796875,10289.826171875,9495.0634765625,9198.0537109375,9617.9150390625,8268.0908203125,8755.2119140625,5400.74462890625,5959.83740234375,8722.4990234375,5636.76171875,6997.41845703125,5862.01416015625,4425.2958984375,8698.4814453125,8645.7021484375,8486.3583984375,7812.58447265625,5360.49755859375,8947.6923828125,6547.92236328125,9475.24609375,9473.4150390625,8661.9013671875,8541.9697265625,8175.23876953125,9504.5849609375,7263.87841796875,5641.4404296875,5616.28662109375,7545.9619140625,8238.7216796875,6360.962890625,5785.50390625,11174.0810546875,8641.1689453125,8399.9482421875,10339.900390625,9753.404296875,9801.6083984375,8786.9921875,8555.537109375,8283.0751953125,9707.5546875,6475.3740234375,9821.3984375,4977.54736328125,9578.65234375,8417.71484375,11031.7822265625,6236.98583984375,8084.90283203125,6399.32470703125,11074.228515625,7870.287109375,10932.4326171875,4770.42626953125,5537.52587890625,7589.39013671875,5390.56591796875,8556.3115234375,6680.23974609375,7859.84619140625,5796.66748046875,10829.140625,9812.2568359375,9165.630859375,4972.90576171875,9354.0927734375,6475.96826171875,6411.74462890625,5665.5966796875,6583.36767578125,9719.517578125,7253.29736328125,5842.6435546875,8772.505859375,10175.6767578125,9736.9375,9567.783203125,8882.3134765625,7751.58740234375,8341.58203125,9194.572265625,7762.27587890625,10591.5361328125,6066.99755859375,6314.44384765625,6410.5546875,6480.07421875,8243.41015625,9889.1328125,6304.38232421875,7559.6005859375,8366.9306640625,9504.7041015625,7219.1865234375,10622.9599609375,6294.05029296875,5652.10009765625,7805.916015625,5341.78759765625,5964.09130859375,8933.53515625,5821.4169921875,8493.2578125,7457.33349609375,8871.396484375,9509.140625,8993.765625,5257.9580078125,6112.20361328125,8325.4892578125,8339.318359375,8466.23046875,6078.677734375,10948.1884765625,8295.484375,10471.9384765625,6198.2255859375,6618.1611328125,6263.060546875,10153.482421875,5384.9345703125,9983.6513671875,9600.0390625,10109.9013671875,6899.88427734375,6977.76513671875,8906.1572265625,6233.5458984375,7881.79052734375,8993.8017578125,10422.8037109375,6614.005859375,8027.138671875,6336.63232421875,6268.71044921875,9108.025390625,6396.5986328125,6062.04150390625,7215.109375,10764.828125,7220.67919921875,7517.9365234375,10216.8427734375,6871.5537109375,8682.9013671875,6057.50146484375,9386.6787109375,5163.00390625,8299.2265625,6694.60888671875,6925.349609375,6625.42138671875,9746.177734375,8280.607421875,8090.5029296875,6927.2275390625,9198.7490234375,10349.2314453125,9014.6005859375,4931.90283203125,5909.5166015625,6859.51806640625,7786.50048828125,4800.01953125,8425.8798828125,10537.0166015625,8028.77783203125,8506.9697265625,6966.3564453125,7876.74267578125,5768.5849609375,6129.62841796875,5695.27880859375,5288.33203125,7399.1318359375,9481.095703125,10506.35546875,5804.46630859375,8529.8759765625,8963.125,9330.1337890625,7652.96533203125,5921.1767578125,9345.6875,9193.13671875,6192.8447265625,5001.884765625,9809.748046875,8196.8330078125,6296.78515625,11216.7890625,6639.865234375,7029.685546875,8074.7197265625,5664.52783203125,7522.28515625,4892.45458984375,4924.23828125,5036.59912109375,9734.4775390625,6652.6875,10897.6640625,8209.6728515625,11123.6708984375,9229.3525390625,10380.1484375,9018.2041015625,9900.8173828125,9983.91796875,8934.9609375,7710.44873046875,10898.578125,8468.5537109375,5462.46875,5866.6904296875,9316.5732421875,7274.74267578125,5821.58056640625,9085.48828125,4431.17822265625,9710.7373046875,8456.220703125,6250.11083984375,6553.3896484375,9538.888671875,10157.2705078125,5729.39111328125,10164.8515625,6750.29296875,7479.81494140625,10508.40234375,6070.6533203125,10115.8564453125,6989.67724609375,5251.72216796875,5080.044921875,9144.2626953125,11038.642578125,10213.8134765625,8550.291015625,6912.97412109375,7129.2734375,10362.0,5652.92822265625,9988.232421875,5226.54443359375,10712.8525390625,6215.55908203125,5933.29931640625,9467.3486328125,5215.99853515625,10843.5615234375,7626.8779296875,7791.30517578125,9334.322265625,10740.2197265625,7622.607421875,7014.484375,7230.99755859375,8675.1328125,8577.9072265625,8578.90234375,10706.474609375,8333.0009765625,10569.6572265625,10602.76953125,6286.84228515625,8692.7080078125,10237.958984375,6301.6767578125,8274.3447265625,5952.41259765625,9501.962890625,8512.521484375,9374.4638671875,6578.83935546875,7095.73291015625,9861.7421875,7109.0546875,6473.9345703125,9765.048828125,8889.3251953125,5182.16357421875,8106.21484375,6934.58349609375,6079.7998046875,6274.50439453125,8380.6845703125,9206.9931640625,10338.986328125,10022.640625,7681.64208984375,5611.47021484375,8226.72265625,4652.00341796875,6975.96533203125,9768.966796875,9391.4404296875,8992.5224609375,6542.01513671875,8914.8583984375,7596.99462890625,9255.751953125,5807.0576171875,9004.7001953125,8645.0791015625,9638.19140625,6207.96728515625,6498.25390625,11478.013671875,6311.60498046875,8814.447265625,5308.66552734375,4539.677734375,10782.25390625,6802.36669921875,7981.84130859375,6927.740234375,10626.87109375,8741.5498046875,8598.6279296875,9453.2978515625,5660.83251953125,7717.33740234375,10610.5244140625,5818.45849609375,9453.2763671875,8803.896484375,6347.560546875,5723.99853515625,9907.1298828125,11223.2783203125,5711.72998046875,9343.9794921875,7751.18310546875,5943.29345703125,6588.22216796875,8466.205078125,7815.57421875,8799.1259765625,5344.16845703125,10745.07421875,7359.4716796875,6532.84423828125,7410.966796875,4717.8740234375,5788.90234375,6737.3837890625,9278.671875,10362.7412109375,8804.3857421875,6720.19970703125,8197.7958984375,5744.9501953125,6307.11328125,8455.072265625,8491.076171875,9466.775390625,7779.8017578125,5429.0078125,7919.2001953125,5040.57373046875,10888.060546875,5169.86669921875,7857.60986328125,4410.00244140625,9282.2939453125,8365.619140625,9174.1875,7591.99609375,8320.169921875,5880.2900390625,6100.2666015625,9361.6591796875,8655.62890625,7765.06591796875,8187.10791015625,5856.591796875,9819.5361328125,9197.419921875,11497.193359375,8894.8154296875,8201.345703125,5228.8994140625,8897.1025390625,6534.66162109375,5843.0712890625,7686.57373046875,9721.8515625,10511.7685546875,9025.66015625,10362.9619140625,8456.734375,6652.4970703125,7511.181640625,5020.67333984375,7429.2490234375,10402.447265625,5231.755859375,5372.62060546875,7214.7197265625,10384.185546875,6892.185546875,5869.203125,7378.27392578125,6674.11279296875,8863.232421875,6653.31298828125,7598.16748046875,8148.76025390625,7987.88623046875,4658.44970703125,9390.400390625,6594.8642578125,4134.62109375,4992.58837890625,8940.115234375,9924.87109375,11782.0166015625,6282.763671875,6533.4501953125,9716.2861328125,7000.4873046875,7033.74755859375,7397.26220703125,6803.26123046875,5312.6328125,6273.81298828125,8800.853515625,8992.5439453125,8721.7548828125,8908.37109375,6757.35400390625,10452.580078125,7082.095703125,6814.8935546875,8952.548828125,8334.1416015625,5405.5810546875,6938.744140625,10916.44140625,8431.201171875,9834.4384765625,8004.212890625,5776.97119140625,9944.2763671875,5059.998046875,10651.5068359375,8984.603515625,11106.7734375,6346.23095703125,5685.10693359375,7705.0,6338.71142578125,5251.82666015625,7966.38525390625,9571.1708984375,10583.3681640625,11154.09375,8678.515625,8990.7041015625,7193.48046875,9106.7587890625,7041.8701171875,6685.2890625,10461.5556640625,6336.16259765625,6787.53759765625,6863.66943359375,7269.1123046875,10090.1533203125,10351.8203125,7860.5908203125,7368.66845703125,9186.333984375,10223.638671875,8608.1416015625,8472.662109375,5512.53857421875,5733.91259765625,10010.26171875,6257.58642578125,6300.39306640625,8273.013671875,9115.166015625,5739.80810546875,8534.623046875,8444.21875,8790.2177734375],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"markers\",\"x\":[7281.5056,5267.81815,12347.171999999999,24513.09126,3736.4647,7358.17565,9788.8659,17085.2676,8211.1002,19798.05455,3077.0955,3385.39915,6837.3687,8538.28845,26392.260290000002,13012.20865,3227.1211,15170.069,11073.176000000001,20773.62775,39556.4945,2134.9015,2198.18985,6555.07035,4340.4409,12622.1795,7740.3369999999995,12475.3513,3987.926,21082.16,1241.565,40103.89,17929.303369999998,8302.53565,3471.4096,5846.9176,13352.0998,9144.565,25656.575259999998,7726.854,13887.9685,30284.642939999998,5266.3656,12797.20962,1146.7966,7046.7222,8627.5411,39597.4072,12323.936000000002,11454.0215,40904.1995,3171.6149,7445.918000000001,13607.36875,27346.04207,12557.6053,10797.3362,5488.262,6282.235,40941.2854,1708.0014,23306.547000000002,28950.4692,1664.9996,17361.7661,7345.084,18157.876,7256.7231,7626.993,26236.57997,7325.0482,1720.3537,7153.5539,6986.696999999999,8232.6388,10370.91255,4889.0368,6474.013000000001,1625.43375,10115.00885,10264.4421,9386.1613,18223.4512,3561.8889,23887.6627,3392.9768,1135.9407,1880.487,5630.45785,10156.7832,22144.032000000003,62592.87309,9500.57305,46113.511,13390.559,4076.4970000000003,6389.37785,9880.068000000001,4751.07,12629.8967,6664.68595,21344.8467,22395.74424,4466.6214,9249.4952,8703.456,38245.59327,36189.1017,19023.26,5080.096,9957.7216,6313.759,4151.0287,17748.5062,37133.8982,21472.4788,4350.5144,41661.602,6985.50695,23241.47453,39983.42595,9174.13565,6710.1919,1633.9618,47055.5321,2899.48935,12231.6136,5138.2567,4646.759,12913.9924,24476.47851,27808.7251,18033.9679,25309.488999999998,6360.9936,20296.86345,10600.5483,9583.8933,34806.4677,5708.866999999999,6457.8434,21259.37795,12928.7911,2904.0879999999997,3537.703,36085.219,10713.643999999998,21232.182259999998,8457.818000000001,18838.70366,3500.6123,6753.0380000000005,1256.299,45710.20785,11289.10925,1141.4451,12644.589,1121.8739,12495.29085,27218.43725,13129.60345,1837.2819,6311.951999999999,12333.828000000001,18218.16139,48173.361,6781.3542,3994.1778,1621.8827,11165.41765,9964.06,7441.501,5240.765,4686.3887,18972.495,6393.60345,3732.6251,19144.57652,7633.7206,11093.6229,46889.2612,11931.12525,4454.40265,1149.3959,34779.615,3277.1609999999996,27533.9129,10959.33,19594.80965,2483.736,10065.413,7512.267,19442.3535,7209.4918,11299.343,10231.4999,14711.7438,4906.40965,13415.0381,6123.5688,5469.0066,1705.6245,12646.207,6435.6237,11482.63485,44400.4064,17663.1442,14590.63205,38711.0,2166.732,8116.68,38746.3551,5584.3057,6775.960999999999,55135.402089999996,18903.49141,5920.1041,3378.91,13430.265,11353.2276,5253.524,4738.2682,2196.4732,11987.1682,1515.3449,17178.6824,12268.63225,14133.03775,7421.19455,36021.0112,5966.8874,17081.08,6059.173000000001,37484.4493,10338.9316,2855.43755,13616.3586,29523.1656,1607.5101,9504.3103,25517.11363,18310.742,9869.8102,38415.474,11554.2236,12609.88702,10594.2257,3238.4357,14394.5579,2585.85065,2927.0647,14254.6082,40932.4295,4883.866,7418.522,9630.396999999999,8606.2174,5003.853,2680.9493,9549.5651,3847.6740000000004,24873.3849,5699.8375,42856.837999999996,3201.24515,22493.65964,11566.30055,7160.3303,1646.4297,9617.66245,2203.47185,1137.011,9487.6442,7731.85785,7443.64305,2523.1695,34472.841,1969.614,7789.635,41676.0811,1694.7964,1906.35825,11763.0009,9225.2564,21098.55405,2709.1119,4992.3764,6600.20595,18608.262,2459.7201,8252.2843,4441.21315,5425.02335,4058.71245,15518.18025,4544.2348,19199.944,8347.1643,5478.0368,14007.222,42303.69215,5377.4578,1628.4709,16420.49455,10436.096,9778.3472,15612.19335,7261.741,11015.1747,10493.9458,6203.90175,10197.7722,6770.1925,1972.95,9304.7019,29141.3603,46255.1125,11013.7119,8162.71625,12244.531,5662.225,5383.536,12094.478000000001,14478.33015,3208.7870000000003,7639.41745,2897.3235,29330.98315,11534.87265,1137.4697,23568.272,13429.0354,9222.4026,36397.576,3956.07145,10965.446000000002,9377.9047,2801.2588,44585.45587,23967.38305,13228.84695,2352.96845,6334.34355,11938.25595,6338.0756,20462.99766,8068.185,4949.7587,9863.4718,39871.7043,19361.9988,6184.2994,1728.8970000000002,7323.734818999999,14418.2804,5246.047,38344.566,7144.86265,2207.69745,3579.8287,10325.206,8310.83915,3481.868,4399.731,5385.3379,4766.022,16232.847,13974.45555,10795.937329999999,3021.80915,8615.3,9361.3268,35585.576,7147.105,4239.89265,11735.87905,17128.42608,5375.0380000000005,10791.96,3443.0640000000003,5415.6612,7727.2532,18765.87545,6358.77645,5002.7827,14449.8544,2597.779,13047.33235,51194.55914,2331.519,11946.6259,8964.06055,7160.094,13725.47184,8825.086,11411.685,8930.93455,24106.91255,17878.900680000002,22462.04375,1391.5287,8240.5896],\"y\":[7436.478515625,6931.84033203125,10697.1787109375,9439.55078125,6086.46337890625,7981.033203125,8246.19140625,5411.2646484375,7694.0751953125,7175.564453125,5289.57763671875,5959.62255859375,7604.05126953125,8346.1884765625,9188.634765625,9217.947265625,6603.3837890625,11618.638671875,9119.9765625,7266.9306640625,8481.74609375,5974.07666015625,4835.36376953125,7194.697265625,6236.72998046875,9884.87109375,9181.7890625,8605.5390625,6830.58837890625,7712.830078125,4449.52978515625,8556.3076171875,8424.9228515625,7503.517578125,7662.06298828125,8774.0732421875,10170.583984375,9797.91015625,8396.6552734375,8154.42919921875,10901.814453125,8587.7451171875,6821.16064453125,8715.595703125,6606.84130859375,7609.337890625,7413.013671875,8302.0478515625,9188.4931640625,8957.6708984375,7321.611328125,6731.59130859375,8827.431640625,9936.37890625,9344.056640625,10230.01171875,9076.6259765625,7786.64404296875,7877.12890625,8701.3837890625,4847.89013671875,8607.2646484375,9957.6767578125,5523.60693359375,6283.8876953125,8558.7177734375,6786.12158203125,8170.65673828125,7959.8544921875,7923.244140625,7400.35107421875,5532.2275390625,7724.0087890625,8243.9375,8203.7705078125,9126.8828125,7256.38037109375,8743.607421875,4517.58544921875,9168.8095703125,8934.158203125,9719.796875,5446.3701171875,5677.30029296875,8571.7587890625,6280.1474609375,5779.65869140625,5884.109375,7871.8984375,8145.40966796875,8114.29443359375,8480.8701171875,8102.99365234375,9540.2958984375,10359.1689453125,7192.61083984375,7507.96240234375,9985.6357421875,7410.1923828125,9273.466796875,7837.15283203125,5237.83447265625,5357.32958984375,6691.5302734375,8539.201171875,8729.19140625,7805.08984375,6461.6953125,6958.94775390625,6041.18212890625,8749.8994140625,7661.58251953125,7072.638671875,5414.31982421875,7042.64208984375,7247.0537109375,6749.40283203125,9031.822265625,6911.150390625,5756.56982421875,7999.2734375,9155.486328125,7311.04052734375,6363.953125,10922.80859375,6280.0810546875,9998.3984375,7751.49755859375,7555.54638671875,9371.0703125,9307.9306640625,9779.857421875,5785.27587890625,8938.51953125,8615.03515625,7010.07421875,9283.47265625,9270.8623046875,6723.53662109375,6447.39501953125,7706.29736328125,7653.78076171875,10241.8154296875,6386.7509765625,6116.16064453125,6900.3037109375,10060.6875,8614.5986328125,9529.2900390625,5515.6328125,7049.02587890625,6092.1640625,5572.21142578125,10391.064453125,9690.3095703125,6199.0751953125,10388.0185546875,4707.81494140625,10124.8662109375,9529.9296875,9293.4765625,7227.552734375,7518.943359375,9680.4111328125,6416.9755859375,11103.8056640625,8550.0478515625,8339.615234375,5443.56591796875,8995.80078125,9232.8642578125,8597.34765625,7360.06640625,8891.5244140625,6962.966796875,7829.93896484375,4957.81201171875,8731.4365234375,8477.4267578125,10682.9140625,10902.4853515625,9130.970703125,6806.2978515625,6804.89892578125,6142.55029296875,7354.3994140625,9613.5146484375,8960.1943359375,7201.43603515625,5666.64892578125,8387.349609375,8052.44775390625,7168.46923828125,7992.77587890625,9569.7578125,9328.0546875,4837.95166015625,5676.8623046875,9013.8642578125,7390.0693359375,6927.25927734375,4666.7783203125,10575.83984375,9532.765625,5734.828125,9840.3095703125,6278.2548828125,10206.6640625,7799.56689453125,6385.73876953125,8796.5693359375,7701.21484375,8041.298828125,8134.15234375,7628.28662109375,6551.884765625,8351.783203125,5203.3525390625,9873.107421875,9792.4140625,7322.638671875,7108.53369140625,4704.560546875,10133.931640625,5031.31005859375,5255.529296875,9808.638671875,6200.8466796875,7874.98779296875,6465.96826171875,7470.53173828125,5351.03125,7384.27490234375,6915.58740234375,9337.1552734375,6322.0615234375,9019.3447265625,9943.033203125,4348.42236328125,10415.900390625,8831.912109375,6339.80810546875,9079.3583984375,8031.57666015625,9516.3017578125,5832.96533203125,9993.765625,7465.5185546875,9908.7626953125,4716.14111328125,8142.4501953125,9652.12109375,7986.9501953125,6857.4306640625,8323.8916015625,9320.39453125,8051.1669921875,6881.5380859375,4980.615234375,10580.7255859375,6047.83544921875,8627.6318359375,8283.0869140625,9625.9501953125,7262.17236328125,5446.19970703125,10693.392578125,8851.396484375,6117.40625,8611.8828125,5237.83447265625,5861.21142578125,9145.9990234375,8848.1318359375,8666.9677734375,5919.20751953125,6459.29052734375,5369.97607421875,8630.142578125,9314.0078125,3841.713623046875,5638.8076171875,9751.8994140625,8179.17431640625,7891.30029296875,4949.2421875,6268.87841796875,8237.890625,6784.62939453125,5503.80615234375,7201.9951171875,6502.71142578125,7244.7333984375,6932.68212890625,4922.02490234375,5731.23876953125,6800.1513671875,9515.7314453125,7358.4638671875,10268.697265625,9495.3779296875,7871.515625,4749.005859375,5647.47900390625,9321.3056640625,8753.130859375,10685.666015625,8553.0048828125,8744.833984375,8575.21875,7467.6630859375,8169.60009765625,7699.5634765625,5624.16845703125,9769.3232421875,9963.3720703125,9941.357421875,8560.9111328125,8297.177734375,10295.73046875,8838.126953125,7221.37255859375,8765.7275390625,8158.68359375,5201.43408203125,8249.853515625,6133.1376953125,10055.4462890625,8298.69140625,5896.16259765625,8651.541015625,10080.4111328125,8281.15234375,6233.04345703125,6526.2802734375,9426.212890625,9090.6708984375,5752.58740234375,7499.17333984375,8739.5263671875,10390.5927734375,5398.84326171875,7798.298828125,9674.3056640625,8143.43505859375,10257.9033203125,9307.8779296875,7632.06787109375,8596.3935546875,8430.978515625,7079.67822265625,7277.064453125,4219.3515625,5137.216796875,11459.4716796875,7762.537109375,6941.75048828125,7357.1513671875,5559.81103515625,6232.9189453125,10203.3115234375,8293.2998046875,5677.3525390625,7336.16162109375,6767.9521484375,5820.486328125,5456.03173828125,10334.205078125,6218.19580078125,6550.63427734375,8374.935546875,8084.35693359375,6550.77587890625,7625.76708984375,6497.23046875,9588.3310546875,6768.5263671875,7447.72216796875,8613.6044921875,6667.9755859375,7278.16845703125,8603.7490234375,6353.3212890625,8144.75439453125,6804.93701171875,9194.7607421875,5363.22265625,9030.7666015625,7384.580078125,5359.3857421875,10055.2001953125,8818.8134765625,8828.4384765625,6934.609375,8563.5673828125,10848.384765625,8501.8017578125,8327.1083984375,7179.94482421875,7791.6611328125,5995.32177734375,8809.5947265625],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"Cargos reales\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Cargos predecidos\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"Cargos reales\"}},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Cargos predecidos\"}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Conjunto de entrenamiento\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Conjunto de prueba\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"shapes\":[{\"line\":{\"dash\":\"dot\"},\"type\":\"line\",\"x0\":4134.62109375,\"x1\":11782.0166015625,\"y0\":4134.62109375,\"y1\":11782.0166015625},{\"line\":{\"dash\":\"dot\"},\"type\":\"line\",\"x0\":3841.713623046875,\"x1\":11618.638671875,\"xref\":\"x2\",\"y0\":3841.713623046875,\"y1\":11618.638671875,\"yref\":\"y2\"}],\"title\":{\"text\":\"Primera red neuronal\"},\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('3e153416-e5a9-4168-9a6b-8a441a22aed6');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualmente, se confirma lo que la $R^2$ nos indicó, se observan tres grupos de datos, y la red sólo aprendio a \"predecir correctamente\" uno de ellos.\n",
        "\n",
        "Crearemos otra red con más capas esperando que se comporte mejor."
      ],
      "metadata": {
        "id": "HFWclg-Ty9kQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Segunda Red Neuronal"
      ],
      "metadata": {
        "id": "nQTwmhQpCn3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algoritmo/Aquitectura"
      ],
      "metadata": {
        "id": "UCrkPP1Wz17a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para esta segunda red neuronal, estamos agregando 3 capas densas con 64 neuronas cada una, es decir, el modelo va a tener 5 capas en total:\n",
        "\n",
        "1. Una capa de entrada,\n",
        "2. tres capas ocultas, y\n",
        "3. una capa de salida."
      ],
      "metadata": {
        "id": "2R06qLjozZOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(10)\n",
        "tf.random.set_seed(10)\n",
        "\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "     layers.Input([X_train_tr.shape[1]]),\n",
        "     layers.Dense(64, activation='relu'),\n",
        "     layers.Dense(64, activation='relu'),\n",
        "     layers.Dense(64, activation='relu'),     \n",
        "     layers.Dense(1)\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "FtTZuFszsFtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer = keras.optimizers.RMSprop(.01), \n",
        "    loss = 'mse', \n",
        "    metrics = ['mae']\n",
        ")"
      ],
      "metadata": {
        "id": "WoVJrNCTy8Hk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiGlEe4Cy__e",
        "outputId": "19920226-99e2-4ed1-bb0a-4f297d4fb939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_1 (Dense)             (None, 64)                576       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,961\n",
            "Trainable params: 8,961\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 200\n",
        "\n",
        "weights_filepath = os.path.join(checkpoint_filepath, 'model_2')\n",
        "\n",
        "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
        "    filepath = weights_filepath, \n",
        "    save_best_only = True, \n",
        "    save_weights_only = True, \n",
        "    monitor = 'val_mae', \n",
        "    mode = 'min'\n",
        ")\n"
      ],
      "metadata": {
        "id": "W61odIylz73J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Entrenamiento"
      ],
      "metadata": {
        "id": "qQNwN-yQz8aK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "history = model.fit(\n",
        "    X_train_tr, y_train, \n",
        "    epochs=epochs, \n",
        "    validation_split=.2, \n",
        "    callbacks=[model_checkpoint_callback]\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jG40cMrgzB4J",
        "outputId": "2c6facde-0cae-4f79-a20a-d1561e3f22b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "24/24 [==============================] - 1s 10ms/step - loss: 198219648.0000 - mae: 10151.2754 - val_loss: 126943272.0000 - val_mae: 9164.9443\n",
            "Epoch 2/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 137418176.0000 - mae: 9418.1621 - val_loss: 125209984.0000 - val_mae: 8882.0654\n",
            "Epoch 3/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 135042752.0000 - mae: 9360.1279 - val_loss: 127599880.0000 - val_mae: 9636.9570\n",
            "Epoch 4/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 133094928.0000 - mae: 9307.5537 - val_loss: 132495056.0000 - val_mae: 10201.2314\n",
            "Epoch 5/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 132692672.0000 - mae: 9320.8213 - val_loss: 122364024.0000 - val_mae: 8005.9160\n",
            "Epoch 6/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 129281776.0000 - mae: 9185.5205 - val_loss: 122406576.0000 - val_mae: 7654.8013\n",
            "Epoch 7/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 127561672.0000 - mae: 9089.5771 - val_loss: 116312152.0000 - val_mae: 8536.4307\n",
            "Epoch 8/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 123125128.0000 - mae: 8973.2383 - val_loss: 112779176.0000 - val_mae: 7906.3989\n",
            "Epoch 9/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 118479736.0000 - mae: 8723.0645 - val_loss: 108697064.0000 - val_mae: 8743.3223\n",
            "Epoch 10/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 107070120.0000 - mae: 8306.1855 - val_loss: 106340408.0000 - val_mae: 6313.5996\n",
            "Epoch 11/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 98415520.0000 - mae: 7741.5439 - val_loss: 84800608.0000 - val_mae: 7678.0723\n",
            "Epoch 12/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 80492152.0000 - mae: 7002.6436 - val_loss: 74204128.0000 - val_mae: 4983.1235\n",
            "Epoch 13/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 67536168.0000 - mae: 6318.7349 - val_loss: 59169148.0000 - val_mae: 4431.7891\n",
            "Epoch 14/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 54935728.0000 - mae: 5463.9678 - val_loss: 54098932.0000 - val_mae: 6368.5371\n",
            "Epoch 15/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 48150856.0000 - mae: 5055.9053 - val_loss: 68263448.0000 - val_mae: 4240.7759\n",
            "Epoch 16/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 52940352.0000 - mae: 5031.3062 - val_loss: 48294188.0000 - val_mae: 3552.7334\n",
            "Epoch 17/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 45285092.0000 - mae: 4622.1406 - val_loss: 50340184.0000 - val_mae: 6014.5532\n",
            "Epoch 18/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 43580852.0000 - mae: 4603.9556 - val_loss: 65094568.0000 - val_mae: 7067.0938\n",
            "Epoch 19/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 45236504.0000 - mae: 4616.5410 - val_loss: 62508644.0000 - val_mae: 6656.3145\n",
            "Epoch 20/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 46727928.0000 - mae: 4589.6479 - val_loss: 45675508.0000 - val_mae: 3496.3979\n",
            "Epoch 21/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 45074568.0000 - mae: 4531.0059 - val_loss: 110880488.0000 - val_mae: 9551.8936\n",
            "Epoch 22/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 44340888.0000 - mae: 4546.2139 - val_loss: 68825584.0000 - val_mae: 7248.5918\n",
            "Epoch 23/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 44311676.0000 - mae: 4633.1479 - val_loss: 39387856.0000 - val_mae: 3733.1084\n",
            "Epoch 24/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 47403844.0000 - mae: 4685.7656 - val_loss: 65272580.0000 - val_mae: 4149.8467\n",
            "Epoch 25/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 42555864.0000 - mae: 4352.0952 - val_loss: 60317048.0000 - val_mae: 6366.0796\n",
            "Epoch 26/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 41313964.0000 - mae: 4305.6680 - val_loss: 40316520.0000 - val_mae: 3566.9314\n",
            "Epoch 27/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 43171152.0000 - mae: 4496.5562 - val_loss: 38079100.0000 - val_mae: 3695.2168\n",
            "Epoch 28/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 43450708.0000 - mae: 4507.8203 - val_loss: 40073120.0000 - val_mae: 4768.1558\n",
            "Epoch 29/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 41936604.0000 - mae: 4364.6733 - val_loss: 46516440.0000 - val_mae: 5435.3682\n",
            "Epoch 30/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 41814264.0000 - mae: 4420.3828 - val_loss: 61968168.0000 - val_mae: 4377.3794\n",
            "Epoch 31/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 40926432.0000 - mae: 4275.5210 - val_loss: 65593664.0000 - val_mae: 4502.8770\n",
            "Epoch 32/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 44922244.0000 - mae: 4547.1680 - val_loss: 45615920.0000 - val_mae: 3330.9414\n",
            "Epoch 33/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 40576924.0000 - mae: 4229.9692 - val_loss: 40072736.0000 - val_mae: 3247.7637\n",
            "Epoch 34/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 39892068.0000 - mae: 4182.9067 - val_loss: 37045392.0000 - val_mae: 3788.3704\n",
            "Epoch 35/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 44833752.0000 - mae: 4398.1274 - val_loss: 55742812.0000 - val_mae: 3492.8315\n",
            "Epoch 36/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 43126240.0000 - mae: 4400.4780 - val_loss: 62773820.0000 - val_mae: 4017.2258\n",
            "Epoch 37/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 39100328.0000 - mae: 4240.6616 - val_loss: 49651724.0000 - val_mae: 3792.5715\n",
            "Epoch 38/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 41420024.0000 - mae: 4334.2183 - val_loss: 83138640.0000 - val_mae: 7799.8726\n",
            "Epoch 39/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 40738604.0000 - mae: 4208.7124 - val_loss: 40620948.0000 - val_mae: 3275.0791\n",
            "Epoch 40/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 42220728.0000 - mae: 4299.1392 - val_loss: 36298004.0000 - val_mae: 4230.6265\n",
            "Epoch 41/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 41326500.0000 - mae: 4244.4517 - val_loss: 35460076.0000 - val_mae: 3396.1074\n",
            "Epoch 42/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 38750136.0000 - mae: 4055.2407 - val_loss: 42302572.0000 - val_mae: 3246.2332\n",
            "Epoch 43/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 36701664.0000 - mae: 3995.1069 - val_loss: 37171276.0000 - val_mae: 4153.1743\n",
            "Epoch 44/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 40822036.0000 - mae: 4269.0220 - val_loss: 35721620.0000 - val_mae: 4000.2451\n",
            "Epoch 45/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 39772008.0000 - mae: 4170.7539 - val_loss: 37317440.0000 - val_mae: 3121.4905\n",
            "Epoch 46/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 38860632.0000 - mae: 4081.6963 - val_loss: 33860444.0000 - val_mae: 3383.8896\n",
            "Epoch 47/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 39139180.0000 - mae: 4082.9583 - val_loss: 36787996.0000 - val_mae: 4170.3813\n",
            "Epoch 48/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 39182352.0000 - mae: 4053.7075 - val_loss: 42046612.0000 - val_mae: 3168.0020\n",
            "Epoch 49/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 35393912.0000 - mae: 3915.5332 - val_loss: 100798984.0000 - val_mae: 8828.0967\n",
            "Epoch 50/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 38336444.0000 - mae: 4046.5093 - val_loss: 36880500.0000 - val_mae: 3023.9819\n",
            "Epoch 51/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 37065972.0000 - mae: 3934.9385 - val_loss: 35552636.0000 - val_mae: 4486.6465\n",
            "Epoch 52/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 35966424.0000 - mae: 3945.2446 - val_loss: 36484612.0000 - val_mae: 3869.6523\n",
            "Epoch 53/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 37381404.0000 - mae: 3919.9648 - val_loss: 39987144.0000 - val_mae: 4701.6128\n",
            "Epoch 54/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 36879288.0000 - mae: 3926.4993 - val_loss: 32552952.0000 - val_mae: 3655.7766\n",
            "Epoch 55/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 34518680.0000 - mae: 3852.6016 - val_loss: 40933720.0000 - val_mae: 3860.3008\n",
            "Epoch 56/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 34792604.0000 - mae: 3887.2615 - val_loss: 57673212.0000 - val_mae: 4058.8992\n",
            "Epoch 57/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 34930816.0000 - mae: 3795.5256 - val_loss: 31971720.0000 - val_mae: 3569.0808\n",
            "Epoch 58/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 35285204.0000 - mae: 3917.2627 - val_loss: 33788444.0000 - val_mae: 3919.7839\n",
            "Epoch 59/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 35085028.0000 - mae: 3828.0581 - val_loss: 31498516.0000 - val_mae: 3342.3657\n",
            "Epoch 60/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 35173388.0000 - mae: 3806.9092 - val_loss: 38532152.0000 - val_mae: 3142.7222\n",
            "Epoch 61/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 35349312.0000 - mae: 3826.6497 - val_loss: 30964584.0000 - val_mae: 3226.8450\n",
            "Epoch 62/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 35031868.0000 - mae: 3841.5283 - val_loss: 32040774.0000 - val_mae: 2916.8086\n",
            "Epoch 63/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 34872248.0000 - mae: 3841.2295 - val_loss: 34260672.0000 - val_mae: 2949.8982\n",
            "Epoch 64/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 32238398.0000 - mae: 3540.2490 - val_loss: 32316726.0000 - val_mae: 3881.5686\n",
            "Epoch 65/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 35874552.0000 - mae: 3874.3035 - val_loss: 31062932.0000 - val_mae: 3315.3889\n",
            "Epoch 66/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 31547324.0000 - mae: 3688.8860 - val_loss: 53076284.0000 - val_mae: 4162.4810\n",
            "Epoch 67/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 36071500.0000 - mae: 3925.3359 - val_loss: 34497936.0000 - val_mae: 2993.7268\n",
            "Epoch 68/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 35722480.0000 - mae: 3815.1978 - val_loss: 31062066.0000 - val_mae: 4000.9495\n",
            "Epoch 69/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 33816364.0000 - mae: 3696.4304 - val_loss: 40560660.0000 - val_mae: 3220.7378\n",
            "Epoch 70/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 31723514.0000 - mae: 3658.3325 - val_loss: 31793860.0000 - val_mae: 2814.6523\n",
            "Epoch 71/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 33744560.0000 - mae: 3740.0488 - val_loss: 44337724.0000 - val_mae: 5913.9556\n",
            "Epoch 72/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 33517850.0000 - mae: 3779.4695 - val_loss: 37540188.0000 - val_mae: 2978.1758\n",
            "Epoch 73/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 33138186.0000 - mae: 3746.9949 - val_loss: 32540876.0000 - val_mae: 3871.1091\n",
            "Epoch 74/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 30803464.0000 - mae: 3512.9175 - val_loss: 29342700.0000 - val_mae: 3466.1970\n",
            "Epoch 75/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 33854988.0000 - mae: 3827.9800 - val_loss: 32384070.0000 - val_mae: 3402.6067\n",
            "Epoch 76/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 31255286.0000 - mae: 3674.8516 - val_loss: 32655940.0000 - val_mae: 4470.4561\n",
            "Epoch 77/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 31585282.0000 - mae: 3670.8997 - val_loss: 39156620.0000 - val_mae: 3071.7297\n",
            "Epoch 78/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 33109940.0000 - mae: 3706.4504 - val_loss: 38553732.0000 - val_mae: 3281.4585\n",
            "Epoch 79/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 30444680.0000 - mae: 3524.2600 - val_loss: 33381080.0000 - val_mae: 4690.9131\n",
            "Epoch 80/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 32445886.0000 - mae: 3726.0164 - val_loss: 32550052.0000 - val_mae: 4633.1992\n",
            "Epoch 81/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 31372244.0000 - mae: 3671.8345 - val_loss: 33345158.0000 - val_mae: 3975.8450\n",
            "Epoch 82/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 30387918.0000 - mae: 3528.0623 - val_loss: 27741770.0000 - val_mae: 2868.3591\n",
            "Epoch 83/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 31439136.0000 - mae: 3613.7681 - val_loss: 28816624.0000 - val_mae: 2674.7439\n",
            "Epoch 84/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 31023210.0000 - mae: 3558.5173 - val_loss: 48726576.0000 - val_mae: 5936.9326\n",
            "Epoch 85/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 30657460.0000 - mae: 3559.7546 - val_loss: 31491108.0000 - val_mae: 2739.7993\n",
            "Epoch 86/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 31477428.0000 - mae: 3623.7339 - val_loss: 27824744.0000 - val_mae: 2836.0195\n",
            "Epoch 87/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 29965124.0000 - mae: 3467.9102 - val_loss: 28651762.0000 - val_mae: 3541.0945\n",
            "Epoch 88/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 31053884.0000 - mae: 3639.7156 - val_loss: 30071328.0000 - val_mae: 4409.9453\n",
            "Epoch 89/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 30312212.0000 - mae: 3566.7590 - val_loss: 31359912.0000 - val_mae: 3710.5876\n",
            "Epoch 90/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 29494564.0000 - mae: 3497.7571 - val_loss: 30237054.0000 - val_mae: 4044.0830\n",
            "Epoch 91/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 30904942.0000 - mae: 3698.8774 - val_loss: 31671794.0000 - val_mae: 2817.8862\n",
            "Epoch 92/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 28953452.0000 - mae: 3423.2263 - val_loss: 33068700.0000 - val_mae: 4261.8286\n",
            "Epoch 93/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 29188006.0000 - mae: 3557.2710 - val_loss: 30411188.0000 - val_mae: 2742.1653\n",
            "Epoch 94/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 30802482.0000 - mae: 3551.5696 - val_loss: 27052632.0000 - val_mae: 3106.7556\n",
            "Epoch 95/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 27999780.0000 - mae: 3381.0569 - val_loss: 26824080.0000 - val_mae: 2501.9429\n",
            "Epoch 96/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 30289250.0000 - mae: 3452.8162 - val_loss: 30643842.0000 - val_mae: 2592.1040\n",
            "Epoch 97/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 28460082.0000 - mae: 3337.0505 - val_loss: 25514120.0000 - val_mae: 2586.0732\n",
            "Epoch 98/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27909610.0000 - mae: 3327.8394 - val_loss: 32727640.0000 - val_mae: 4514.3501\n",
            "Epoch 99/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 28561438.0000 - mae: 3546.7236 - val_loss: 29455270.0000 - val_mae: 2919.9954\n",
            "Epoch 100/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27538038.0000 - mae: 3262.9626 - val_loss: 29844346.0000 - val_mae: 4102.6284\n",
            "Epoch 101/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 29377304.0000 - mae: 3388.6575 - val_loss: 36243716.0000 - val_mae: 5223.6982\n",
            "Epoch 102/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 28034070.0000 - mae: 3355.2507 - val_loss: 27076502.0000 - val_mae: 3968.1262\n",
            "Epoch 103/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 28693556.0000 - mae: 3440.0054 - val_loss: 26468914.0000 - val_mae: 2436.2122\n",
            "Epoch 104/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 28151250.0000 - mae: 3266.5908 - val_loss: 32466192.0000 - val_mae: 4558.4951\n",
            "Epoch 105/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 28640814.0000 - mae: 3423.6914 - val_loss: 29194822.0000 - val_mae: 2485.6418\n",
            "Epoch 106/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27380744.0000 - mae: 3234.8606 - val_loss: 32421300.0000 - val_mae: 2773.0403\n",
            "Epoch 107/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26083712.0000 - mae: 3191.1912 - val_loss: 25684558.0000 - val_mae: 3788.1792\n",
            "Epoch 108/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26749766.0000 - mae: 3276.6248 - val_loss: 44018316.0000 - val_mae: 3840.9255\n",
            "Epoch 109/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27160262.0000 - mae: 3249.8596 - val_loss: 50540388.0000 - val_mae: 6541.1421\n",
            "Epoch 110/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 28521078.0000 - mae: 3458.8088 - val_loss: 36799624.0000 - val_mae: 2877.9194\n",
            "Epoch 111/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25890832.0000 - mae: 3192.6304 - val_loss: 32789484.0000 - val_mae: 3952.2261\n",
            "Epoch 112/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27937168.0000 - mae: 3342.0352 - val_loss: 30278006.0000 - val_mae: 2884.6165\n",
            "Epoch 113/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27127044.0000 - mae: 3173.0278 - val_loss: 30774214.0000 - val_mae: 2699.8491\n",
            "Epoch 114/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27509432.0000 - mae: 3255.8679 - val_loss: 28987836.0000 - val_mae: 3951.0964\n",
            "Epoch 115/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26900082.0000 - mae: 3220.8059 - val_loss: 31944114.0000 - val_mae: 2699.0776\n",
            "Epoch 116/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26966148.0000 - mae: 3272.7725 - val_loss: 30768198.0000 - val_mae: 2589.6123\n",
            "Epoch 117/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27104530.0000 - mae: 3302.4731 - val_loss: 30831158.0000 - val_mae: 2565.6455\n",
            "Epoch 118/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26577328.0000 - mae: 3166.1729 - val_loss: 26713812.0000 - val_mae: 3948.4402\n",
            "Epoch 119/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 26457392.0000 - mae: 3273.7629 - val_loss: 47573596.0000 - val_mae: 3937.9763\n",
            "Epoch 120/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26582860.0000 - mae: 3177.7764 - val_loss: 24395580.0000 - val_mae: 2988.1958\n",
            "Epoch 121/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25444706.0000 - mae: 3115.2944 - val_loss: 27000726.0000 - val_mae: 2964.0867\n",
            "Epoch 122/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25788068.0000 - mae: 3202.7168 - val_loss: 32718462.0000 - val_mae: 3023.2686\n",
            "Epoch 123/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 27793066.0000 - mae: 3263.8499 - val_loss: 24680838.0000 - val_mae: 2443.5190\n",
            "Epoch 124/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26051448.0000 - mae: 3190.7637 - val_loss: 24181050.0000 - val_mae: 2616.6951\n",
            "Epoch 125/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 25737616.0000 - mae: 3051.4653 - val_loss: 29480370.0000 - val_mae: 4435.5527\n",
            "Epoch 126/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 28071328.0000 - mae: 3326.4988 - val_loss: 29750778.0000 - val_mae: 3227.8394\n",
            "Epoch 127/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26252138.0000 - mae: 3151.5278 - val_loss: 26085902.0000 - val_mae: 3398.2075\n",
            "Epoch 128/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25583512.0000 - mae: 3217.7231 - val_loss: 24360110.0000 - val_mae: 2414.5710\n",
            "Epoch 129/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 29458100.0000 - mae: 3377.8408 - val_loss: 27644460.0000 - val_mae: 2667.8120\n",
            "Epoch 130/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26075530.0000 - mae: 3103.2493 - val_loss: 37950356.0000 - val_mae: 5031.7476\n",
            "Epoch 131/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25667542.0000 - mae: 3207.4644 - val_loss: 29197698.0000 - val_mae: 2589.7905\n",
            "Epoch 132/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24976664.0000 - mae: 3035.5867 - val_loss: 32547824.0000 - val_mae: 3909.2280\n",
            "Epoch 133/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26901992.0000 - mae: 3292.0327 - val_loss: 27862018.0000 - val_mae: 2523.1943\n",
            "Epoch 134/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 23674278.0000 - mae: 2917.9727 - val_loss: 49772768.0000 - val_mae: 6460.6421\n",
            "Epoch 135/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26852328.0000 - mae: 3291.8777 - val_loss: 34917732.0000 - val_mae: 2877.8989\n",
            "Epoch 136/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24791998.0000 - mae: 3060.8762 - val_loss: 26441244.0000 - val_mae: 2506.1689\n",
            "Epoch 137/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26527654.0000 - mae: 3148.6052 - val_loss: 26132344.0000 - val_mae: 3367.2666\n",
            "Epoch 138/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24267686.0000 - mae: 2990.7932 - val_loss: 29963518.0000 - val_mae: 3113.1592\n",
            "Epoch 139/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25193070.0000 - mae: 3144.6914 - val_loss: 30604964.0000 - val_mae: 2699.8787\n",
            "Epoch 140/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 28040154.0000 - mae: 3265.8159 - val_loss: 25029234.0000 - val_mae: 3524.9741\n",
            "Epoch 141/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24763078.0000 - mae: 3075.9500 - val_loss: 29363012.0000 - val_mae: 3997.5374\n",
            "Epoch 142/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25250978.0000 - mae: 2989.6475 - val_loss: 31371362.0000 - val_mae: 4677.4824\n",
            "Epoch 143/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26249200.0000 - mae: 3250.0996 - val_loss: 29551192.0000 - val_mae: 4465.0107\n",
            "Epoch 144/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24878570.0000 - mae: 3134.9463 - val_loss: 27465832.0000 - val_mae: 3767.3840\n",
            "Epoch 145/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26436732.0000 - mae: 3273.9368 - val_loss: 25789960.0000 - val_mae: 2760.9951\n",
            "Epoch 146/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 23895406.0000 - mae: 2997.7156 - val_loss: 40541216.0000 - val_mae: 3195.5940\n",
            "Epoch 147/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24590068.0000 - mae: 3041.8271 - val_loss: 33661944.0000 - val_mae: 2826.7070\n",
            "Epoch 148/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25363234.0000 - mae: 3114.5120 - val_loss: 33581340.0000 - val_mae: 4405.4775\n",
            "Epoch 149/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25354810.0000 - mae: 3086.9629 - val_loss: 29395214.0000 - val_mae: 4438.6396\n",
            "Epoch 150/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25130920.0000 - mae: 3189.6697 - val_loss: 24971842.0000 - val_mae: 2431.3752\n",
            "Epoch 151/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25538558.0000 - mae: 3120.0696 - val_loss: 33501804.0000 - val_mae: 2954.4189\n",
            "Epoch 152/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25978242.0000 - mae: 3130.7754 - val_loss: 34757544.0000 - val_mae: 3365.7471\n",
            "Epoch 153/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24650594.0000 - mae: 3108.3354 - val_loss: 29619050.0000 - val_mae: 3901.0991\n",
            "Epoch 154/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 23956052.0000 - mae: 3003.9258 - val_loss: 27892444.0000 - val_mae: 2468.6543\n",
            "Epoch 155/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24786320.0000 - mae: 3027.1479 - val_loss: 34944096.0000 - val_mae: 5056.9834\n",
            "Epoch 156/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24735478.0000 - mae: 3111.5879 - val_loss: 43079676.0000 - val_mae: 3530.2544\n",
            "Epoch 157/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25196156.0000 - mae: 3137.4458 - val_loss: 36933200.0000 - val_mae: 2950.5195\n",
            "Epoch 158/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25593262.0000 - mae: 3037.1531 - val_loss: 40467424.0000 - val_mae: 5647.0996\n",
            "Epoch 159/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24600458.0000 - mae: 3174.6406 - val_loss: 69093328.0000 - val_mae: 3793.0374\n",
            "Epoch 160/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 26585824.0000 - mae: 3185.1416 - val_loss: 25497610.0000 - val_mae: 2404.4382\n",
            "Epoch 161/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 24666528.0000 - mae: 2961.6570 - val_loss: 26751870.0000 - val_mae: 3798.3464\n",
            "Epoch 162/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 27369406.0000 - mae: 3264.6519 - val_loss: 24798150.0000 - val_mae: 2702.1294\n",
            "Epoch 163/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 23503336.0000 - mae: 2925.3196 - val_loss: 27308628.0000 - val_mae: 3480.2148\n",
            "Epoch 164/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24137394.0000 - mae: 3072.0750 - val_loss: 29790296.0000 - val_mae: 4483.8560\n",
            "Epoch 165/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 26008532.0000 - mae: 3181.2075 - val_loss: 26198838.0000 - val_mae: 3712.1829\n",
            "Epoch 166/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 23570682.0000 - mae: 3017.4861 - val_loss: 27214594.0000 - val_mae: 3937.0894\n",
            "Epoch 167/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24539714.0000 - mae: 3034.0613 - val_loss: 29092918.0000 - val_mae: 3812.1799\n",
            "Epoch 168/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24887638.0000 - mae: 3183.6040 - val_loss: 25063452.0000 - val_mae: 3229.9209\n",
            "Epoch 169/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 23612690.0000 - mae: 3039.6892 - val_loss: 43766304.0000 - val_mae: 5898.6777\n",
            "Epoch 170/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25100876.0000 - mae: 3129.6423 - val_loss: 24979786.0000 - val_mae: 2945.2593\n",
            "Epoch 171/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24677630.0000 - mae: 3081.0791 - val_loss: 34094288.0000 - val_mae: 2798.1814\n",
            "Epoch 172/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25914272.0000 - mae: 3138.9861 - val_loss: 33804188.0000 - val_mae: 2830.5073\n",
            "Epoch 173/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25316540.0000 - mae: 3075.6306 - val_loss: 24740126.0000 - val_mae: 2860.4487\n",
            "Epoch 174/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 23913382.0000 - mae: 2946.8423 - val_loss: 29937010.0000 - val_mae: 4360.3745\n",
            "Epoch 175/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25382260.0000 - mae: 3152.3713 - val_loss: 29597492.0000 - val_mae: 4145.7920\n",
            "Epoch 176/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24875466.0000 - mae: 3068.8721 - val_loss: 26374342.0000 - val_mae: 2399.9211\n",
            "Epoch 177/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 25387706.0000 - mae: 3063.8096 - val_loss: 25272418.0000 - val_mae: 2339.6162\n",
            "Epoch 178/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 24219126.0000 - mae: 2934.3215 - val_loss: 27466340.0000 - val_mae: 3674.4834\n",
            "Epoch 179/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 24308842.0000 - mae: 3109.0564 - val_loss: 27217324.0000 - val_mae: 3764.0315\n",
            "Epoch 180/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24984390.0000 - mae: 3124.8042 - val_loss: 29148120.0000 - val_mae: 2582.9377\n",
            "Epoch 181/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24385196.0000 - mae: 2975.2029 - val_loss: 24936724.0000 - val_mae: 2887.0374\n",
            "Epoch 182/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25376936.0000 - mae: 3128.4482 - val_loss: 29307830.0000 - val_mae: 3079.2825\n",
            "Epoch 183/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 23205418.0000 - mae: 2934.5405 - val_loss: 29330792.0000 - val_mae: 4229.4316\n",
            "Epoch 184/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 24546826.0000 - mae: 3087.0417 - val_loss: 58242368.0000 - val_mae: 3953.8076\n",
            "Epoch 185/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24566674.0000 - mae: 3065.4673 - val_loss: 28969364.0000 - val_mae: 3728.8799\n",
            "Epoch 186/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 24494442.0000 - mae: 3059.2693 - val_loss: 32362512.0000 - val_mae: 3974.2683\n",
            "Epoch 187/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24086030.0000 - mae: 3036.9458 - val_loss: 34619536.0000 - val_mae: 4869.1543\n",
            "Epoch 188/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25496814.0000 - mae: 3152.6829 - val_loss: 35983972.0000 - val_mae: 5032.3735\n",
            "Epoch 189/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24656354.0000 - mae: 3129.0188 - val_loss: 26687008.0000 - val_mae: 2892.7078\n",
            "Epoch 190/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24419240.0000 - mae: 3022.1177 - val_loss: 35579892.0000 - val_mae: 3689.3225\n",
            "Epoch 191/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 24648798.0000 - mae: 3027.2874 - val_loss: 37513564.0000 - val_mae: 5280.6973\n",
            "Epoch 192/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24024272.0000 - mae: 3085.2546 - val_loss: 25994120.0000 - val_mae: 3150.6331\n",
            "Epoch 193/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24856764.0000 - mae: 3094.7473 - val_loss: 27463922.0000 - val_mae: 3393.2739\n",
            "Epoch 194/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24147540.0000 - mae: 3031.8840 - val_loss: 32961748.0000 - val_mae: 4791.8638\n",
            "Epoch 195/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24522064.0000 - mae: 3088.6411 - val_loss: 27897748.0000 - val_mae: 3914.9878\n",
            "Epoch 196/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 23156664.0000 - mae: 2992.6414 - val_loss: 28590168.0000 - val_mae: 2561.8254\n",
            "Epoch 197/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 24817678.0000 - mae: 3062.9321 - val_loss: 26962952.0000 - val_mae: 2468.0173\n",
            "Epoch 198/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 23986458.0000 - mae: 3070.8643 - val_loss: 28257716.0000 - val_mae: 3135.2471\n",
            "Epoch 199/200\n",
            "24/24 [==============================] - 0s 4ms/step - loss: 25384970.0000 - mae: 3070.0613 - val_loss: 29388140.0000 - val_mae: 3216.6067\n",
            "Epoch 200/200\n",
            "24/24 [==============================] - 0s 5ms/step - loss: 23682212.0000 - mae: 2972.6621 - val_loss: 44360692.0000 - val_mae: 3312.4868\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = go.Figure()\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(x = list(range(epochs)), y = history.history['mae'], name = 'entrenamiento')\n",
        ")\n",
        "\n",
        "fig.add_trace(\n",
        "    go.Scatter(x = list(range(epochs)), y = history.history['val_mae'], name = 'validación')\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    hovermode = 'x unified', \n",
        "    legend_title = 'Conjunto de', \n",
        "    xaxis_title = 'Época', \n",
        "    yaxis_title = 'Error Absoluto Medio (MAE)', \n",
        "    title = 'MAE en cada época del entrenamiento<br><sup>Segunda red neuronal</sup>', \n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "t2WacT42zDVE",
        "outputId": "33479cf4-8685-475c-97e2-d9bbd36f5ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"e1ace8c0-95a0-41d8-893c-c0fdb186b18e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e1ace8c0-95a0-41d8-893c-c0fdb186b18e\")) {                    Plotly.newPlot(                        \"e1ace8c0-95a0-41d8-893c-c0fdb186b18e\",                        [{\"name\":\"entrenamiento\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199],\"y\":[10151.275390625,9418.162109375,9360.1279296875,9307.5537109375,9320.8212890625,9185.5205078125,9089.5771484375,8973.23828125,8723.064453125,8306.185546875,7741.5439453125,7002.6435546875,6318.73486328125,5463.9677734375,5055.9052734375,5031.30615234375,4622.140625,4603.95556640625,4616.541015625,4589.64794921875,4531.005859375,4546.2138671875,4633.14794921875,4685.765625,4352.09521484375,4305.66796875,4496.55615234375,4507.8203125,4364.67333984375,4420.3828125,4275.52099609375,4547.16796875,4229.96923828125,4182.90673828125,4398.12744140625,4400.47802734375,4240.66162109375,4334.21826171875,4208.71240234375,4299.13916015625,4244.45166015625,4055.24072265625,3995.10693359375,4269.02197265625,4170.75390625,4081.6962890625,4082.958251953125,4053.70751953125,3915.533203125,4046.50927734375,3934.9384765625,3945.24462890625,3919.96484375,3926.499267578125,3852.6015625,3887.261474609375,3795.525634765625,3917.2626953125,3828.05810546875,3806.9091796875,3826.649658203125,3841.5283203125,3841.2294921875,3540.2490234375,3874.303466796875,3688.885986328125,3925.3359375,3815.19775390625,3696.430419921875,3658.33251953125,3740.048828125,3779.469482421875,3746.994873046875,3512.91748046875,3827.97998046875,3674.8515625,3670.899658203125,3706.450439453125,3524.260009765625,3726.016357421875,3671.83447265625,3528.062255859375,3613.76806640625,3558.517333984375,3559.754638671875,3623.73388671875,3467.91015625,3639.715576171875,3566.759033203125,3497.757080078125,3698.87744140625,3423.226318359375,3557.27099609375,3551.569580078125,3381.056884765625,3452.816162109375,3337.050537109375,3327.83935546875,3546.7236328125,3262.962646484375,3388.657470703125,3355.250732421875,3440.00537109375,3266.5908203125,3423.69140625,3234.860595703125,3191.191162109375,3276.624755859375,3249.859619140625,3458.808837890625,3192.63037109375,3342.03515625,3173.02783203125,3255.867919921875,3220.805908203125,3272.7724609375,3302.47314453125,3166.1728515625,3273.762939453125,3177.7763671875,3115.29443359375,3202.716796875,3263.849853515625,3190.763671875,3051.46533203125,3326.498779296875,3151.52783203125,3217.72314453125,3377.8408203125,3103.249267578125,3207.46435546875,3035.586669921875,3292.03271484375,2917.97265625,3291.877685546875,3060.876220703125,3148.605224609375,2990.793212890625,3144.69140625,3265.81591796875,3075.949951171875,2989.6474609375,3250.099609375,3134.9462890625,3273.936767578125,2997.715576171875,3041.8271484375,3114.511962890625,3086.962890625,3189.669677734375,3120.069580078125,3130.775390625,3108.33544921875,3003.92578125,3027.14794921875,3111.587890625,3137.44580078125,3037.153076171875,3174.640625,3185.1416015625,2961.656982421875,3264.65185546875,2925.319580078125,3072.074951171875,3181.20751953125,3017.486083984375,3034.061279296875,3183.60400390625,3039.689208984375,3129.642333984375,3081.0791015625,3138.986083984375,3075.630615234375,2946.84228515625,3152.371337890625,3068.8720703125,3063.8095703125,2934.321533203125,3109.056396484375,3124.80419921875,2975.202880859375,3128.4482421875,2934.54052734375,3087.041748046875,3065.46728515625,3059.269287109375,3036.94580078125,3152.682861328125,3129.018798828125,3022.11767578125,3027.287353515625,3085.254638671875,3094.747314453125,3031.884033203125,3088.64111328125,2992.641357421875,3062.93212890625,3070.8642578125,3070.061279296875,2972.662109375],\"type\":\"scatter\"},{\"name\":\"validaci\\u00f3n\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199],\"y\":[9164.9443359375,8882.0654296875,9636.95703125,10201.2314453125,8005.916015625,7654.80126953125,8536.4306640625,7906.39892578125,8743.322265625,6313.599609375,7678.072265625,4983.12353515625,4431.7890625,6368.537109375,4240.77587890625,3552.7333984375,6014.55322265625,7067.09375,6656.314453125,3496.39794921875,9551.8935546875,7248.591796875,3733.1083984375,4149.8466796875,6366.07958984375,3566.931396484375,3695.216796875,4768.15576171875,5435.3681640625,4377.37939453125,4502.876953125,3330.94140625,3247.763671875,3788.370361328125,3492.83154296875,4017.225830078125,3792.571533203125,7799.87255859375,3275.0791015625,4230.62646484375,3396.107421875,3246.233154296875,4153.17431640625,4000.2451171875,3121.490478515625,3383.8896484375,4170.38134765625,3168.001953125,8828.0966796875,3023.98193359375,4486.646484375,3869.65234375,4701.61279296875,3655.776611328125,3860.30078125,4058.899169921875,3569.080810546875,3919.783935546875,3342.36572265625,3142.72216796875,3226.844970703125,2916.80859375,2949.898193359375,3881.568603515625,3315.388916015625,4162.48095703125,2993.726806640625,4000.949462890625,3220.73779296875,2814.65234375,5913.95556640625,2978.17578125,3871.109130859375,3466.197021484375,3402.606689453125,4470.4560546875,3071.729736328125,3281.45849609375,4690.9130859375,4633.19921875,3975.844970703125,2868.359130859375,2674.743896484375,5936.9326171875,2739.79931640625,2836.01953125,3541.094482421875,4409.9453125,3710.587646484375,4044.0830078125,2817.88623046875,4261.82861328125,2742.165283203125,3106.755615234375,2501.94287109375,2592.10400390625,2586.0732421875,4514.35009765625,2919.995361328125,4102.62841796875,5223.6982421875,3968.126220703125,2436.212158203125,4558.4951171875,2485.641845703125,2773.040283203125,3788.17919921875,3840.925537109375,6541.14208984375,2877.91943359375,3952.22607421875,2884.616455078125,2699.84912109375,3951.096435546875,2699.07763671875,2589.6123046875,2565.6455078125,3948.440185546875,3937.976318359375,2988.19580078125,2964.086669921875,3023.2685546875,2443.51904296875,2616.695068359375,4435.552734375,3227.83935546875,3398.20751953125,2414.571044921875,2667.81201171875,5031.74755859375,2589.79052734375,3909.22802734375,2523.1943359375,6460.64208984375,2877.89892578125,2506.1689453125,3367.2666015625,3113.1591796875,2699.878662109375,3524.97412109375,3997.537353515625,4677.482421875,4465.0107421875,3767.384033203125,2760.9951171875,3195.593994140625,2826.70703125,4405.4775390625,4438.6396484375,2431.375244140625,2954.4189453125,3365.7470703125,3901.09912109375,2468.654296875,5056.9833984375,3530.25439453125,2950.51953125,5647.099609375,3793.037353515625,2404.438232421875,3798.346435546875,2702.12939453125,3480.21484375,4483.85595703125,3712.182861328125,3937.08935546875,3812.179931640625,3229.9208984375,5898.677734375,2945.25927734375,2798.181396484375,2830.50732421875,2860.44873046875,4360.37451171875,4145.7919921875,2399.921142578125,2339.6162109375,3674.4833984375,3764.031494140625,2582.937744140625,2887.037353515625,3079.282470703125,4229.431640625,3953.8076171875,3728.8798828125,3974.268310546875,4869.154296875,5032.37353515625,2892.707763671875,3689.322509765625,5280.697265625,3150.633056640625,3393.27392578125,4791.86376953125,3914.98779296875,2561.825439453125,2468.017333984375,3135.2470703125,3216.606689453125,3312.48681640625],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"hovermode\":\"x unified\",\"legend\":{\"title\":{\"text\":\"Conjunto de\"}},\"xaxis\":{\"title\":{\"text\":\"\\u00c9poca\"}},\"yaxis\":{\"title\":{\"text\":\"Error Absoluto Medio (MAE)\"}},\"title\":{\"text\":\"MAE en cada \\u00e9poca del entrenamiento<br><sup>Segunda red neuronal</sup>\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e1ace8c0-95a0-41d8-893c-c0fdb186b18e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observamos un comportamiento \"errático\" en los valores del MAE a lo largo de cada época. Lo importante es que la gráfica muestra que el modelo sí aprendió (el valor de la métrica fue disminuyendo conforme avanzan las épocas de entrenamiento)."
      ],
      "metadata": {
        "id": "xqfE75-Z0AaQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(weights_filepath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1LAcmNC_DeX",
        "outputId": "463df38a-d0ed-4e07-9cef-a553390e6b41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6bb0573c90>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluación"
      ],
      "metadata": {
        "id": "2VPw8OQGCq-J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_train_nn2 = model.predict(X_train_tr)[:, 0]\n",
        "y_pred_test_nn2 = model.predict(X_test_tr)[:, 0]"
      ],
      "metadata": {
        "id": "jas0YHGp_GlR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Métricas"
      ],
      "metadata": {
        "id": "aGTEa5WICt-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    f\"R2 train: {r2_score(y_train, y_pred_train_nn2)}\", \n",
        "    f\"R2 test: {r2_score(y_test, y_pred_test_nn2)}\", \n",
        "    \"\",\n",
        "    f\"MSE train: {mean_squared_error(y_train, y_pred_train_nn2)}\", \n",
        "    f\"MSE test: {mean_squared_error(y_test, y_pred_test_nn2)}\", \n",
        "    \"\",\n",
        "    f\"MAE train: {mean_absolute_error(y_train, y_pred_train_nn2)}\", \n",
        "    f\"MAE test: {mean_absolute_error(y_test, y_pred_test_nn2)}\", \n",
        "    sep = '\\n'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bvw6OYH_Ll8",
        "outputId": "246ecd3b-e5b1-461a-a313-e2e685c56dd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "R2 train: 0.848485778204348\n",
            "R2 test: 0.8240586212831948\n",
            "\n",
            "MSE train: 23273107.971546568\n",
            "MSE test: 22847374.7846029\n",
            "\n",
            "MAE train: 2295.9363390972435\n",
            "MAE test: 2408.6658949440202\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obtenemos una $R^2$ mejor que cuando ajustamos el modelo con la regresión lineal y el MAE se redujo casi a la mitad. Parece que esta red aprendió mejor a predecir los datos."
      ],
      "metadata": {
        "id": "yr7KSkge0aAY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualización"
      ],
      "metadata": {
        "id": "6kTOfNLmCvDb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model_evaluation(y_train, y_pred_train_nn2, y_test, y_pred_test_nn2, 'Segunda red neuronal')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "OEvX6SnLztxH",
        "outputId": "3849e1de-210a-4263-809c-e879266a9406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.8.3.min.js\"></script>                <div id=\"12109313-6e1e-4bdb-ba0a-7604b27a43c1\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"12109313-6e1e-4bdb-ba0a-7604b27a43c1\")) {                    Plotly.newPlot(                        \"12109313-6e1e-4bdb-ba0a-7604b27a43c1\",                        [{\"mode\":\"markers\",\"x\":[3167.45585,2689.4954,11576.13,16586.49771,6746.7425,5976.8311,5649.715,15161.5344,2007.945,19214.705530000003,16455.70785,10594.50155,27117.99378,4296.2712,7151.092,8410.04685,22331.5668,37165.1638,17468.9839,2494.022,6356.2707,11305.93455,12235.8392,14256.1928,13822.803,7173.35995,9140.951,1711.0268,5926.846,32108.662819999998,17942.106,43896.3763,8671.19125,6112.35295,15019.76005,11365.952,2709.24395,43254.41795,1263.249,3757.8448,20420.60465,2302.3,11658.11505,11394.06555,1967.0227,3353.4703,10269.46,18963.171919999997,31620.001060000002,30259.995560000003,6571.544,1632.56445,2755.02095,5934.3798,14319.031,19964.7463,6849.026,2498.4144,1631.6683,3766.8838,9625.92,3309.7926,37079.372,15359.1045,36124.5737,2842.76075,1622.1885,46599.1084,6948.7008,4500.33925,2128.43105,9095.06825,19673.335730000003,48824.45,8688.85885,3645.0894,2480.9791,2154.361,33307.5508,12643.3778,27375.90478,8556.907,37270.1512,6593.5083,9101.798,13019.16105,28868.6639,1621.3402,6571.02435,34828.654,4260.744000000001,8765.249,4040.55825,9875.6804,4337.7352,11356.6609,9290.1395,34439.8559,8891.1395,7162.0122,8026.6666,35160.13457,13555.0049,4719.73655,7222.78625,2534.39375,1719.4363,1635.73365,1725.5523,1824.2854,8342.90875,12829.4551,12979.358,4391.652,19933.458,7623.518,7640.3092,28101.33305,8442.667,2721.3208,2803.69785,12044.341999999999,1131.5066,11743.9341,5974.3847,2731.9122,26140.3603,4243.59005,4846.92015,28287.897660000002,20630.28351,2741.948,29186.48236,1261.859,6877.9801,14283.4594,4402.233,37742.5757,13217.0945,11163.568000000001,13451.122,26467.09737,58571.074479999996,13204.28565,1639.5631,2473.3341,48970.2476,9910.35985,3044.2133,60021.39897,41034.2214,9877.6077,8125.7845,2020.5523,25382.297000000002,34303.1672,7742.1098,10702.6424,8516.829,27322.733860000004,37701.8768,26125.674769999998,30184.9367,10107.2206,3070.8087,5152.134,4746.344,5910.944,4415.1588,14043.4767,16085.1275,8428.0693,39727.614,8283.6807,14210.53595,10355.641,12265.5069,6455.86265,11488.31695,4134.08245,9563.029,18804.7524,11856.4115,11264.541000000001,1631.8212,38792.6856,42560.4304,2150.469,7050.642,42112.2356,41949.2441,4462.7218,5484.4673,20234.85475,1877.9294,11085.5868,8219.2039,13126.67745,5227.98875,3268.84665,12479.70895,6250.435,28923.136919999997,2136.88225,13937.6665,36580.28216,6652.5288,3947.4131,19444.2658,12741.16745,1832.094,13770.0979,6113.23105,6198.7518,38511.6283,14382.70905,16450.8947,7133.9025,37829.7242,6406.4107,6933.24225,2137.6536,12404.8791,10096.97,26109.32905,3906.127,44423.803,12124.9924,1253.9360000000001,34166.273,45702.02235,13470.86,2632.992,38709.176,9182.17,1826.8429999999998,7243.8136,12430.95335,10118.424,4827.90495,5312.16985,3213.62205,4779.6023,4931.647,23082.95533,4433.9159,7985.815,33750.2918,48675.5177,33475.81715,9872.701,44260.7499,10601.412,2117.33885,11070.535,33900.653,2639.0429,5428.7277,11837.16,9301.89355,35491.64,47291.055,9566.9909,14410.9321,20878.78443,42983.4585,10796.35025,11884.04858,11244.3769,3279.86855,6796.86325,27037.9141,10848.1343,10560.4917,44202.6536,9288.0267,9620.3307,5458.04645,36149.4835,11381.3254,21595.38229,21880.82,1977.815,36837.467000000004,6640.54485,4347.02335,36219.40545,26926.5144,4435.0942,6128.79745,3597.5959999999995,1704.5681,16776.30405,2727.3951,7348.142,1242.816,45863.205,24671.66334,6186.1269999999995,19539.243000000002,2566.4707,49577.6624,39836.519,6402.29135,8827.2099,17904.52705,6272.4772,6500.2359,2257.47525,2155.6815,2362.22905,1815.8759,1875.344,9724.53,11538.421,2219.4451,2156.7518,42969.8527,1708.92575,9722.7695,15820.698999999999,11842.442,8582.3023,33907.547999999995,38998.546,7345.7266,4463.2051,15828.821730000001,2867.1196,5438.7491,4564.19145,46718.16325,12592.5345,23563.016180000002,6185.3208,10976.24575,27941.28758,4189.1131,41097.16175,36197.699,15817.9857,26018.95052,3857.75925,3490.5491,6238.298000000001,2396.0959,7419.4779,8413.46305,1880.07,7050.0213,4518.82625,11552.903999999999,10982.5013,4673.3922,23807.2406,20149.3229,12949.1554,4753.6368,27000.98473,24180.9335,7731.4271,7749.1564,9644.2525,42211.1382,6940.90985,24869.8368,13457.9608,12224.35085,8551.347,22478.6,12925.886,5012.471,9414.92,14988.431999999999,4618.0799,2730.10785,19749.383380000003,3556.9223,39241.442,17043.3414,8277.523000000001,4830.63,7518.02535,10214.636,18767.7377,1242.26,4536.259,8965.79575,34617.84065,16796.41194,11848.141000000001,8932.084,20009.63365,1759.338,45008.9555,37465.34375,46661.4424,11881.9696,5972.378000000001,1252.407,3875.7341,1743.214,3981.9768,13635.6379,10381.4787,9048.0273,3704.3545,5209.57885,7265.7025,47462.894,11674.13,1737.376,2775.19215,3693.428,5028.1466,1639.5631,9058.7303,10226.2842,4529.477,1629.8335,12815.44495,24915.04626,14692.66935,7986.47525,8017.06115,12105.32,3554.203,10461.9794,5615.369000000001,4795.6568,19515.5416,2103.08,4349.462,12523.6048,10072.05505,1136.3994,21677.28345,1704.70015,21774.32215,13063.883,5116.5004,10825.2537,3353.284,4149.736,13405.3903,10977.2063,14349.8544,11741.726,44641.1974,4762.329000000001,7371.772,3176.8159,2850.68375,40273.6455,17560.37975,4433.3877,1526.3120000000001,1702.4553,6373.55735,10942.13205,8547.6913,8604.48365,3176.2877,11150.78,4527.18295,12233.828000000001,9850.431999999999,7196.866999999999,11729.6795,21223.6758,10435.06525,5272.1758,2250.8352,33732.6867,14358.36437,8233.0975,17496.306,2789.0574,14901.5167,5124.1887,8027.968000000001,13143.33665,43578.9394,10807.4863,8522.003,5400.9805,9447.3824,10601.63225,4237.12655,14001.2867,1615.7667,11840.77505,40182.246,15230.32405,4877.98105,28340.18885,2699.56835,9748.9106,8539.671,47403.88,20167.336030000002,17352.6803,4058.1161,16297.846000000001,8596.8278,2497.0383,38282.7495,36898.73308,14474.675,13224.693000000001,42124.5153,14571.8908,11253.421,1534.3045,18246.4955,32548.3405,3877.30425,11842.62375,38126.2465,16577.7795,24915.22085,8569.8618,7804.1605,10736.87075,8968.33,9991.03765,21195.818,11658.37915,8603.8234,13919.8229,4915.05985,2217.6012,1634.5734,4934.705,32787.458589999995,12269.68865,2217.46915,5373.36425,8601.3293,12142.5786,7077.1894,20709.02034,1633.0444,2130.6759,7537.1639,14426.07385,1261.442,5757.41345,2211.13075,8782.469000000001,6748.5912,11833.7823,10928.848999999998,5709.1644,2203.73595,5354.07465,23065.4207,6875.960999999999,22412.6485,2055.3249,12950.0712,40003.33225,12363.546999999999,4032.2407,5855.9025,17626.23951,11033.6617,3594.17085,11396.9002,41999.52,9391.346,3577.9990000000003,4504.6624,40720.55105,3392.3652,9264.796999999999,22192.43711,10450.552,2416.955,5989.52365,20177.671130000002,21984.47061,8835.26495,36307.7983,18648.4217,39722.7462,12485.8009,5327.40025,6496.8859999999995,10579.711000000001,6686.4313,10577.087,1980.07,11455.28,2221.56445,5397.6167,1986.9334,17179.522,3761.292,8733.22925,6082.405,9855.1314,6117.4945,9866.30485,13844.7972,7935.29115,2913.5690000000004,18328.2381,1682.5970000000002,39611.7577,2201.0971,8823.279,30063.58055,39047.285,24535.69855,3659.3459999999995,1163.4627,4718.20355,2020.1770000000001,24059.68019,3206.49135,7201.70085,11881.358,46130.5265,23288.9284,8520.026,8023.13545,42760.5022,21348.706000000002,35147.52848,14451.83515,12096.6512,34254.05335,4005.4225,14001.1338,9715.841,12890.05765,48673.5588,7228.21565,19719.6947,39774.2763,2974.1259999999997,6548.19505,1964.78,3861.20965,2200.83085,20781.48892,19521.9682,13981.85035,8515.7587,14235.072,41919.097,13143.86485,25992.82104,12638.195,10959.6947,42111.6647,7729.64575,24227.33724,10106.13425,1842.519,16138.76205,9283.562,43943.8761,3484.3309999999997,48885.135610000005,1731.6770000000001,14394.39815,44501.3982,2404.7338,3591.48,10806.839,11944.59435,2464.6188,46151.1245,4894.7533,9193.8385,14119.62,4133.64165,11945.1327,20745.9891,25081.76784,2710.82855,11657.7189,13470.8044,9800.8882,23244.7902,19350.3689,4449.462,13880.948999999999,16115.3045,33471.97189,2304.0022,47305.305,2138.0707,3056.3881,28468.91901,2395.17155,13887.204,21771.3423,8269.044,12222.8983,48549.17835,8605.3615,5325.651,20984.0936,10407.08585,12032.326000000001,10422.91665,12648.7034,9282.4806,47896.79135,13393.756000000001,1981.5819,9411.005,9541.69555,2902.9065,21978.6769,3161.454,9704.66805,8444.474,11187.6567,20277.80751,3366.6697,11436.73815,4747.0529,1917.3184,13041.921,18806.14547,2457.502,7337.7480000000005,4922.9159,1727.54,3756.6216,7954.517,25333.33284,11362.755,43813.8661,5969.723000000001,1769.53165,2438.0552,13844.506000000001,5261.46945,9634.538,10923.9332,6610.1097,34838.873,24393.6224,3410.324,12957.118,3260.199,28476.734989999997,8116.26885,8978.1851,4661.28635,19107.7796,12574.048999999999,2866.091,8334.5896,16884.924,13747.87235,47496.49445,4357.04365,5836.5204,5472.4490000000005,21797.0004,8823.98575,19496.71917,13112.6048,2690.1138,4667.60765,10602.385,4428.88785,11512.405,11830.6072,3393.35635,2719.27975,46200.9851,12982.8747,18955.22017,10564.8845,8534.6718,2254.7967,4571.41305,11326.71487,6551.7501,11520.09985,2045.68525,48517.56315,21659.9301,4438.2634,36950.2567,2527.81865,4687.7970000000005,5125.2157,7448.40395,30942.1918,8334.45755,3972.9247,6196.448,16657.71745,27724.28875,6289.7549,39725.51805,11743.298999999999,7624.63,2205.9808,6653.7886,14455.64405,47928.03,1712.227,4889.9995,2585.269,12629.1656,40419.0191,25678.77845,35069.37452,7682.67,1909.52745,2643.2685,10704.47,8988.15875,4320.41085,24603.04837,5693.4305,12730.9996,11286.5387,13831.1152,8062.764,39125.33225,15006.57945,8871.1517,3943.5954,32734.1863,4670.64,8798.593,8083.9198,11345.518999999998,9432.9253,7441.053000000001,2026.9741,7045.499,1632.03625,37607.5277,13462.52,2102.2647,2104.1134,6414.178000000001,11363.2832,4562.8421,4719.52405,6067.12675,5257.50795,6666.243,1984.4533,3935.1799,23401.30575,6600.361,1627.28245,8124.4084,3062.50825,1727.785,3180.5101,43753.33705,30166.618169999998,15555.18875,4234.927,18259.216,12129.61415,5148.5526,11737.84884,5594.8455,3292.52985,2457.21115,35595.5898,24667.418999999998,40974.1649,24520.264,10043.249,3172.018,36910.60803,6799.4580000000005,3866.8552,10141.1362,6079.6715,1744.465,19040.876,14313.8463,4266.1658,12146.971000000001,3989.841,4561.1885,10085.846,1837.237,47269.85400000001,23045.56616,12981.3457,3925.7582,3558.62025,5245.2269,4185.0979,11272.331390000001,5729.0053,11879.10405,16069.08475,63770.42801,8280.6227,11082.5772,4837.5823,8527.532,4074.4537,5031.26955,11090.7178,1532.4697,34672.1472,2322.6218,6858.4796,13224.05705,52590.82939,8059.6791,7526.70645,7152.6714,8944.1151,12029.2867,22218.1149,3046.062,1748.774,43921.1837,1674.6323,4137.5227,9447.25035,7650.77375,2261.5688,9861.025,5979.731,7147.4728],\"y\":[3575.0869140625,2857.23779296875,10569.75,2027.2921142578125,7935.939453125,7283.5341796875,5128.64794921875,14915.2822265625,1746.4552001953125,5131.26904296875,15163.828125,11270.810546875,10966.8349609375,5643.52978515625,7523.15234375,9307.203125,30279.28515625,36945.56640625,26954.47265625,2799.3857421875,6167.13330078125,11798.3505859375,11861.1083984375,13585.529296875,12446.7236328125,8625.1728515625,10245.6796875,2319.8212890625,6340.8193359375,9626.302734375,20932.28515625,46530.57421875,8761.3447265625,6616.0791015625,14321.6650390625,10556.6748046875,3788.86962890625,42424.359375,898.4341430664062,4860.16796875,5103.04345703125,2869.509033203125,11975.173828125,11963.619140625,2083.8173828125,4244.53515625,10885.4072265625,3907.89892578125,13974.3232421875,12918.7236328125,6487.69287109375,2201.36572265625,3541.870849609375,7304.16845703125,12132.828125,21576.35546875,7429.73388671875,3035.377685546875,903.9786376953125,3375.306884765625,9276.8857421875,4109.26904296875,38342.9453125,16205.5078125,32804.515625,3327.764404296875,1208.3623046875,44727.125,7144.98193359375,6202.21435546875,2665.545166015625,10309.8408203125,3384.3154296875,51371.43359375,10135.095703125,4921.3251953125,1776.26171875,1843.726806640625,30492.4609375,12645.5595703125,7029.5810546875,8233.0703125,36278.08984375,8444.87109375,9408.765625,13046.6083984375,32610.92578125,2022.108154296875,7142.48974609375,34497.62890625,4338.48291015625,9758.583984375,5044.37109375,8803.6591796875,4660.6640625,11586.3837890625,10353.7529296875,34281.0078125,9928.748046875,8419.41015625,9107.9560546875,11079.28125,13406.8271484375,5326.06201171875,8368.9404296875,2607.107421875,2170.795166015625,2049.19580078125,1829.0821533203125,2305.62744140625,8834.26953125,7568.90380859375,11739.0908203125,4122.662109375,26734.01953125,8180.609375,9145.107421875,31343.1640625,10195.8916015625,3331.273193359375,2770.403564453125,12282.1982421875,1484.786865234375,11860.248046875,7499.94677734375,2748.524658203125,9435.2421875,5289.53564453125,5993.32958984375,12804.8330078125,9713.236328125,2722.133544921875,12221.177734375,956.3692626953125,7973.97705078125,16014.8525390625,4570.4208984375,36967.59765625,13174.330078125,10739.640625,12508.201171875,11228.2822265625,47451.6484375,12816.4189453125,1808.3740234375,2152.8427734375,50696.47265625,10637.8642578125,3016.405517578125,41866.234375,41938.86328125,10200.08203125,9005.080078125,1524.1767578125,31364.4296875,29222.5,9567.5263671875,11854.3828125,10002.974609375,12845.2138671875,38460.59765625,18496.66015625,38396.1953125,11351.8232421875,3992.828125,5256.94775390625,5407.998046875,5948.0556640625,5624.91650390625,13517.66015625,15286.09375,9015.6904296875,37173.8671875,9091.212890625,13410.3232421875,11064.7978515625,12552.498046875,7577.77197265625,12161.0576171875,5307.357421875,9755.4873046875,8007.3984375,11308.8271484375,11119.775390625,912.5703125,41200.875,43327.0390625,2026.3248291015625,7364.80078125,44255.9609375,42227.47265625,5316.927734375,7329.11474609375,26421.482421875,1553.4041748046875,11565.46875,9672.005859375,1971.53173828125,6284.0263671875,3127.250244140625,12309.9296875,7208.24169921875,12837.841796875,3126.3037109375,13311.5498046875,15159.5,8199.44140625,4605.283203125,18800.5,12606.8125,1818.2786865234375,13281.8408203125,8569.923828125,7343.5859375,44699.4140625,13689.623046875,18712.3515625,8442.9482421875,30760.017578125,7723.8935546875,7700.7421875,3289.896240234375,4350.8369140625,10109.884765625,28924.802734375,3843.9638671875,43553.5859375,12070.083984375,1334.115234375,33967.9609375,47225.43359375,11573.9814453125,2183.433837890625,38030.33984375,9618.857421875,1662.1939697265625,8499.2490234375,12319.2138671875,10046.142578125,5592.33056640625,7075.611328125,2919.2958984375,6049.17236328125,5279.521484375,1031.6856689453125,5178.865234375,9151.908203125,30594.41796875,50537.953125,32090.294921875,9553.9287109375,41934.453125,9780.8466796875,2227.166748046875,10837.921875,32619.59765625,3163.075439453125,6765.18603515625,11402.0087890625,10015.7041015625,32352.625,44017.5234375,10264.9365234375,13687.9423828125,8896.0166015625,46337.15234375,11533.1435546875,2715.9580078125,12054.0322265625,3588.45361328125,7702.05029296875,26063.537109375,11180.6591796875,11661.58203125,45485.203125,10422.1787109375,10375.2783203125,7050.40283203125,37356.109375,9734.04296875,2914.5439453125,23534.041015625,2294.079345703125,37232.58203125,7711.99951171875,4108.02197265625,37715.9921875,27104.31640625,5392.54052734375,8856.9794921875,3410.563232421875,2078.88134765625,14142.9755859375,2891.86669921875,6727.6083984375,1518.940185546875,47681.57421875,9620.4873046875,5966.48193359375,24085.154296875,2992.791748046875,48420.68359375,40722.44140625,7661.82470703125,9950.2822265625,21115.234375,8214.0048828125,8004.93603515625,2616.917236328125,2243.707275390625,2711.31103515625,2742.08203125,1239.39013671875,9868.78515625,9817.1220703125,1876.5977783203125,2176.399169921875,44162.7890625,1828.6455078125,10503.21484375,15231.46875,11091.060546875,9781.3125,31423.4609375,34028.0546875,7979.35546875,4857.66650390625,8462.453125,4214.24853515625,5325.083984375,6409.7646484375,38993.92578125,12541.4365234375,7876.6240234375,7251.59375,11311.3203125,13355.30859375,5747.7216796875,37564.15625,36029.59375,15242.2548828125,6159.2421875,4292.96484375,3628.5390625,6176.9990234375,3022.295654296875,8630.16796875,9417.9951171875,1459.2674560546875,7276.3642578125,4639.80029296875,11254.4228515625,11068.4951171875,5309.25439453125,26821.498046875,25384.796875,13469.0703125,4319.56005859375,13701.3076171875,27545.103515625,9399.9130859375,8609.611328125,9974.7177734375,41849.11328125,8238.68359375,28430.62109375,13433.8681640625,12343.2470703125,8517.88671875,30749.8828125,13482.5341796875,4498.8447265625,9559.55078125,13419.0283203125,7836.2529296875,4572.26611328125,11864.5361328125,3536.035400390625,39991.953125,21953.083984375,8312.7626953125,7367.98388671875,8780.1103515625,10100.6904296875,18026.2421875,1503.7135009765625,4052.553466796875,9539.5087890625,36226.40625,4490.87158203125,10896.3212890625,9145.6572265625,24491.908203125,963.7174072265625,39901.0078125,35388.12109375,47092.62109375,12642.86328125,5242.94384765625,1430.2691650390625,4067.1484375,1106.6959228515625,4385.8857421875,12958.041015625,11255.181640625,10155.58984375,5084.1591796875,6132.3701171875,8190.427734375,49418.453125,10899.9677734375,1427.1783447265625,3129.759521484375,3038.22802734375,6341.8125,1808.3740234375,8918.8974609375,10747.69921875,4690.96923828125,889.7179565429688,12969.521484375,7599.3720703125,13737.7421875,9147.4931640625,9044.76171875,11731.962890625,3387.357421875,11124.3759765625,5355.46142578125,4343.65673828125,21180.8984375,2210.06689453125,4552.2158203125,12203.1845703125,10919.0166015625,1242.8504638671875,21479.42578125,2075.134033203125,25405.203125,10628.4296875,6007.5400390625,11322.5791015625,3247.643798828125,3716.516357421875,12860.650390625,11400.1328125,13009.162109375,11517.7353515625,45324.16796875,4563.4091796875,7099.685546875,3928.84912109375,2925.3369140625,39894.90625,22638.3359375,5176.17578125,1537.2762451171875,2090.94775390625,6155.0341796875,11415.724609375,9947.935546875,9944.8330078125,3947.510009765625,11101.41796875,5296.01318359375,11486.6513671875,9734.998046875,7523.57958984375,12361.5576171875,22319.85546875,11095.6689453125,6081.51318359375,2413.72705078125,33509.2109375,4728.93603515625,9741.2060546875,24142.828125,3369.841552734375,13514.1884765625,5204.05615234375,8587.40625,13054.1982421875,36260.87890625,11729.4150390625,8596.5654296875,4808.86328125,10500.7998046875,11075.7861328125,5375.97265625,14218.47265625,1533.187744140625,11959.6484375,36633.66015625,14615.982421875,5479.4267578125,9401.0517578125,3212.185546875,8970.9970703125,9064.0126953125,47616.53515625,16885.494140625,24846.974609375,4500.22265625,24455.8125,9668.494140625,3121.91650390625,38153.6640625,34547.48046875,14006.462890625,12343.734375,41083.84375,13448.078125,11137.71875,1577.7833251953125,26327.81640625,27316.78125,4461.00439453125,11847.9228515625,37700.234375,19622.23828125,30955.935546875,7292.38427734375,8039.73583984375,11574.6533203125,9342.9091796875,10237.9052734375,19774.216796875,11959.2109375,9918.59375,14113.8056640625,8060.8671875,3353.00634765625,1067.2470703125,5175.52978515625,29846.1328125,12388.0029296875,3343.222900390625,6570.2001953125,10038.7431640625,12740.6884765625,8421.6943359375,11358.9736328125,981.314697265625,2781.083251953125,8404.3515625,4809.20361328125,973.7498779296875,5236.2880859375,2894.513671875,9272.6181640625,7921.66015625,12442.3701171875,11724.5546875,5681.41162109375,2474.904541015625,6684.115234375,28650.46484375,7348.0263671875,20940.716796875,2294.93505859375,12532.9208984375,37029.83984375,11962.0390625,5062.67919921875,6911.98486328125,6155.5595703125,12761.9384765625,4120.765625,11750.431640625,35374.171875,9151.5166015625,2864.632568359375,4999.0048828125,34358.15625,3698.744140625,9631.8759765625,12709.0771484375,10067.8154296875,2253.439697265625,7643.65771484375,4289.48046875,5456.294921875,9746.0205078125,35569.30078125,30923.501953125,40484.8125,12684.6708984375,7158.154296875,6941.72216796875,10534.6806640625,7727.7080078125,10523.208984375,1826.31982421875,10830.1591796875,2482.63720703125,5048.9814453125,2223.640380859375,15080.6171875,3511.01025390625,9518.48828125,6444.74755859375,11075.21875,7323.794921875,10105.3759765625,13181.3837890625,7148.525390625,2408.39013671875,27207.3984375,1257.5592041015625,39571.09765625,2291.239501953125,9092.5244140625,12507.076171875,39886.9453125,29240.58984375,3879.2607421875,1065.1395263671875,5878.79052734375,1122.961669921875,5927.080078125,3586.6416015625,8233.4892578125,11497.201171875,39067.921875,5221.5478515625,8722.759765625,8358.447265625,41648.10546875,24666.4296875,28104.404296875,13945.7958984375,12615.7958984375,33126.11328125,4299.4501953125,14214.65234375,10826.8994140625,1854.863037109375,50194.48828125,8361.02734375,26751.283203125,43223.47265625,2914.8798828125,7456.54833984375,2036.705078125,4029.756103515625,2310.305419921875,11152.87890625,26988.451171875,13470.677734375,10040.7880859375,12866.8232421875,38795.296875,13036.150390625,11986.0849609375,11589.70703125,11091.57421875,45194.359375,9126.6064453125,10798.1435546875,11057.8994140625,1989.2320556640625,16889.22265625,9927.447265625,31017.986328125,3668.27880859375,43744.60546875,1597.7447509765625,14301.380859375,46376.24609375,3168.37744140625,3537.02294921875,10577.6015625,12136.1884765625,2711.9541015625,47903.2734375,6117.314453125,9645.220703125,13663.3388671875,4870.22998046875,12137.2333984375,27503.859375,3229.171875,3922.03369140625,11999.1142578125,12653.9853515625,9649.6708984375,22985.2421875,28118.32421875,5324.76171875,12382.7998046875,16399.400390625,11448.5390625,2646.86767578125,45018.80078125,3193.741455078125,2823.64599609375,9579.9189453125,3088.15283203125,13254.130859375,25610.6796875,8814.51953125,12394.5634765625,52105.78515625,9853.8251953125,5225.220703125,28394.19140625,11318.3095703125,11409.767578125,11459.1962890625,12169.2509765625,10575.3154296875,49164.8125,12228.2734375,2560.180908203125,10275.439453125,7611.81396484375,3856.124755859375,24550.869140625,3061.097412109375,11033.5078125,8942.1376953125,11646.69921875,4965.88330078125,3113.38134765625,11722.1728515625,6546.87060546875,1848.50732421875,11983.8974609375,8901.8369140625,2423.690185546875,7568.30419921875,6915.7900390625,1705.01904296875,5049.453125,8117.07275390625,11157.7724609375,10680.0478515625,38322.58984375,6721.87890625,2089.587646484375,2037.0955810546875,16613.443359375,6249.41650390625,8707.4140625,11154.6796875,6281.88720703125,34629.21875,27503.37890625,2572.928466796875,11884.728515625,4087.664794921875,6450.5595703125,9137.7177734375,10162.01171875,5280.025390625,25975.373046875,10914.4912109375,3239.973876953125,9484.8310546875,28548.71875,12908.13671875,46231.40234375,4830.068359375,6315.69140625,5957.7841796875,10888.0322265625,9506.0087890625,9242.6884765625,12540.1865234375,2736.091552734375,4632.47216796875,9708.51953125,5430.51171875,11911.3330078125,11881.12109375,5574.49609375,4010.8642578125,49321.20703125,12703.9873046875,2212.972900390625,10786.6787109375,9818.203125,2164.6005859375,5611.181640625,4253.88818359375,9322.9267578125,11421.3603515625,2752.76708984375,49088.34375,26324.89453125,5316.90478515625,36760.56640625,2950.850830078125,5930.51025390625,6394.17529296875,7264.13671875,37190.34765625,9489.34375,4697.07958984375,5581.255859375,16903.44921875,2107.41796875,7797.603515625,32849.85546875,12511.3310546875,8141.3232421875,2602.28564453125,7702.9072265625,9657.681640625,47512.75390625,1621.040283203125,5841.99755859375,2377.850830078125,13214.3154296875,41591.3046875,24925.4140625,20908.505859375,8971.4658203125,2163.19677734375,2994.204345703125,10164.9736328125,9784.7763671875,4517.25927734375,7774.45361328125,6180.4501953125,12905.552734375,11613.2783203125,13247.759765625,8407.6923828125,37575.8515625,11124.4755859375,10079.041015625,3618.7177734375,11110.87890625,4781.0517578125,8523.21484375,7268.2744140625,11108.4599609375,9335.7548828125,7967.9267578125,1256.5140380859375,7622.7744140625,2198.951904296875,38384.4375,12031.3583984375,2456.378173828125,2340.118896484375,6724.71826171875,11747.0634765625,5583.08203125,5832.138671875,7523.75341796875,6275.7275390625,6588.35205078125,2090.48583984375,4588.42041015625,28040.40625,7185.23095703125,2069.183349609375,9218.025390625,3648.94287109375,1557.1529541015625,3732.2548828125,50210.90234375,13525.490234375,14770.955078125,3744.9462890625,21861.564453125,12325.8447265625,5169.96142578125,5673.24267578125,5434.35498046875,3624.662841796875,2471.3115234375,32130.439453125,26690.212890625,38956.69921875,33901.546875,10827.9072265625,2531.883544921875,12501.1640625,8050.11962890625,5026.5654296875,11100.669921875,6463.5712890625,1054.55419921875,25939.74609375,13554.7509765625,4135.77392578125,11562.5595703125,2881.993408203125,9217.451171875,9939.5888671875,1986.43359375,49235.65625,10708.8017578125,12756.2314453125,5860.30908203125,3886.396240234375,5616.66455078125,5834.3701171875,2299.252685546875,6675.66015625,12191.767578125,15292.3564453125,51923.60546875,9475.6123046875,11589.2392578125,6078.9521484375,9418.6572265625,5554.3642578125,6182.79736328125,10489.548828125,1655.0234375,31220.69140625,2675.055908203125,8461.7705078125,13310.6533203125,43413.5234375,8896.5849609375,8467.2001953125,8129.828125,9628.26171875,12445.2900390625,23100.19921875,2881.62646484375,874.9539184570312,39154.734375,1993.3822021484375,4251.28955078125,10505.4658203125,6687.74462890625,2827.60498046875,10042.1826171875,6075.560546875,8661.931640625],\"type\":\"scatter\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"mode\":\"markers\",\"x\":[7281.5056,5267.81815,12347.171999999999,24513.09126,3736.4647,7358.17565,9788.8659,17085.2676,8211.1002,19798.05455,3077.0955,3385.39915,6837.3687,8538.28845,26392.260290000002,13012.20865,3227.1211,15170.069,11073.176000000001,20773.62775,39556.4945,2134.9015,2198.18985,6555.07035,4340.4409,12622.1795,7740.3369999999995,12475.3513,3987.926,21082.16,1241.565,40103.89,17929.303369999998,8302.53565,3471.4096,5846.9176,13352.0998,9144.565,25656.575259999998,7726.854,13887.9685,30284.642939999998,5266.3656,12797.20962,1146.7966,7046.7222,8627.5411,39597.4072,12323.936000000002,11454.0215,40904.1995,3171.6149,7445.918000000001,13607.36875,27346.04207,12557.6053,10797.3362,5488.262,6282.235,40941.2854,1708.0014,23306.547000000002,28950.4692,1664.9996,17361.7661,7345.084,18157.876,7256.7231,7626.993,26236.57997,7325.0482,1720.3537,7153.5539,6986.696999999999,8232.6388,10370.91255,4889.0368,6474.013000000001,1625.43375,10115.00885,10264.4421,9386.1613,18223.4512,3561.8889,23887.6627,3392.9768,1135.9407,1880.487,5630.45785,10156.7832,22144.032000000003,62592.87309,9500.57305,46113.511,13390.559,4076.4970000000003,6389.37785,9880.068000000001,4751.07,12629.8967,6664.68595,21344.8467,22395.74424,4466.6214,9249.4952,8703.456,38245.59327,36189.1017,19023.26,5080.096,9957.7216,6313.759,4151.0287,17748.5062,37133.8982,21472.4788,4350.5144,41661.602,6985.50695,23241.47453,39983.42595,9174.13565,6710.1919,1633.9618,47055.5321,2899.48935,12231.6136,5138.2567,4646.759,12913.9924,24476.47851,27808.7251,18033.9679,25309.488999999998,6360.9936,20296.86345,10600.5483,9583.8933,34806.4677,5708.866999999999,6457.8434,21259.37795,12928.7911,2904.0879999999997,3537.703,36085.219,10713.643999999998,21232.182259999998,8457.818000000001,18838.70366,3500.6123,6753.0380000000005,1256.299,45710.20785,11289.10925,1141.4451,12644.589,1121.8739,12495.29085,27218.43725,13129.60345,1837.2819,6311.951999999999,12333.828000000001,18218.16139,48173.361,6781.3542,3994.1778,1621.8827,11165.41765,9964.06,7441.501,5240.765,4686.3887,18972.495,6393.60345,3732.6251,19144.57652,7633.7206,11093.6229,46889.2612,11931.12525,4454.40265,1149.3959,34779.615,3277.1609999999996,27533.9129,10959.33,19594.80965,2483.736,10065.413,7512.267,19442.3535,7209.4918,11299.343,10231.4999,14711.7438,4906.40965,13415.0381,6123.5688,5469.0066,1705.6245,12646.207,6435.6237,11482.63485,44400.4064,17663.1442,14590.63205,38711.0,2166.732,8116.68,38746.3551,5584.3057,6775.960999999999,55135.402089999996,18903.49141,5920.1041,3378.91,13430.265,11353.2276,5253.524,4738.2682,2196.4732,11987.1682,1515.3449,17178.6824,12268.63225,14133.03775,7421.19455,36021.0112,5966.8874,17081.08,6059.173000000001,37484.4493,10338.9316,2855.43755,13616.3586,29523.1656,1607.5101,9504.3103,25517.11363,18310.742,9869.8102,38415.474,11554.2236,12609.88702,10594.2257,3238.4357,14394.5579,2585.85065,2927.0647,14254.6082,40932.4295,4883.866,7418.522,9630.396999999999,8606.2174,5003.853,2680.9493,9549.5651,3847.6740000000004,24873.3849,5699.8375,42856.837999999996,3201.24515,22493.65964,11566.30055,7160.3303,1646.4297,9617.66245,2203.47185,1137.011,9487.6442,7731.85785,7443.64305,2523.1695,34472.841,1969.614,7789.635,41676.0811,1694.7964,1906.35825,11763.0009,9225.2564,21098.55405,2709.1119,4992.3764,6600.20595,18608.262,2459.7201,8252.2843,4441.21315,5425.02335,4058.71245,15518.18025,4544.2348,19199.944,8347.1643,5478.0368,14007.222,42303.69215,5377.4578,1628.4709,16420.49455,10436.096,9778.3472,15612.19335,7261.741,11015.1747,10493.9458,6203.90175,10197.7722,6770.1925,1972.95,9304.7019,29141.3603,46255.1125,11013.7119,8162.71625,12244.531,5662.225,5383.536,12094.478000000001,14478.33015,3208.7870000000003,7639.41745,2897.3235,29330.98315,11534.87265,1137.4697,23568.272,13429.0354,9222.4026,36397.576,3956.07145,10965.446000000002,9377.9047,2801.2588,44585.45587,23967.38305,13228.84695,2352.96845,6334.34355,11938.25595,6338.0756,20462.99766,8068.185,4949.7587,9863.4718,39871.7043,19361.9988,6184.2994,1728.8970000000002,7323.734818999999,14418.2804,5246.047,38344.566,7144.86265,2207.69745,3579.8287,10325.206,8310.83915,3481.868,4399.731,5385.3379,4766.022,16232.847,13974.45555,10795.937329999999,3021.80915,8615.3,9361.3268,35585.576,7147.105,4239.89265,11735.87905,17128.42608,5375.0380000000005,10791.96,3443.0640000000003,5415.6612,7727.2532,18765.87545,6358.77645,5002.7827,14449.8544,2597.779,13047.33235,51194.55914,2331.519,11946.6259,8964.06055,7160.094,13725.47184,8825.086,11411.685,8930.93455,24106.91255,17878.900680000002,22462.04375,1391.5287,8240.5896],\"y\":[8106.1806640625,6428.17138671875,11598.818359375,13259.6328125,4552.53125,8223.8017578125,10913.662109375,23539.5859375,9882.8046875,18333.24609375,3242.926025390625,3139.881103515625,8419.1484375,9583.6259765625,9807.4326171875,12672.3564453125,3048.597900390625,13865.990234375,10678.3193359375,28096.62109375,35704.94921875,3013.908447265625,2174.3779296875,7789.95361328125,5411.63525390625,12443.5126953125,8038.99658203125,12022.505859375,3372.208251953125,29690.865234375,1484.679443359375,38928.08984375,8421.10546875,9586.1826171875,3678.1640625,5205.36865234375,13111.935546875,8859.46484375,10411.8515625,8332.9365234375,13228.0068359375,10596.8359375,6485.7470703125,7195.8671875,823.3880615234375,8746.830078125,9692.9345703125,35756.80859375,12709.9130859375,11865.1865234375,49429.75390625,3118.77880859375,7778.8359375,13620.4326171875,10690.8046875,12852.4111328125,11413.6357421875,6001.98291015625,6950.47021484375,37692.07421875,1886.775634765625,29848.439453125,30472.322265625,2540.633544921875,17245.375,7014.92138671875,5167.501953125,8351.6591796875,8044.69677734375,9039.458984375,8658.4697265625,2125.778564453125,8282.5400390625,8069.642578125,9757.412109375,11093.3916015625,5819.27294921875,5451.1181640625,2052.76611328125,10713.3330078125,10918.4462890625,10481.1591796875,24932.734375,4819.40869140625,24220.240234375,3666.811767578125,1261.968994140625,1025.028564453125,5744.3955078125,10654.2001953125,33477.47265625,31571.09765625,10598.2470703125,51742.7265625,12337.5166015625,4346.84814453125,7705.94970703125,9051.546875,5213.56201171875,12993.8369140625,7953.1904296875,2459.918701171875,5546.14111328125,8298.7763671875,10155.408203125,9127.7919921875,24415.83203125,33164.9375,19990.85546875,4614.4638671875,11067.9140625,7641.5078125,4281.82275390625,28162.65234375,38128.31640625,28065.8671875,5317.4951171875,42887.30078125,8260.392578125,4611.31494140625,39196.44921875,10045.564453125,7863.41015625,1032.8739013671875,44175.8515625,3117.716552734375,12005.5009765625,5253.96484375,4966.4697265625,13118.2607421875,10939.5244140625,29506.68359375,26997.912109375,34771.6640625,9565.939453125,25043.93359375,11163.9453125,10103.494140625,29937.37890625,5566.2822265625,7564.73095703125,24869.001953125,13418.2392578125,3215.5859375,3971.4462890625,32087.900390625,9654.177734375,8162.1376953125,8300.6181640625,7200.84033203125,3541.249755859375,7756.4169921875,1203.100830078125,40712.609375,11680.1611328125,1032.5443115234375,11371.2216796875,1908.0892333984375,12560.51171875,34150.46484375,13091.005859375,1631.12744140625,6624.5791015625,11902.232421875,3209.707763671875,50727.640625,7217.1416015625,3084.2509765625,1221.1080322265625,11953.978515625,10899.0908203125,7869.6103515625,5194.20556640625,4425.74658203125,22147.33203125,7678.12939453125,5003.9521484375,9165.421875,8918.8134765625,10116.35546875,42264.515625,12486.4814453125,5561.10791015625,742.9406127929688,34392.7421875,3160.398193359375,34472.703125,10900.2900390625,17586.59765625,4035.60205078125,10083.1875,7817.90087890625,5165.93505859375,8122.49072265625,11085.1376953125,10930.1953125,14002.87109375,5609.716796875,12626.052734375,7440.453125,6338.7392578125,2036.2510986328125,11780.1162109375,6282.00390625,1061.78515625,46062.12109375,19603.7890625,14426.8466796875,38058.5234375,1219.7152099609375,8381.1796875,35290.5625,6919.4619140625,6815.24951171875,40899.59375,4568.41552734375,5744.380859375,3548.211181640625,13592.0830078125,11679.53515625,5533.1494140625,5274.0966796875,2086.02490234375,12726.1279296875,2507.42041015625,25288.15234375,12425.3251953125,912.5703125,8569.51953125,35224.1875,8022.4443359375,29121.306640625,6223.1318359375,37280.96875,11133.4365234375,3170.00634765625,13403.5380859375,34119.4140625,1920.9700927734375,9508.50390625,11894.4189453125,23823.173828125,10723.955078125,35297.52734375,12520.4521484375,3581.39404296875,11329.333984375,2646.604736328125,14012.3671875,2603.97021484375,2501.109130859375,13481.013671875,44506.9375,5213.82470703125,7507.5556640625,9803.0576171875,9802.236328125,5275.81982421875,3515.968017578125,8151.91748046875,4135.23486328125,26311.3984375,7044.61962890625,41396.4140625,3816.9873046875,3023.337158203125,11284.15625,8566.447265625,1634.64794921875,10810.359375,2459.918701171875,1217.3585205078125,10782.6357421875,7874.3701171875,7835.4521484375,3352.6923828125,32683.9453125,2384.201416015625,9414.07421875,41785.80859375,2056.492431640625,2362.49755859375,12294.7529296875,10464.525390625,22013.595703125,3775.3017578125,6285.55615234375,7714.94189453125,22314.974609375,2606.894287109375,9412.9912109375,5681.19482421875,6103.87255859375,4657.0400390625,20830.14453125,5931.54150390625,24036.701171875,9521.8564453125,6765.7294921875,13078.2431640625,36714.12109375,6433.904296875,2079.7763671875,14071.9345703125,10460.9951171875,10572.4287109375,14717.525390625,8318.6220703125,11443.154296875,11132.9052734375,7309.6669921875,11231.94140625,8383.44921875,2403.98291015625,10422.7607421875,33315.6796875,49275.01953125,11714.5908203125,8919.615234375,12190.0087890625,4923.93115234375,5631.482421875,11636.8017578125,10499.4189453125,3247.23486328125,8254.005859375,3221.31494140625,30704.880859375,11691.5908203125,1198.23974609375,32583.599609375,13013.2802734375,10164.66015625,37767.45703125,4783.4482421875,10769.3349609375,10772.8525390625,2699.238037109375,36608.5703125,26394.208984375,12640.87109375,3186.714111328125,9200.9404296875,12339.865234375,7961.12890625,12271.1650390625,8234.791015625,4892.90966796875,10539.87109375,40557.5859375,22098.47265625,7543.82958984375,1573.6971435546875,2384.98828125,13984.984375,4663.955078125,40376.16015625,8322.2919921875,2699.695068359375,6185.412109375,10239.6494140625,9258.9638671875,5139.96337890625,6140.18603515625,6474.982421875,5441.3291015625,23859.78515625,13718.76953125,1538.981689453125,3311.67724609375,9016.8515625,10601.6171875,35522.86328125,8067.09130859375,5312.12841796875,12061.216796875,7489.7861328125,5200.271484375,10567.587890625,3348.41162109375,7468.5380859375,8456.0771484375,20696.35546875,7620.26220703125,6544.8837890625,13856.3056640625,2313.1103515625,13288.66796875,37642.1796875,1772.2659912109375,12576.4794921875,10073.9013671875,7529.841796875,8857.2080078125,9028.685546875,10520.9853515625,9851.19921875,26387.294921875,3885.822021484375,32041.22265625,1623.02294921875,9371.4130859375],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.45],\"title\":{\"text\":\"Cargos reales\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Cargos predecidos\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.55,1.0],\"title\":{\"text\":\"Cargos reales\"}},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Cargos predecidos\"}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Conjunto de entrenamiento\",\"x\":0.225,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Conjunto de prueba\",\"x\":0.775,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"shapes\":[{\"line\":{\"dash\":\"dot\"},\"type\":\"line\",\"x0\":874.9539184570312,\"x1\":52105.78515625,\"y0\":874.9539184570312,\"y1\":52105.78515625},{\"line\":{\"dash\":\"dot\"},\"type\":\"line\",\"x0\":742.9406127929688,\"x1\":51742.7265625,\"xref\":\"x2\",\"y0\":742.9406127929688,\"y1\":51742.7265625,\"yref\":\"y2\"}],\"title\":{\"text\":\"Segunda red neuronal\"},\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('12109313-6e1e-4bdb-ba0a-7604b27a43c1');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nuevamente, parece que hay 3 o 4 grupos de casos. Sin embargo, la diferencia es menos obvia que cuando se graficaron los resultados de la regresión lineal (y más aún en comparación a la primera red). "
      ],
      "metadata": {
        "id": "7Bxwkj7E0vCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusión"
      ],
      "metadata": {
        "id": "aqJW27FvlX1f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "La regresión lineal fue un buen primer acercamiento a este problema considerando que arrojó una $R^2$ mayor a 0.7. Sin embargo, para cargos grandes no se comportó bien y se vieron 3 o 4 grupos distintos. Un análisis a detalle de las variables e ingeniería de características seguramente ayuda a mejorar el desempeño. \n",
        "\n",
        "El segundo intento con la red neuronal predice mejor que la anterior, es decir, logró *aprender* mejor las relaciones entre las variables. Con más capas, el desempeño mejora, pero se dejará como ejercicio."
      ],
      "metadata": {
        "id": "mirMh75XlZST"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejercicios"
      ],
      "metadata": {
        "id": "J1vgW_Dol06E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Realizar un análisis descriptivo de los datos.\n",
        "\n",
        "2. ¿Por qué se obtuvieron mejores resultados con la red neuronal que con la regresión lineal?\n",
        "\n",
        "    2.1 ¿Por qué la primera red neuronal predice peor que la regresión lineal?\n",
        "    \n",
        "    2.2 ¿Cuál es la intuición detrás de \"si hay más capas, usualmente la red neuronal predice mejor\"?\n",
        "\n",
        "3. Aplicar transformaciones a las variables y ajustar de nuevo la regresión lineal y una red neuronal para ver si mejoran los resultados.\n",
        "\n",
        "4. Diseñar una red más compleja que mejore los resultados obtenidos"
      ],
      "metadata": {
        "id": "X51_URG3l2EJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Extra"
      ],
      "metadata": {
        "id": "ij6LiBu-mUVe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Comprobar los supuestos de la regresión lineal:\n",
        "\n",
        "    1. Linearidad.\n",
        "\n",
        "    2. Los errores son ruido blanco.\n",
        "\n",
        "    3. Sin multicolinealidad en los predictores\n",
        "\n",
        "    4. Homocedasticidad\n",
        "\n",
        "2. ¿Se viola alguno? ¿Se puede hacer algo para arreglarlo?\n",
        "\n",
        "3. El problema que se presentó es un problema de **regresión**. Además de la regresión lineal y la red neuronal presentada, ¿qué otros algoritmos de regresión conoce?\n",
        "\n",
        "    1. Utilice al menos uno, evalué y compare los resultados obtenidos.\n",
        "\n",
        "    2. ¿Los resultados fueron mejores que con la regresión lineal? En caso de ser afirmativa la respuesta, ¿nos dice algo sobre los datos?"
      ],
      "metadata": {
        "id": "GbN60vjCmVkd"
      }
    }
  ]
}